{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
      "  Cloning https://github.com/facebookresearch/detectron2.git to c:\\users\\ajaym\\appdata\\local\\temp\\pip-req-build-h15ujg51\n",
      "  Resolved https://github.com/facebookresearch/detectron2.git to commit 9604f5995cc628619f0e4fd913453b4d7d61db3f\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: Pillow>=7.1 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from detectron2==0.6) (10.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from detectron2==0.6) (3.9.2)\n",
      "Collecting pycocotools>=2.0.2 (from detectron2==0.6)\n",
      "  Downloading pycocotools-2.0.8-cp39-cp39-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: termcolor>=1.1 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from detectron2==0.6) (2.1.0)\n",
      "Collecting yacs>=0.1.8 (from detectron2==0.6)\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
      "Collecting tabulate (from detectron2==0.6)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting cloudpickle (from detectron2==0.6)\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting tqdm>4.29.0 (from detectron2==0.6)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: tensorboard in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from detectron2==0.6) (2.10.0)\n",
      "Collecting fvcore<0.1.6,>=0.1.5 (from detectron2==0.6)\n",
      "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting iopath<0.1.10,>=0.1.7 (from detectron2==0.6)\n",
      "  Downloading iopath-0.1.9-py3-none-any.whl.metadata (370 bytes)\n",
      "Collecting omegaconf<2.4,>=2.1 (from detectron2==0.6)\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting hydra-core>=1.1 (from detectron2==0.6)\n",
      "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting black (from detectron2==0.6)\n",
      "  Downloading black-25.1.0-cp39-cp39-win_amd64.whl.metadata (81 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from detectron2==0.6) (24.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.26.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.1)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.1->detectron2==0.6)\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting portalocker (from iopath<0.1.10,>=0.1.7->detectron2==0.6)\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from matplotlib->detectron2==0.6) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from matplotlib->detectron2==0.6) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from matplotlib->detectron2==0.6) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from matplotlib->detectron2==0.6) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from matplotlib->detectron2==0.6) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from matplotlib->detectron2==0.6) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from matplotlib->detectron2==0.6) (6.4.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tqdm>4.29.0->detectron2==0.6) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from black->detectron2==0.6) (8.1.7)\n",
      "Collecting mypy-extensions>=0.4.3 (from black->detectron2==0.6)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pathspec>=0.9.0 (from black->detectron2==0.6)\n",
      "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: platformdirs>=2 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from black->detectron2==0.6) (3.10.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from black->detectron2==0.6) (2.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from black->detectron2==0.6) (4.11.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorboard->detectron2==0.6) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorboard->detectron2==0.6) (1.48.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorboard->detectron2==0.6) (2.29.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorboard->detectron2==0.6) (0.4.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorboard->detectron2==0.6) (3.4.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorboard->detectron2==0.6) (3.19.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorboard->detectron2==0.6) (2.32.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorboard->detectron2==0.6) (72.1.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorboard->detectron2==0.6) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorboard->detectron2==0.6) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorboard->detectron2==0.6) (3.0.3)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorboard->detectron2==0.6) (0.44.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.6) (2.0.0)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from grpcio>=1.24.3->tensorboard->detectron2==0.6) (1.16.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib->detectron2==0.6) (3.17.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from markdown>=2.6.8->tensorboard->detectron2==0.6) (7.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (2.1.3)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from portalocker->iopath<0.1.10,>=0.1.7->detectron2==0.6) (305.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.6) (3.2.2)\n",
      "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
      "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Downloading pycocotools-2.0.8-cp39-cp39-win_amd64.whl (85 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Downloading black-25.1.0-cp39-cp39-win_amd64.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.4/1.4 MB 9.3 MB/s eta 0:00:00\n",
      "Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Building wheels for collected packages: detectron2, fvcore, antlr4-python3-runtime\n",
      "  Building wheel for detectron2 (setup.py): started\n",
      "  Building wheel for detectron2 (setup.py): still running...\n",
      "  Building wheel for detectron2 (setup.py): still running...\n",
      "  Building wheel for detectron2 (setup.py): finished with status 'done'\n",
      "  Created wheel for detectron2: filename=detectron2-0.6-cp39-cp39-win_amd64.whl size=887552 sha256=705a1f0c24b8be1945091f3ac8a108d8cb15ee91bfd0d3307437071d06afc418\n",
      "  Stored in directory: C:\\Users\\ajaym\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-baxfex1q\\wheels\\59\\b4\\83\\84bfca751fa4dcc59998468be8688eb50e97408a83af171d42\n",
      "  Building wheel for fvcore (setup.py): started\n",
      "  Building wheel for fvcore (setup.py): finished with status 'done'\n",
      "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61405 sha256=1e21a19c1e249fa5cc3627aeaed243ce4a9dd8bc1bdf7c19a4adf6ecfeac7563\n",
      "  Stored in directory: c:\\users\\ajaym\\appdata\\local\\pip\\cache\\wheels\\83\\42\\02\\66178d16e5c44dc26d309931834956baeda371956e86fbd876\n",
      "  Building wheel for antlr4-python3-runtime (setup.py): started\n",
      "  Building wheel for antlr4-python3-runtime (setup.py): finished with status 'done'\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144577 sha256=062bedb2ac8bb379511532f2a9aa7586ea143ba86642bd2755ff108ac3e4c0ae\n",
      "  Stored in directory: c:\\users\\ajaym\\appdata\\local\\pip\\cache\\wheels\\23\\cf\\80\\f3efa822e6ab23277902ee9165fe772eeb1dfb8014f359020a\n",
      "Successfully built detectron2 fvcore antlr4-python3-runtime\n",
      "Installing collected packages: antlr4-python3-runtime, yacs, tqdm, tabulate, portalocker, pathspec, omegaconf, mypy-extensions, cloudpickle, iopath, hydra-core, black, pycocotools, fvcore, detectron2\n",
      "Successfully installed antlr4-python3-runtime-4.9.3 black-25.1.0 cloudpickle-3.1.1 detectron2-0.6 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.0.0 omegaconf-2.3.0 pathspec-0.12.1 portalocker-3.1.1 pycocotools-2.0.8 tabulate-0.9.0 tqdm-4.67.1 yacs-0.1.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git 'C:\\Users\\ajaym\\AppData\\Local\\Temp\\pip-req-build-h15ujg51'\n"
     ]
    }
   ],
   "source": [
    "%pip install git+https://github.com/facebookresearch/detectron2.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ultralytics\n",
      "  Downloading ultralytics-8.3.74-py3-none-any.whl.metadata (35 kB)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: torch in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from ultralytics) (3.9.2)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from ultralytics) (10.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from ultralytics) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from ultralytics) (1.13.1)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from ultralytics) (5.9.0)\n",
      "Collecting py-cpuinfo (from ultralytics)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from ultralytics) (1.3.5)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from ultralytics) (0.13.2)\n",
      "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
      "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (6.4.5)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib>=3.3.0->ultralytics) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Downloading ultralytics-8.3.74-py3-none-any.whl (914 kB)\n",
      "   ---------------------------------------- 0.0/914.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 914.7/914.7 kB 10.5 MB/s eta 0:00:00\n",
      "Downloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: py-cpuinfo, ultralytics-thop, ultralytics\n",
      "Successfully installed py-cpuinfo-9.0.0 ultralytics-8.3.74 ultralytics-thop-2.0.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ultralytics opencv-python numpy torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n-pose.pt\"\n",
    "response = requests.get(url, stream=True)\n",
    "\n",
    "with open(\"yolov8n-hand.pt\", \"wb\") as file:\n",
    "    for chunk in response.iter_content(chunk_size=8192):\n",
    "        file.write(chunk)\n",
    "\n",
    "print(\"Download complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL - YoLo8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 (no detections), 356.6ms\n",
      "Speed: 9.0ms preprocess, 356.6ms inference, 5.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 203.7ms\n",
      "Speed: 9.0ms preprocess, 203.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 162.7ms\n",
      "Speed: 2.0ms preprocess, 162.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 248.7ms\n",
      "Speed: 3.0ms preprocess, 248.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 152.5ms\n",
      "Speed: 2.0ms preprocess, 152.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 158.1ms\n",
      "Speed: 1.0ms preprocess, 158.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 246.1ms\n",
      "Speed: 3.0ms preprocess, 246.1ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 208.7ms\n",
      "Speed: 3.0ms preprocess, 208.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 181.8ms\n",
      "Speed: 3.0ms preprocess, 181.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 197.6ms\n",
      "Speed: 5.0ms preprocess, 197.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 214.0ms\n",
      "Speed: 3.0ms preprocess, 214.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 209.4ms\n",
      "Speed: 4.0ms preprocess, 209.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 238.3ms\n",
      "Speed: 3.0ms preprocess, 238.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 166.3ms\n",
      "Speed: 1.0ms preprocess, 166.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 237.0ms\n",
      "Speed: 2.9ms preprocess, 237.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 237.5ms\n",
      "Speed: 4.0ms preprocess, 237.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 236.2ms\n",
      "Speed: 3.9ms preprocess, 236.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 241.1ms\n",
      "Speed: 3.1ms preprocess, 241.1ms inference, 7.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 222.3ms\n",
      "Speed: 3.0ms preprocess, 222.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 195.4ms\n",
      "Speed: 4.0ms preprocess, 195.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 243.6ms\n",
      "Speed: 4.0ms preprocess, 243.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 212.0ms\n",
      "Speed: 4.0ms preprocess, 212.0ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 210.1ms\n",
      "Speed: 2.0ms preprocess, 210.1ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 188.6ms\n",
      "Speed: 3.0ms preprocess, 188.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 187.3ms\n",
      "Speed: 2.0ms preprocess, 187.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 266.9ms\n",
      "Speed: 2.0ms preprocess, 266.9ms inference, 4.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 251.4ms\n",
      "Speed: 4.0ms preprocess, 251.4ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 251.7ms\n",
      "Speed: 3.0ms preprocess, 251.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 236.8ms\n",
      "Speed: 3.0ms preprocess, 236.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 263.9ms\n",
      "Speed: 3.0ms preprocess, 263.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.3ms\n",
      "Speed: 2.0ms preprocess, 135.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 160.2ms\n",
      "Speed: 2.0ms preprocess, 160.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 163.8ms\n",
      "Speed: 2.8ms preprocess, 163.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 155.4ms\n",
      "Speed: 2.0ms preprocess, 155.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 152.9ms\n",
      "Speed: 2.0ms preprocess, 152.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 157.8ms\n",
      "Speed: 0.9ms preprocess, 157.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 155.3ms\n",
      "Speed: 2.0ms preprocess, 155.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 165.5ms\n",
      "Speed: 2.0ms preprocess, 165.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 182.9ms\n",
      "Speed: 2.9ms preprocess, 182.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 165.8ms\n",
      "Speed: 3.0ms preprocess, 165.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 157.1ms\n",
      "Speed: 2.0ms preprocess, 157.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 176.7ms\n",
      "Speed: 2.0ms preprocess, 176.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 163.2ms\n",
      "Speed: 5.0ms preprocess, 163.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 177.4ms\n",
      "Speed: 2.0ms preprocess, 177.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 249.0ms\n",
      "Speed: 3.0ms preprocess, 249.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 233.8ms\n",
      "Speed: 3.0ms preprocess, 233.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 250.9ms\n",
      "Speed: 2.0ms preprocess, 250.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 243.5ms\n",
      "Speed: 3.1ms preprocess, 243.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 184.0ms\n",
      "Speed: 2.9ms preprocess, 184.0ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 178.4ms\n",
      "Speed: 1.9ms preprocess, 178.4ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 164.0ms\n",
      "Speed: 1.8ms preprocess, 164.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 108.1ms\n",
      "Speed: 3.0ms preprocess, 108.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 129.9ms\n",
      "Speed: 2.0ms preprocess, 129.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 105.9ms\n",
      "Speed: 2.0ms preprocess, 105.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 148.0ms\n",
      "Speed: 3.0ms preprocess, 148.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 93.8ms\n",
      "Speed: 2.0ms preprocess, 93.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 81.1ms\n",
      "Speed: 2.0ms preprocess, 81.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 136.6ms\n",
      "Speed: 2.0ms preprocess, 136.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 182.3ms\n",
      "Speed: 2.0ms preprocess, 182.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 129.4ms\n",
      "Speed: 2.0ms preprocess, 129.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 131.0ms\n",
      "Speed: 2.0ms preprocess, 131.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 123.8ms\n",
      "Speed: 2.0ms preprocess, 123.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 132.3ms\n",
      "Speed: 2.0ms preprocess, 132.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 162.2ms\n",
      "Speed: 2.0ms preprocess, 162.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 231.1ms\n",
      "Speed: 3.0ms preprocess, 231.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 203.6ms\n",
      "Speed: 3.0ms preprocess, 203.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 198.0ms\n",
      "Speed: 2.1ms preprocess, 198.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 153.2ms\n",
      "Speed: 3.0ms preprocess, 153.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 108.5ms\n",
      "Speed: 2.0ms preprocess, 108.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 98.2ms\n",
      "Speed: 3.0ms preprocess, 98.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 119.3ms\n",
      "Speed: 2.2ms preprocess, 119.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 93.7ms\n",
      "Speed: 2.0ms preprocess, 93.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 82.4ms\n",
      "Speed: 2.0ms preprocess, 82.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 88.6ms\n",
      "Speed: 2.0ms preprocess, 88.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 75.3ms\n",
      "Speed: 2.0ms preprocess, 75.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 129.1ms\n",
      "Speed: 3.0ms preprocess, 129.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 189.7ms\n",
      "Speed: 3.0ms preprocess, 189.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 178.5ms\n",
      "Speed: 2.0ms preprocess, 178.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 157.2ms\n",
      "Speed: 3.0ms preprocess, 157.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 134.0ms\n",
      "Speed: 2.0ms preprocess, 134.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 85.5ms\n",
      "Speed: 2.0ms preprocess, 85.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 87.1ms\n",
      "Speed: 2.0ms preprocess, 87.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 100.7ms\n",
      "Speed: 2.0ms preprocess, 100.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 114.5ms\n",
      "Speed: 2.0ms preprocess, 114.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 92.6ms\n",
      "Speed: 2.0ms preprocess, 92.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 118.9ms\n",
      "Speed: 2.0ms preprocess, 118.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 81.6ms\n",
      "Speed: 2.0ms preprocess, 81.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 115.9ms\n",
      "Speed: 2.0ms preprocess, 115.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 86.7ms\n",
      "Speed: 2.0ms preprocess, 86.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 108.2ms\n",
      "Speed: 2.1ms preprocess, 108.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 114.7ms\n",
      "Speed: 1.0ms preprocess, 114.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 103.8ms\n",
      "Speed: 2.0ms preprocess, 103.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 63.4ms\n",
      "Speed: 2.0ms preprocess, 63.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 71.5ms\n",
      "Speed: 1.0ms preprocess, 71.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 108.1ms\n",
      "Speed: 2.0ms preprocess, 108.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 143.3ms\n",
      "Speed: 3.0ms preprocess, 143.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 93.1ms\n",
      "Speed: 1.0ms preprocess, 93.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 148.3ms\n",
      "Speed: 3.0ms preprocess, 148.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 142.3ms\n",
      "Speed: 2.0ms preprocess, 142.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 131.1ms\n",
      "Speed: 2.0ms preprocess, 131.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 92.3ms\n",
      "Speed: 2.0ms preprocess, 92.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 96.1ms\n",
      "Speed: 2.0ms preprocess, 96.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 166.6ms\n",
      "Speed: 2.0ms preprocess, 166.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 185.7ms\n",
      "Speed: 4.0ms preprocess, 185.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 159.2ms\n",
      "Speed: 3.0ms preprocess, 159.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 230.9ms\n",
      "Speed: 5.0ms preprocess, 230.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 108.9ms\n",
      "Speed: 2.0ms preprocess, 108.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 68.5ms\n",
      "Speed: 2.0ms preprocess, 68.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 131.5ms\n",
      "Speed: 2.0ms preprocess, 131.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 109.8ms\n",
      "Speed: 2.0ms preprocess, 109.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 223.1ms\n",
      "Speed: 3.0ms preprocess, 223.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 248.5ms\n",
      "Speed: 3.0ms preprocess, 248.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 211.7ms\n",
      "Speed: 5.0ms preprocess, 211.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 192.3ms\n",
      "Speed: 4.0ms preprocess, 192.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 160.4ms\n",
      "Speed: 2.9ms preprocess, 160.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 150.8ms\n",
      "Speed: 2.0ms preprocess, 150.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 153.6ms\n",
      "Speed: 3.0ms preprocess, 153.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 162.5ms\n",
      "Speed: 3.0ms preprocess, 162.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 158.4ms\n",
      "Speed: 2.0ms preprocess, 158.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 152.3ms\n",
      "Speed: 2.0ms preprocess, 152.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 96.5ms\n",
      "Speed: 2.0ms preprocess, 96.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 150.0ms\n",
      "Speed: 2.0ms preprocess, 150.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 156.9ms\n",
      "Speed: 3.0ms preprocess, 156.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 138.0ms\n",
      "Speed: 2.0ms preprocess, 138.0ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 101.7ms\n",
      "Speed: 2.0ms preprocess, 101.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 148.9ms\n",
      "Speed: 2.9ms preprocess, 148.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 168.5ms\n",
      "Speed: 2.0ms preprocess, 168.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 157.6ms\n",
      "Speed: 3.0ms preprocess, 157.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 157.5ms\n",
      "Speed: 3.0ms preprocess, 157.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 171.3ms\n",
      "Speed: 3.0ms preprocess, 171.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 165.2ms\n",
      "Speed: 2.0ms preprocess, 165.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 159.8ms\n",
      "Speed: 3.0ms preprocess, 159.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 141.5ms\n",
      "Speed: 1.9ms preprocess, 141.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 180.9ms\n",
      "Speed: 2.0ms preprocess, 180.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 204.1ms\n",
      "Speed: 3.1ms preprocess, 204.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 145.9ms\n",
      "Speed: 3.0ms preprocess, 145.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 203.8ms\n",
      "Speed: 2.1ms preprocess, 203.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 102.1ms\n",
      "Speed: 2.9ms preprocess, 102.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 117.6ms\n",
      "Speed: 2.0ms preprocess, 117.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 164.1ms\n",
      "Speed: 1.0ms preprocess, 164.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 143.0ms\n",
      "Speed: 2.1ms preprocess, 143.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 100.1ms\n",
      "Speed: 2.0ms preprocess, 100.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 152.4ms\n",
      "Speed: 2.0ms preprocess, 152.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 147.1ms\n",
      "Speed: 3.0ms preprocess, 147.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 181.6ms\n",
      "Speed: 2.0ms preprocess, 181.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 198.1ms\n",
      "Speed: 4.0ms preprocess, 198.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 99.4ms\n",
      "Speed: 2.0ms preprocess, 99.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 129.0ms\n",
      "Speed: 1.0ms preprocess, 129.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 165.3ms\n",
      "Speed: 2.0ms preprocess, 165.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 188.2ms\n",
      "Speed: 2.9ms preprocess, 188.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 222.1ms\n",
      "Speed: 2.5ms preprocess, 222.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 176.1ms\n",
      "Speed: 2.0ms preprocess, 176.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 191.4ms\n",
      "Speed: 4.0ms preprocess, 191.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 186.1ms\n",
      "Speed: 3.9ms preprocess, 186.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 182.7ms\n",
      "Speed: 4.0ms preprocess, 182.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 177.0ms\n",
      "Speed: 3.0ms preprocess, 177.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 157.0ms\n",
      "Speed: 3.0ms preprocess, 157.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 152.9ms\n",
      "Speed: 2.9ms preprocess, 152.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 209.9ms\n",
      "Speed: 2.0ms preprocess, 209.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 181.8ms\n",
      "Speed: 2.0ms preprocess, 181.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 178.0ms\n",
      "Speed: 3.0ms preprocess, 178.0ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 160.1ms\n",
      "Speed: 3.0ms preprocess, 160.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 154.0ms\n",
      "Speed: 2.0ms preprocess, 154.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 149.5ms\n",
      "Speed: 3.0ms preprocess, 149.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 147.1ms\n",
      "Speed: 3.0ms preprocess, 147.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 146.7ms\n",
      "Speed: 3.0ms preprocess, 146.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 149.9ms\n",
      "Speed: 2.0ms preprocess, 149.9ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 159.5ms\n",
      "Speed: 2.0ms preprocess, 159.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 149.7ms\n",
      "Speed: 2.0ms preprocess, 149.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 139.9ms\n",
      "Speed: 2.9ms preprocess, 139.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 155.5ms\n",
      "Speed: 3.0ms preprocess, 155.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 151.3ms\n",
      "Speed: 1.9ms preprocess, 151.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 115.9ms\n",
      "Speed: 3.0ms preprocess, 115.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 162.1ms\n",
      "Speed: 2.0ms preprocess, 162.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 211.8ms\n",
      "Speed: 3.0ms preprocess, 211.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 171.0ms\n",
      "Speed: 3.0ms preprocess, 171.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 215.0ms\n",
      "Speed: 2.9ms preprocess, 215.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 168.3ms\n",
      "Speed: 3.0ms preprocess, 168.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 146.9ms\n",
      "Speed: 2.0ms preprocess, 146.9ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 112.2ms\n",
      "Speed: 2.9ms preprocess, 112.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 206.7ms\n",
      "Speed: 2.0ms preprocess, 206.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 202.3ms\n",
      "Speed: 1.9ms preprocess, 202.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 208.6ms\n",
      "Speed: 2.0ms preprocess, 208.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 162.3ms\n",
      "Speed: 3.0ms preprocess, 162.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 192.3ms\n",
      "Speed: 2.0ms preprocess, 192.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 183.8ms\n",
      "Speed: 2.0ms preprocess, 183.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 117.1ms\n",
      "Speed: 2.0ms preprocess, 117.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 196.4ms\n",
      "Speed: 4.0ms preprocess, 196.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 220.1ms\n",
      "Speed: 3.0ms preprocess, 220.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 188.2ms\n",
      "Speed: 3.0ms preprocess, 188.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 182.4ms\n",
      "Speed: 3.0ms preprocess, 182.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 180.1ms\n",
      "Speed: 2.0ms preprocess, 180.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 186.5ms\n",
      "Speed: 2.0ms preprocess, 186.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 223.8ms\n",
      "Speed: 3.0ms preprocess, 223.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 149.5ms\n",
      "Speed: 2.0ms preprocess, 149.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 151.6ms\n",
      "Speed: 3.0ms preprocess, 151.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 118.9ms\n",
      "Speed: 2.0ms preprocess, 118.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 137.8ms\n",
      "Speed: 2.0ms preprocess, 137.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 181.3ms\n",
      "Speed: 2.0ms preprocess, 181.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 209.3ms\n",
      "Speed: 2.9ms preprocess, 209.3ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 249.5ms\n",
      "Speed: 3.3ms preprocess, 249.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 268.5ms\n",
      "Speed: 4.0ms preprocess, 268.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 225.1ms\n",
      "Speed: 2.9ms preprocess, 225.1ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 164.9ms\n",
      "Speed: 3.0ms preprocess, 164.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 166.1ms\n",
      "Speed: 2.0ms preprocess, 166.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 238.1ms\n",
      "Speed: 3.0ms preprocess, 238.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 193.9ms\n",
      "Speed: 3.0ms preprocess, 193.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 158.7ms\n",
      "Speed: 4.0ms preprocess, 158.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 176.6ms\n",
      "Speed: 2.9ms preprocess, 176.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 88.1ms\n",
      "Speed: 2.1ms preprocess, 88.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 97.8ms\n",
      "Speed: 2.0ms preprocess, 97.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 100.5ms\n",
      "Speed: 0.9ms preprocess, 100.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 206.4ms\n",
      "Speed: 2.0ms preprocess, 206.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 227.2ms\n",
      "Speed: 3.9ms preprocess, 227.2ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 202.2ms\n",
      "Speed: 3.0ms preprocess, 202.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 214.6ms\n",
      "Speed: 2.0ms preprocess, 214.6ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 208.0ms\n",
      "Speed: 4.0ms preprocess, 208.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 201.2ms\n",
      "Speed: 2.0ms preprocess, 201.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 201.3ms\n",
      "Speed: 3.0ms preprocess, 201.3ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 233.8ms\n",
      "Speed: 3.9ms preprocess, 233.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 216.7ms\n",
      "Speed: 2.0ms preprocess, 216.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 211.0ms\n",
      "Speed: 3.0ms preprocess, 211.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 202.7ms\n",
      "Speed: 4.0ms preprocess, 202.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 181.3ms\n",
      "Speed: 3.0ms preprocess, 181.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 184.2ms\n",
      "Speed: 2.0ms preprocess, 184.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 221.6ms\n",
      "Speed: 3.0ms preprocess, 221.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 167.5ms\n",
      "Speed: 2.0ms preprocess, 167.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLOv8 Hand Detection Model (Pre-trained)\n",
    "model = YOLO(\"yolov8n-hand.pt\")  # Download trained YOLO model for hands\n",
    "\n",
    "# Start webcam capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Run YOLO hand detection\n",
    "    results = model(frame)\n",
    "\n",
    "    # Create an empty mask\n",
    "    hand_mask = np.zeros_like(frame[:, :, 0])\n",
    "\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])  # Get bounding box coordinates\n",
    "            hand_mask[y1:y2, x1:x2] = 255  # Fill detected hand region\n",
    "\n",
    "    # Apply mask on original frame\n",
    "    segmented_hand = cv2.bitwise_and(frame, frame, mask=hand_mask)\n",
    "\n",
    "    # Show results\n",
    "    cv2.imshow(\"Original\", frame)\n",
    "    cv2.imshow(\"YOLO Hand Segmentation\", segmented_hand)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "from torchvision.models.segmentation import deeplabv3_resnet101\n",
    "\n",
    "# Load DeepLabV3+ Model\n",
    "model = deeplabv3_resnet101(pretrained=True).eval()\n",
    "\n",
    "# Define preprocessing transforms\n",
    "transform = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize((512, 512)),  # Model expects (512, 512)\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Resize the frame to (512, 512) for DeepLabV3\n",
    "    resized_frame = cv2.resize(rgb_frame, (512, 512))\n",
    "\n",
    "    # Convert to tensor & process\n",
    "    input_tensor = transform(resized_frame).unsqueeze(0)\n",
    "\n",
    "    # Run segmentation\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)['out'][0]\n",
    "\n",
    "    # Convert output to mask\n",
    "    skin_mask = output.argmax(0).byte().cpu().numpy()\n",
    "\n",
    "    # Resize the mask back to the original frame size\n",
    "    skin_mask = cv2.resize(skin_mask, (frame.shape[1], frame.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # Convert to binary mask\n",
    "    skin_mask = (skin_mask == 1).astype(np.uint8) * 255\n",
    "\n",
    "    # Convert mask to 3-channel to match the frame\n",
    "    skin_mask_3ch = cv2.cvtColor(skin_mask, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    # Apply mask on the original frame\n",
    "    segmented_skin = cv2.bitwise_and(frame, skin_mask_3ch)\n",
    "\n",
    "    # Show results\n",
    "    cv2.imshow(\"Original\", frame)\n",
    "    cv2.imshow(\"Skin Segmentation\", segmented_skin)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 BodyPix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "https://tfhub.dev/google/tfjs-model/bodypix/resnet50/float/1 does not appear to be a valid module.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ajaym\\anaconda3\\envs\\F21DL\\lib\\tarfile.py:191\u001b[0m, in \u001b[0;36mnti\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    190\u001b[0m     s \u001b[38;5;241m=\u001b[39m nts(s, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mascii\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 191\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 8: 'ts\" cont'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidHeaderError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ajaym\\anaconda3\\envs\\F21DL\\lib\\tarfile.py:2588\u001b[0m, in \u001b[0;36mTarFile.next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2587\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2588\u001b[0m     tarinfo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarinfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromtarfile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2589\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EOFHeaderError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\ajaym\\anaconda3\\envs\\F21DL\\lib\\tarfile.py:1292\u001b[0m, in \u001b[0;36mTarInfo.fromtarfile\u001b[1;34m(cls, tarfile)\u001b[0m\n\u001b[0;32m   1291\u001b[0m buf \u001b[38;5;241m=\u001b[39m tarfile\u001b[38;5;241m.\u001b[39mfileobj\u001b[38;5;241m.\u001b[39mread(BLOCKSIZE)\n\u001b[1;32m-> 1292\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrombuf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1293\u001b[0m obj\u001b[38;5;241m.\u001b[39moffset \u001b[38;5;241m=\u001b[39m tarfile\u001b[38;5;241m.\u001b[39mfileobj\u001b[38;5;241m.\u001b[39mtell() \u001b[38;5;241m-\u001b[39m BLOCKSIZE\n",
      "File \u001b[1;32mc:\\Users\\ajaym\\anaconda3\\envs\\F21DL\\lib\\tarfile.py:1234\u001b[0m, in \u001b[0;36mTarInfo.frombuf\u001b[1;34m(cls, buf, encoding, errors)\u001b[0m\n\u001b[0;32m   1232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m EOFHeaderError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend of file header\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1234\u001b[0m chksum \u001b[38;5;241m=\u001b[39m \u001b[43mnti\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m148\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m156\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chksum \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m calc_chksums(buf):\n",
      "File \u001b[1;32mc:\\Users\\ajaym\\anaconda3\\envs\\F21DL\\lib\\tarfile.py:193\u001b[0m, in \u001b[0;36mnti\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m--> 193\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidHeaderError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid header\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n\n",
      "\u001b[1;31mInvalidHeaderError\u001b[0m: invalid header",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mReadError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ajaym\\anaconda3\\envs\\F21DL\\lib\\site-packages\\tensorflow_hub\\resolver.py:192\u001b[0m, in \u001b[0;36mDownloadManager.download_and_uncompress\u001b[1;34m(self, fileobj, dst_path)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 192\u001b[0m   \u001b[43mfile_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_tarfile_to_destination\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_progress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m   total_size_str \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39mbytes_to_readable_str(\n\u001b[0;32m    195\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_total_bytes_downloaded, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\ajaym\\anaconda3\\envs\\F21DL\\lib\\site-packages\\tensorflow_hub\\file_utils.py:47\u001b[0m, in \u001b[0;36mextract_tarfile_to_destination\u001b[1;34m(fileobj, dst_path, log_function)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Extract a tarfile. Optional: log the progress.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtarfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr|*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfileobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfileobj\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m tgz:\n\u001b[0;32m     48\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m tarinfo \u001b[38;5;129;01min\u001b[39;00m tgz:\n",
      "File \u001b[1;32mc:\\Users\\ajaym\\anaconda3\\envs\\F21DL\\lib\\tarfile.py:1822\u001b[0m, in \u001b[0;36mTarFile.open\u001b[1;34m(cls, name, mode, fileobj, bufsize, **kwargs)\u001b[0m\n\u001b[0;32m   1821\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1822\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(name, filemode, stream, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1823\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ajaym\\anaconda3\\envs\\F21DL\\lib\\tarfile.py:1703\u001b[0m, in \u001b[0;36mTarFile.__init__\u001b[1;34m(self, name, mode, fileobj, format, tarinfo, dereference, ignore_zeros, encoding, errors, pax_headers, debug, errorlevel, copybufsize)\u001b[0m\n\u001b[0;32m   1702\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfirstmember \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1703\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfirstmember \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1706\u001b[0m     \u001b[38;5;66;03m# Move to the end of the archive,\u001b[39;00m\n\u001b[0;32m   1707\u001b[0m     \u001b[38;5;66;03m# before the first empty block.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ajaym\\anaconda3\\envs\\F21DL\\lib\\tarfile.py:2600\u001b[0m, in \u001b[0;36mTarFile.next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2599\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffset \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2600\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ReadError(\u001b[38;5;28mstr\u001b[39m(e))\n\u001b[0;32m   2601\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EmptyHeaderError:\n",
      "\u001b[1;31mReadError\u001b[0m: invalid header",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m model_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://tfhub.dev/google/tfjs-model/bodypix/resnet50/float/1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel loaded successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msegment_hand\u001b[39m(frame):\n",
      "File \u001b[1;32mc:\\Users\\ajaym\\anaconda3\\envs\\F21DL\\lib\\site-packages\\tensorflow_hub\\module_v2.py:100\u001b[0m, in \u001b[0;36mload\u001b[1;34m(handle, tags, options)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     99\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a string, got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m handle)\n\u001b[1;32m--> 100\u001b[0m module_path \u001b[38;5;241m=\u001b[39m \u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m is_hub_module_v1 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(_get_module_proto_path(module_path))\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tags \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_hub_module_v1:\n",
      "File \u001b[1;32mc:\\Users\\ajaym\\anaconda3\\envs\\F21DL\\lib\\site-packages\\tensorflow_hub\\module_v2.py:55\u001b[0m, in \u001b[0;36mresolve\u001b[1;34m(handle)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresolve\u001b[39m(handle):\n\u001b[0;32m     32\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Resolves a module handle into a path.\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03m  This function works both for plain TF2 SavedModels and the legacy TF1 Hub\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m    A string representing the Module path.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ajaym\\anaconda3\\envs\\F21DL\\lib\\site-packages\\tensorflow_hub\\registry.py:49\u001b[0m, in \u001b[0;36mMultiImplRegister.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m impl \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impls):\n\u001b[0;32m     48\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mis_supported(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     50\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     fails\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mtype\u001b[39m(impl)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ajaym\\anaconda3\\envs\\F21DL\\lib\\site-packages\\tensorflow_hub\\compressed_module_resolver.py:81\u001b[0m, in \u001b[0;36mHttpCompressedFileResolver.__call__\u001b[1;34m(self, handle)\u001b[0m\n\u001b[0;32m     77\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_urlopen(request)\n\u001b[0;32m     78\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m resolver\u001b[38;5;241m.\u001b[39mDownloadManager(handle)\u001b[38;5;241m.\u001b[39mdownload_and_uncompress(\n\u001b[0;32m     79\u001b[0m       response, tmp_dir)\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matomic_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lock_file_timeout_sec\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ajaym\\anaconda3\\envs\\F21DL\\lib\\site-packages\\tensorflow_hub\\resolver.py:421\u001b[0m, in \u001b[0;36matomic_download\u001b[1;34m(handle, download_fn, module_dir, lock_file_timeout_sec)\u001b[0m\n\u001b[0;32m    419\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading TF-Hub Module \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, handle)\n\u001b[0;32m    420\u001b[0m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mMakeDirs(tmp_dir)\n\u001b[1;32m--> 421\u001b[0m \u001b[43mdownload_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmp_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;66;03m# Write module descriptor to capture information about which module was\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;66;03m# downloaded by whom and when. The file stored at the same level as a\u001b[39;00m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;66;03m# directory in order to keep the content of the 'model_dir' exactly as it\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;66;03m# module caching protocol and no code in the TF-Hub library reads its\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# content.\u001b[39;00m\n\u001b[0;32m    431\u001b[0m _write_module_descriptor_file(handle, module_dir)\n",
      "File \u001b[1;32mc:\\Users\\ajaym\\anaconda3\\envs\\F21DL\\lib\\site-packages\\tensorflow_hub\\compressed_module_resolver.py:78\u001b[0m, in \u001b[0;36mHttpCompressedFileResolver.__call__.<locals>.download\u001b[1;34m(handle, tmp_dir)\u001b[0m\n\u001b[0;32m     75\u001b[0m request \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_compressed_format_query(handle))\n\u001b[0;32m     77\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_urlopen(request)\n\u001b[1;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDownloadManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_uncompress\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmp_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ajaym\\anaconda3\\envs\\F21DL\\lib\\site-packages\\tensorflow_hub\\resolver.py:200\u001b[0m, in \u001b[0;36mDownloadManager.download_and_uncompress\u001b[1;34m(self, fileobj, dst_path)\u001b[0m\n\u001b[0;32m    196\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_download_progress_msg(\n\u001b[0;32m    197\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloaded \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, Total size: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url, total_size_str),\n\u001b[0;32m    198\u001b[0m       flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m tarfile\u001b[38;5;241m.\u001b[39mReadError:\n\u001b[1;32m--> 200\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m does not appear to be a valid module.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url)\n",
      "\u001b[1;31mOSError\u001b[0m: https://tfhub.dev/google/tfjs-model/bodypix/resnet50/float/1 does not appear to be a valid module."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Load the BodyPix model\n",
    "model_url = \"https://tfhub.dev/google/tfjs-model/bodypix/resnet50/float/1\"\n",
    "print(\"Loading model...\")\n",
    "model = hub.load(model_url)\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "def segment_hand(frame):\n",
    "    \"\"\"\n",
    "    Segments only the hand from the given frame using BodyPix.\n",
    "    \"\"\"\n",
    "    # Resize and normalize the image for the model\n",
    "    input_image = cv2.resize(frame, (257, 257)) / 255.0\n",
    "    input_tensor = tf.convert_to_tensor(input_image, dtype=tf.float32)\n",
    "    input_tensor = tf.expand_dims(input_tensor, axis=0)\n",
    "\n",
    "    # Run inference\n",
    "    output = model.signatures[\"serving_default\"](input_tensor)\n",
    "    segmentation_mask = output['float_segments'][0].numpy()\n",
    "\n",
    "    # Resize mask to match original frame size\n",
    "    segmentation_mask = cv2.resize(segmentation_mask, (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "    # Threshold the segmentation mask (adjust threshold for best results)\n",
    "    hand_mask = (segmentation_mask > 0.9).astype(np.uint8) * 255\n",
    "\n",
    "    # Apply mask to extract hand\n",
    "    hand_segmented = cv2.bitwise_and(frame, frame, mask=hand_mask)\n",
    "\n",
    "    # Create black background\n",
    "    background = np.zeros_like(frame)\n",
    "    hand_only = np.where(hand_mask[:, :, None] == 255, hand_segmented, background)\n",
    "\n",
    "    return hand_only\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Flip frame horizontally for natural mirroring\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Process frame\n",
    "    hand_only = segment_hand(frame)\n",
    "\n",
    "    # Display original and segmented frames\n",
    "    cv2.imshow(\"Original Frame\", frame)\n",
    "    cv2.im\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting segment-anything\n",
      "  Downloading segment_anything-1.0-py3-none-any.whl.metadata (487 bytes)\n",
      "Downloading segment_anything-1.0-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: segment-anything\n",
      "Successfully installed segment-anything-1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
      "  Cloning https://github.com/facebookresearch/segment-anything.git to c:\\users\\ajaym\\appdata\\local\\temp\\pip-req-build-htt1bqdo\n",
      "  Resolved https://github.com/facebookresearch/segment-anything.git to commit dca509fe793f601edb92606367a655c15ac00fdf\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git 'C:\\Users\\ajaym\\AppData\\Local\\Temp\\pip-req-build-htt1bqdo'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnxruntime\n",
      "  Downloading onnxruntime-1.19.2-cp39-cp39-win_amd64.whl.metadata (4.7 kB)\n",
      "Collecting coloredlogs (from onnxruntime)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from onnxruntime) (24.3.25)\n",
      "Requirement already satisfied: numpy>=1.21.6 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from onnxruntime) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from onnxruntime) (24.1)\n",
      "Requirement already satisfied: protobuf in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from onnxruntime) (3.19.6)\n",
      "Requirement already satisfied: sympy in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from onnxruntime) (1.13.1)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime)\n",
      "  Downloading pyreadline3-3.5.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading onnxruntime-1.19.2-cp39-cp39-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.0/11.1 MB 7.2 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.8/11.1 MB 4.4 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.4/11.1 MB 3.6 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.1/11.1 MB 4.0 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.1/11.1 MB 4.0 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.1/11.1 MB 4.0 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.1/11.1 MB 4.0 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.5/11.1 MB 2.6 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 5.2/11.1 MB 2.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.3/11.1 MB 3.1 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 7.9/11.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.4/11.1 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.4/11.1 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.7/11.1 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 3.7 MB/s eta 0:00:00\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Downloading pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: pyreadline3, humanfriendly, coloredlogs, onnxruntime\n",
      "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.19.2 pyreadline3-3.5.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio\n",
    "%pip install segment-anything\n",
    "%pip install git+https://github.com/facebookresearch/segment-anything.git\n",
    "%pip install onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "import onnxruntime\n",
    "\n",
    "# Load the pre-trained SAM model (ViT-H)\n",
    "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the SAM model\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device)\n",
    "predictor = SamPredictor(sam)\n",
    "\n",
    "def segment_hand_sam(frame):\n",
    "    \"\"\"Segment hand using SAM and return a black background image with only the hand visible.\"\"\"\n",
    "    \n",
    "    # Convert frame to RGB for SAM\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Set SAM input image\n",
    "    predictor.set_image(image_rgb)\n",
    "    \n",
    "    # Use a central point as a prompt (adjust for better results)\n",
    "    h, w, _ = frame.shape\n",
    "    input_point = np.array([[w // 2, h // 2]])  # Assume hand is centered\n",
    "    input_label = np.array([1])  # 1 means foreground (hand)\n",
    "    \n",
    "    # Predict mask using SAM\n",
    "    masks, _, _ = predictor.predict(point_coords=input_point, point_labels=input_label, multimask_output=False)\n",
    "    mask = masks[0]  # Use the first mask\n",
    "    \n",
    "    # Resize mask to match frame size\n",
    "    mask_resized = cv2.resize(mask.astype(np.uint8), (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    # Apply mask: Set background to black\n",
    "    segmented_hand = cv2.bitwise_and(frame, frame, mask=mask_resized)\n",
    "    \n",
    "    return segmented_hand\n",
    "\n",
    "# Open Webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Apply hand segmentation\n",
    "    segmented_hand = segment_hand_sam(frame)\n",
    "\n",
    "    # Show result\n",
    "    cv2.imshow(\"Real-Time Hand Segmentation (SAM)\", segmented_hand)\n",
    "\n",
    "    # Exit on 'q' key\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import (\n",
    "    MobileNet, MobileNetV2, MobileNetV3Small, MobileNetV3Large,\n",
    "    ResNet50, EfficientNetB0, Xception, DenseNet121, VGG16, VGG19,\n",
    "    NASNetMobile, InceptionV3\n",
    ")\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import LeakyReLU, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from datetime import datetime\n",
    "import json\n",
    "import threading\n",
    "\n",
    "# For evaluation metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# For plotting\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For UI\n",
    "import tkinter as tk\n",
    "from tkinter import Tk, Label, Button, filedialog, StringVar, DoubleVar, IntVar, OptionMenu, Entry, Checkbutton, BooleanVar\n",
    "from tkinter import messagebox\n",
    "from tkinter import scrolledtext\n",
    "from tkinter import END\n",
    "from tkinter import N, S, E, W\n",
    "from tkinter import Listbox\n",
    "from tkinter import MULTIPLE\n",
    "from tkinter import VERTICAL\n",
    "from tkinter import HORIZONTAL\n",
    "from tkinter import Scrollbar\n",
    "from tkinter import RIGHT, LEFT, Y, BOTH\n",
    "from tkinter import Frame\n",
    "from tkinter import TOP, BOTTOM\n",
    "from tkinter import X\n",
    "from tkinter import Y\n",
    "\n",
    "# Use themed widgets for a modern look.\n",
    "from tkinter import ttk\n",
    "\n",
    "# ---------------------------\n",
    "# Grad-CAM helper functions\n",
    "# ---------------------------\n",
    "def find_last_conv_layer(model):\n",
    "    \"\"\"Finds the name of the last Conv2D layer in the model.\"\"\"\n",
    "    for layer in reversed(model.layers):\n",
    "        if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "            return layer.name\n",
    "    return None\n",
    "\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    \"\"\"Generates a Grad-CAM heatmap for a given image array and model.\"\"\"\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(predictions[0])\n",
    "        loss = predictions[:, pred_index]\n",
    "\n",
    "    grads = tape.gradient(loss, conv_outputs)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "    conv_outputs = conv_outputs[0]\n",
    "    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "    # Normalize the heatmap between 0 and 1\n",
    "    heatmap = tf.maximum(heatmap, 0) / (tf.math.reduce_max(heatmap) + tf.keras.backend.epsilon())\n",
    "    return heatmap.numpy()\n",
    "\n",
    "def save_gradcam_overlay(img_path, heatmap, output_path, alpha=0.4):\n",
    "    \"\"\"Overlays the heatmap on the original image and saves the result.\"\"\"\n",
    "    import cv2\n",
    "    img = cv2.imread(img_path)\n",
    "    # Convert BGR to RGB\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    superimposed_img = heatmap * alpha + img\n",
    "    superimposed_img = np.uint8(superimposed_img)\n",
    "    plt.imsave(output_path, superimposed_img)\n",
    "\n",
    "# ---------------------------\n",
    "# Main Application Class\n",
    "# ---------------------------\n",
    "class ASLTrainerApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"ASL Image Trainer\")\n",
    "        self.root.geometry(\"800x800\")\n",
    "        self.root.minsize(600, 600)\n",
    "\n",
    "        # ---------------------------\n",
    "        # Variables\n",
    "        # ---------------------------\n",
    "        self.dataset_path = StringVar()\n",
    "        self.test_dataset_path = StringVar()\n",
    "        self.model_name = StringVar(value=\"BasicCNN\")\n",
    "        self.epochs = IntVar(value=50)\n",
    "        self.batch_size = IntVar(value=16)\n",
    "        self.learning_rate = DoubleVar(value=0.001)\n",
    "        self.fine_tune = BooleanVar(value=False)\n",
    "        self.use_cross_validation = BooleanVar(value=False)\n",
    "        self.num_folds = IntVar(value=5)\n",
    "        # New options:\n",
    "        self.use_separate_test = BooleanVar(value=False)\n",
    "        self.split_ratio = StringVar(value=\"80-20\")\n",
    "        self.train_sequentially = BooleanVar(value=False)\n",
    "\n",
    "        # Mapping for split ratios\n",
    "        self.split_options = [\"80-20\", \"75-25\", \"70-30\"]\n",
    "        self.split_mapping = {\"80-20\": 0.2, \"75-25\": 0.25, \"70-30\": 0.3}\n",
    "\n",
    "        # Updated models dictionary with MobileNetV3 and VGG19 added.\n",
    "        self.models = {\n",
    "            \"BasicCNN\": self.build_basic_cnn,\n",
    "            \"MobileNet\": MobileNet,\n",
    "            \"MobileNetV2\": MobileNetV2,\n",
    "            \"MobileNetV3Small\": MobileNetV3Small,\n",
    "            \"MobileNetV3Large\": MobileNetV3Large,\n",
    "            \"EfficientNetB0\": EfficientNetB0,\n",
    "            \"ResNet50\": ResNet50,\n",
    "            \"Xception\": Xception,\n",
    "            \"DenseNet121\": DenseNet121,\n",
    "            \"VGG16\": VGG16,\n",
    "            \"VGG19\": VGG19,\n",
    "            \"NASNetMobile\": NASNetMobile,\n",
    "            \"InceptionV3\": InceptionV3\n",
    "        }\n",
    "\n",
    "        # ---------------------------\n",
    "        # Layout Setup\n",
    "        # ---------------------------\n",
    "        # Create a main frame to hold the left (sequential queue) and right (controls) panels.\n",
    "        self.main_frame = ttk.Frame(root)\n",
    "        self.main_frame.pack(fill='both', expand=True, padx=10, pady=10)\n",
    "\n",
    "        # Left panel: Sequential Training Queue\n",
    "        self.left_frame = ttk.Frame(self.main_frame)\n",
    "        self.left_frame.pack(side='left', fill='y', padx=10, pady=10)\n",
    "        ttk.Label(self.left_frame, text=\"Sequential Training Queue\", font=(\"Arial\", 12, \"bold\")).pack(pady=5)\n",
    "        self.queue_tree = ttk.Treeview(self.left_frame, columns=(\"Model\", \"Status\"), show='headings', height=10)\n",
    "        self.queue_tree.heading(\"Model\", text=\"Model\")\n",
    "        self.queue_tree.heading(\"Status\", text=\"Status\")\n",
    "        self.queue_tree.column(\"Model\", width=120)\n",
    "        self.queue_tree.column(\"Status\", width=120)\n",
    "        self.queue_tree.pack(pady=5)\n",
    "\n",
    "        # Right panel: Training Options and Controls\n",
    "        self.right_frame = ttk.Frame(self.main_frame)\n",
    "        self.right_frame.pack(side='left', fill='both', expand=True, padx=10, pady=10)\n",
    "\n",
    "        # Title and dataset selection\n",
    "        ttk.Label(self.right_frame, text=\"ASL Image Trainer\", font=(\"Arial\", 16, \"bold\")).pack(pady=10)\n",
    "        ttk.Button(self.right_frame, text=\"Select Training Dataset\", command=self.select_dataset).pack(pady=5)\n",
    "        ttk.Label(self.right_frame, textvariable=self.dataset_path).pack(pady=5)\n",
    "\n",
    "        # Option: Use separate test dataset\n",
    "        ttk.Checkbutton(self.right_frame, text=\"Use Separate Test Dataset\", variable=self.use_separate_test, command=self.toggle_test_dataset).pack(pady=5)\n",
    "        self.test_dataset_button = ttk.Button(self.right_frame, text=\"Select Test Dataset\", command=self.select_test_dataset)\n",
    "        self.test_dataset_label = ttk.Label(self.right_frame, textvariable=self.test_dataset_path)\n",
    "        # Initially hidden if not selected:\n",
    "        if self.use_separate_test.get():\n",
    "            self.test_dataset_button.pack(pady=5)\n",
    "            self.test_dataset_label.pack(pady=5)\n",
    "\n",
    "        # Train-Test Split Ratio\n",
    "        ttk.Label(self.right_frame, text=\"Train-Test Split Ratio:\").pack(pady=5)\n",
    "        self.split_dropdown = ttk.OptionMenu(self.right_frame, self.split_ratio, self.split_ratio.get(), *self.split_options)\n",
    "        self.split_dropdown.pack(pady=5)\n",
    "\n",
    "        # Option: Train models sequentially\n",
    "        ttk.Checkbutton(self.right_frame, text=\"Train Models Sequentially\", variable=self.train_sequentially, command=self.toggle_training_mode).pack(pady=5)\n",
    "\n",
    "        # A frame that will contain either single model training options or sequential training UI.\n",
    "        self.training_mode_frame = ttk.Frame(self.right_frame)\n",
    "        self.training_mode_frame.pack(fill='both', expand=True)\n",
    "\n",
    "        # Single Model Training UI\n",
    "        self.single_model_frame = ttk.Frame(self.training_mode_frame)\n",
    "        self.single_model_frame.pack(fill='both', expand=True)\n",
    "        ttk.Label(self.single_model_frame, text=\"Select Model:\").pack(pady=5)\n",
    "        self.model_dropdown = ttk.OptionMenu(self.single_model_frame, self.model_name, self.model_name.get(), *self.models.keys())\n",
    "        self.model_dropdown.pack(pady=5)\n",
    "        ttk.Label(self.single_model_frame, text=\"Epochs:\").pack(pady=5)\n",
    "        ttk.Entry(self.single_model_frame, textvariable=self.epochs).pack(pady=5)\n",
    "        ttk.Label(self.single_model_frame, text=\"Batch Size:\").pack(pady=5)\n",
    "        ttk.Entry(self.single_model_frame, textvariable=self.batch_size).pack(pady=5)\n",
    "        ttk.Label(self.single_model_frame, text=\"Learning Rate:\").pack(pady=5)\n",
    "        ttk.Entry(self.single_model_frame, textvariable=self.learning_rate).pack(pady=5)\n",
    "        ttk.Checkbutton(self.single_model_frame, text=\"Fine-tune Model\", variable=self.fine_tune).pack(pady=5)\n",
    "        ttk.Checkbutton(self.single_model_frame, text=\"Use Cross Validation\", variable=self.use_cross_validation).pack(pady=5)\n",
    "        ttk.Label(self.single_model_frame, text=\"Number of Folds:\").pack(pady=5)\n",
    "        ttk.Entry(self.single_model_frame, textvariable=self.num_folds).pack(pady=5)\n",
    "        ttk.Button(self.single_model_frame, text=\"Train Model\", command=self.train_model).pack(pady=10)\n",
    "\n",
    "        # Sequential Training UI (hidden by default)\n",
    "        self.sequential_model_frame = ttk.Frame(self.training_mode_frame)\n",
    "        # Initially hidden:\n",
    "        self.sequential_model_frame.pack_forget()\n",
    "        ttk.Label(self.sequential_model_frame, text=\"Select Models for Sequential Training:\").pack(pady=5)\n",
    "        self.seq_listbox = Listbox(self.sequential_model_frame, selectmode=MULTIPLE, height=6)\n",
    "        for model_key in self.models.keys():\n",
    "            self.seq_listbox.insert(END, model_key)\n",
    "        self.seq_listbox.pack(pady=5, fill='x')\n",
    "        ttk.Button(self.sequential_model_frame, text=\"Train Selected Models Sequentially\", command=self.train_models_sequentially).pack(pady=10)\n",
    "\n",
    "        # Bottom panel: Log, Timer, and Progress Bar\n",
    "        self.bottom_frame = ttk.Frame(root)\n",
    "        self.bottom_frame.pack(side='bottom', fill='x', padx=10, pady=10)\n",
    "        self.timer_label = ttk.Label(self.bottom_frame, text=\"Timer: 00:00:00\")\n",
    "        self.timer_label.pack(side='left', padx=5)\n",
    "        self.progress_bar = ttk.Progressbar(self.bottom_frame, orient='horizontal', length=200, mode='determinate')\n",
    "        self.progress_bar.pack(side='left', padx=5)\n",
    "        self.log_text = tk.Text(self.bottom_frame, height=5)\n",
    "        self.log_text.pack(fill='x', padx=5, pady=5)\n",
    "\n",
    "    # ---------------------------\n",
    "    # UI Helper Methods\n",
    "    # ---------------------------\n",
    "    def toggle_test_dataset(self):\n",
    "        if self.use_separate_test.get():\n",
    "            self.test_dataset_button.pack(pady=5)\n",
    "            self.test_dataset_label.pack(pady=5)\n",
    "        else:\n",
    "            self.test_dataset_button.pack_forget()\n",
    "            self.test_dataset_label.pack_forget()\n",
    "\n",
    "    def toggle_training_mode(self):\n",
    "        if self.train_sequentially.get():\n",
    "            self.single_model_frame.pack_forget()\n",
    "            self.sequential_model_frame.pack(fill='both', expand=True)\n",
    "        else:\n",
    "            self.sequential_model_frame.pack_forget()\n",
    "            self.single_model_frame.pack(fill='both', expand=True)\n",
    "\n",
    "    def log(self, message):\n",
    "        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        self.log_text.insert(END, f\"{timestamp} - {message}\\n\")\n",
    "        self.log_text.see(END)\n",
    "\n",
    "    def update_queue_status(self, model, status):\n",
    "        # Update the Treeview for the given model (iid is the model name)\n",
    "        self.queue_tree.set(model, \"Status\", status)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Dataset Selection\n",
    "    # ---------------------------\n",
    "    def select_dataset(self):\n",
    "        path = filedialog.askdirectory()\n",
    "        if path:\n",
    "            self.dataset_path.set(path)\n",
    "\n",
    "    def select_test_dataset(self):\n",
    "        path = filedialog.askdirectory()\n",
    "        if path:\n",
    "            self.test_dataset_path.set(path)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Model Building\n",
    "    # ---------------------------\n",
    "    def build_basic_cnn(self, input_shape, num_classes):\n",
    "        model = Sequential([\n",
    "            Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001)),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Flatten(),\n",
    "            Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "            Dropout(0.5),\n",
    "            Dense(128),\n",
    "            LeakyReLU(alpha=0.1),\n",
    "            Dropout(0.4),\n",
    "            Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "        return model\n",
    "\n",
    "    def build_model(self, model_name, input_shape, num_classes, fine_tune):\n",
    "        if model_name == \"BasicCNN\":\n",
    "            return self.build_basic_cnn(input_shape, num_classes)\n",
    "        else:\n",
    "            base_model = self.models[model_name](weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "            if not fine_tune:\n",
    "                for layer in base_model.layers:\n",
    "                    layer.trainable = False\n",
    "            x = base_model.output\n",
    "            x = GlobalAveragePooling2D()(x)\n",
    "            x = Dense(128, activation=\"relu\")(x)\n",
    "            predictions = Dense(num_classes, activation=\"softmax\")(x)\n",
    "            return Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Training Methods\n",
    "    # ---------------------------\n",
    "    def train_model(self):\n",
    "        if not self.dataset_path.get():\n",
    "            self.log(\"Please select a training dataset folder.\")\n",
    "            return\n",
    "        # Start training in a new thread (single model training)\n",
    "        training_thread = threading.Thread(target=self._train_model_thread)\n",
    "        training_thread.start()\n",
    "\n",
    "    def _train_model_thread(self, model_name_override=None):\n",
    "        # Use the override if provided (for sequential training)\n",
    "        model_name = model_name_override if model_name_override is not None else self.model_name.get()\n",
    "        epochs = self.epochs.get()\n",
    "        batch_size = self.batch_size.get()\n",
    "        learning_rate = self.learning_rate.get()\n",
    "        fine_tune = self.fine_tune.get()\n",
    "\n",
    "        # Check for cross validation option\n",
    "        if self.use_cross_validation.get():\n",
    "            self.perform_cross_validation(model_name, epochs, batch_size, learning_rate, fine_tune)\n",
    "            return\n",
    "\n",
    "        self.log(\"Preparing data for model: \" + model_name)\n",
    "\n",
    "        # Choose image size based on model requirements\n",
    "        if model_name == \"BasicCNN\":\n",
    "            img_size = (64, 64)\n",
    "        elif model_name in [\"Xception\", \"InceptionV3\"]:\n",
    "            img_size = (299, 299)\n",
    "        else:\n",
    "            img_size = (224, 224)\n",
    "\n",
    "        # Use the chosen split ratio\n",
    "        val_split = self.split_mapping.get(self.split_ratio.get(), 0.2)\n",
    "        datagen = ImageDataGenerator(\n",
    "            rescale=1.0/255,\n",
    "            validation_split=val_split,\n",
    "            rotation_range=5,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=False\n",
    "        )\n",
    "\n",
    "        train_generator = datagen.flow_from_directory(\n",
    "            self.dataset_path.get(),\n",
    "            target_size=img_size,\n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical',\n",
    "            subset='training'\n",
    "        )\n",
    "\n",
    "        validation_generator = datagen.flow_from_directory(\n",
    "            self.dataset_path.get(),\n",
    "            target_size=img_size,\n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical',\n",
    "            subset='validation',\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        num_classes = len(train_generator.class_indices)\n",
    "        model = self.build_model(model_name, (*img_size, 3), num_classes, fine_tune)\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy', \n",
    "                               tf.keras.metrics.Precision(name='precision'),\n",
    "                               tf.keras.metrics.Recall(name='recall')])\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
    "        self.log(\"Training model: \" + model_name)\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            validation_data=validation_generator,\n",
    "            epochs=epochs,\n",
    "            verbose=1,\n",
    "            callbacks=[self.TrainingCallback(self.root, epochs), early_stopping]\n",
    "        )\n",
    "\n",
    "        # Save the model along with training details\n",
    "        self.log(\"Training complete for model: \" + model_name + \". Saving model and training details...\")\n",
    "        model_dir = r\"C:\\Users\\User\\OneDrive\\Documents\\SignLanguageApp\\TrainedBinary5Model\"\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        model_file_name = f\"{model_name}_model.h5\"\n",
    "        model_save_path = os.path.join(model_dir, model_file_name)\n",
    "        try:\n",
    "            model.save(model_save_path)\n",
    "            self.log(\"Model saved successfully to: \" + model_save_path)\n",
    "        except Exception as e:\n",
    "            self.log(\"Error saving model: \" + str(e))\n",
    "            return\n",
    "\n",
    "        training_details = {\n",
    "            \"model_name\": model_name,\n",
    "            \"epochs\": epochs,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"fine_tune\": fine_tune,\n",
    "            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"train_val_split\": f\"Training ({100-int(val_split*100)}%) | Validation ({int(val_split*100)}%)\",\n",
    "            \"training_history\": history.history\n",
    "        }\n",
    "        details_file_name = f\"{model_name}_training_details.json\"\n",
    "        details_save_path = os.path.join(model_dir, details_file_name)\n",
    "        try:\n",
    "            with open(details_save_path, \"w\") as details_file:\n",
    "                json.dump(training_details, details_file, indent=4)\n",
    "            self.log(\"Training details saved successfully to: \" + details_save_path)\n",
    "        except Exception as e:\n",
    "            self.log(\"Error saving training details: \" + str(e))\n",
    "            return\n",
    "\n",
    "        results_folder = os.path.join(os.getcwd(), f\"{model_name}_results\")\n",
    "        os.makedirs(results_folder, exist_ok=True)\n",
    "        self.plot_training_history(history, results_folder, model_name)\n",
    "        # If a separate test dataset is selected, use it for evaluation; otherwise use the training dataset.\n",
    "        test_path = self.test_dataset_path.get() if self.use_separate_test.get() and self.test_dataset_path.get() else self.dataset_path.get()\n",
    "        self.evaluate_and_save_metrics(model, validation_generator, model_name, test_path, results_folder)\n",
    "\n",
    "        # ---------------------------\n",
    "        # Generate and save Grad-CAM heatmap for a sample training image\n",
    "        # ---------------------------\n",
    "        try:\n",
    "            class_folders = sorted([d for d in os.listdir(self.dataset_path.get()) if os.path.isdir(os.path.join(self.dataset_path.get(), d))])\n",
    "            if len(class_folders) > 0:\n",
    "                sample_class_dir = os.path.join(self.dataset_path.get(), class_folders[0])\n",
    "                sample_img = None\n",
    "                for file in os.listdir(sample_class_dir):\n",
    "                    if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                        sample_img = os.path.join(sample_class_dir, file)\n",
    "                        break\n",
    "                if sample_img is not None:\n",
    "                    self.generate_and_save_gradcam_heatmap(model, sample_img, img_size, model_name, results_folder)\n",
    "                else:\n",
    "                    self.log(\"No image found in the sample class folder for Grad-CAM.\")\n",
    "            else:\n",
    "                self.log(\"No class folders found in the dataset directory.\")\n",
    "        except Exception as e:\n",
    "            self.log(\"Error generating Grad-CAM heatmap: \" + str(e))\n",
    "\n",
    "    def perform_cross_validation(self, model_name, epochs, batch_size, learning_rate, fine_tune):\n",
    "        self.log(\"Performing cross validation for model: \" + model_name)\n",
    "        file_paths, labels, class_names = self.load_image_paths_and_labels(self.dataset_path.get())\n",
    "        if model_name == \"BasicCNN\":\n",
    "            img_size = (64, 64)\n",
    "        elif model_name in [\"Xception\", \"InceptionV3\"]:\n",
    "            img_size = (299, 299)\n",
    "        else:\n",
    "            img_size = (224, 224)\n",
    "        \n",
    "        def load_and_preprocess_image(path, label):\n",
    "            image = tf.io.read_file(path)\n",
    "            image = tf.image.decode_image(image, channels=3)\n",
    "            image.set_shape([None, None, 3])\n",
    "            image = tf.image.resize(image, img_size)\n",
    "            image = tf.cast(image, tf.float32) / 255.0\n",
    "            return image, label\n",
    "\n",
    "        kf = KFold(n_splits=self.num_folds.get(), shuffle=True, random_state=42)\n",
    "        all_y_true = []\n",
    "        all_y_pred = []\n",
    "        fold_metrics = []\n",
    "        fold_idx = 1\n",
    "        \n",
    "        for train_index, val_index in kf.split(file_paths):\n",
    "            self.log(f\"Training fold {fold_idx}/{self.num_folds.get()}...\")\n",
    "            train_paths = file_paths[train_index]\n",
    "            train_labels = labels[train_index]\n",
    "            val_paths = file_paths[val_index]\n",
    "            val_labels = labels[val_index]\n",
    "            \n",
    "            train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
    "            train_ds = train_ds.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "            train_ds = train_ds.shuffle(buffer_size=1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "            \n",
    "            val_ds = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n",
    "            val_ds = val_ds.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "            val_ds = val_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "            \n",
    "            num_classes = len(class_names)\n",
    "            model = self.build_model(model_name, (*img_size, 3), num_classes, fine_tune)\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                          loss='sparse_categorical_crossentropy',\n",
    "                          metrics=['accuracy'])\n",
    "            model.fit(train_ds, epochs=epochs, validation_data=val_ds, verbose=1)\n",
    "            y_pred_prob = model.predict(val_ds)\n",
    "            y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "            all_y_true.extend(val_labels)\n",
    "            all_y_pred.extend(y_pred)\n",
    "            \n",
    "            fold_accuracy = accuracy_score(val_labels, y_pred)\n",
    "            fold_precision = precision_score(val_labels, y_pred, average='weighted')\n",
    "            fold_recall = recall_score(val_labels, y_pred, average='weighted')\n",
    "            fold_f1 = f1_score(val_labels, y_pred, average='weighted')\n",
    "            fold_metrics.append((fold_accuracy, fold_precision, fold_recall, fold_f1))\n",
    "            fold_idx += 1\n",
    "        \n",
    "        overall_accuracy = accuracy_score(all_y_true, all_y_pred)\n",
    "        overall_precision = precision_score(all_y_true, all_y_pred, average='weighted')\n",
    "        overall_recall = recall_score(all_y_true, all_y_pred, average='weighted')\n",
    "        overall_f1 = f1_score(all_y_true, all_y_pred, average='weighted')\n",
    "        \n",
    "        report = classification_report(all_y_true, all_y_pred, target_names=class_names)\n",
    "        cm = confusion_matrix(all_y_true, all_y_pred)\n",
    "        \n",
    "        cv_results_folder = os.path.join(os.getcwd(), f\"{model_name}_cv_results\")\n",
    "        os.makedirs(cv_results_folder, exist_ok=True)\n",
    "        txt_file = os.path.join(cv_results_folder, f\"{model_name}_cv_evaluation.txt\")\n",
    "        with open(txt_file, \"w\") as f:\n",
    "            f.write(f\"Cross Validation Results for model: {model_name}\\n\")\n",
    "            f.write(f\"Number of Folds: {self.num_folds.get()}\\n\")\n",
    "            f.write(\"Fold Metrics (Accuracy, Precision, Recall, F1):\\n\")\n",
    "            for i, m in enumerate(fold_metrics, start=1):\n",
    "                f.write(f\"Fold {i}: {m}\\n\")\n",
    "            f.write(\"\\nOverall Metrics:\\n\")\n",
    "            f.write(f\"Accuracy: {overall_accuracy:.4f}\\n\")\n",
    "            f.write(f\"Precision: {overall_precision:.4f}\\n\")\n",
    "            f.write(f\"Recall: {overall_recall:.4f}\\n\")\n",
    "            f.write(f\"F1 Score: {overall_f1:.4f}\\n\\n\")\n",
    "            f.write(\"Classification Report:\\n\")\n",
    "            f.write(report)\n",
    "        \n",
    "        plt.figure(figsize=(8,6))\n",
    "        metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "        metrics_values = [overall_accuracy, overall_precision, overall_recall, overall_f1]\n",
    "        sns.barplot(x=metrics_names, y=metrics_values)\n",
    "        plt.ylim(0, 1)\n",
    "        plt.title('Cross Validation Overall Metrics')\n",
    "        metrics_graph_path = os.path.join(cv_results_folder, f\"{model_name}_cv_metrics_bar.png\")\n",
    "        plt.savefig(metrics_graph_path)\n",
    "        plt.close()\n",
    "        \n",
    "        plt.figure(figsize=(10,8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.title('Cross Validation Confusion Matrix')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        cm_path = os.path.join(cv_results_folder, f\"{model_name}_cv_confusion_matrix.png\")\n",
    "        plt.savefig(cm_path)\n",
    "        plt.close()\n",
    "        \n",
    "        self.log(\"Cross validation complete. Results saved.\")\n",
    "\n",
    "        self.log(\"Training final model on entire dataset after CV...\")\n",
    "        final_datagen = ImageDataGenerator(\n",
    "            rescale=1.0/255,\n",
    "            validation_split=val_split,\n",
    "            rotation_range=10,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=False\n",
    "        )\n",
    "        final_train_generator = final_datagen.flow_from_directory(\n",
    "            self.dataset_path.get(),\n",
    "            target_size=img_size,\n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical',\n",
    "            subset='training'\n",
    "        )\n",
    "        final_validation_generator = final_datagen.flow_from_directory(\n",
    "            self.dataset_path.get(),\n",
    "            target_size=img_size,\n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical',\n",
    "            subset='validation',\n",
    "            shuffle=False\n",
    "        )\n",
    "        num_classes = len(final_train_generator.class_indices)\n",
    "        final_model = self.build_model(model_name, (*img_size, 3), num_classes, fine_tune)\n",
    "        final_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                            loss='categorical_crossentropy',\n",
    "                            metrics=['accuracy'])\n",
    "        final_early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        final_history = final_model.fit(\n",
    "            final_train_generator,\n",
    "            validation_data=final_validation_generator,\n",
    "            epochs=epochs,\n",
    "            verbose=1,\n",
    "            callbacks=[final_early_stopping]\n",
    "        )\n",
    "\n",
    "        final_model_dir = r\"C:\\Users\\User\\OneDrive\\Documents\\SignLanguageApp\\TrainedBinary5Model_CV\"\n",
    "        os.makedirs(final_model_dir, exist_ok=True)\n",
    "        final_model_file_name = f\"{model_name}_final_model.h5\"\n",
    "        final_model_save_path = os.path.join(final_model_dir, final_model_file_name)\n",
    "        try:\n",
    "            final_model.save(final_model_save_path)\n",
    "            self.log(\"Final model saved successfully to: \" + final_model_save_path)\n",
    "        except Exception as e:\n",
    "            self.log(\"Error saving final model: \" + str(e))\n",
    "            return\n",
    "\n",
    "        final_training_details = {\n",
    "            \"model_name\": model_name,\n",
    "            \"epochs\": epochs,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"fine_tune\": fine_tune,\n",
    "            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"train_val_split\": f\"Training ({100-int(val_split*100)}%) | Validation ({int(val_split*100)}%)\",\n",
    "            \"final_training_history\": final_history.history,\n",
    "            \"cross_validation_results\": {\n",
    "                \"overall_accuracy\": overall_accuracy,\n",
    "                \"overall_precision\": overall_precision,\n",
    "                \"overall_recall\": overall_recall,\n",
    "                \"overall_f1\": overall_f1,\n",
    "                \"classification_report\": report\n",
    "            }\n",
    "        }\n",
    "        final_details_file_name = f\"{model_name}_final_training_details.json\"\n",
    "        final_details_save_path = os.path.join(final_model_dir, final_details_file_name)\n",
    "        try:\n",
    "            with open(final_details_save_path, \"w\") as f:\n",
    "                json.dump(final_training_details, f, indent=4)\n",
    "            self.log(\"Final training details saved successfully to: \" + final_details_save_path)\n",
    "        except Exception as e:\n",
    "            self.log(\"Error saving final training details: \" + str(e))\n",
    "            return\n",
    "\n",
    "        self.log(f\"Final model and training details saved in {final_model_dir}\")\n",
    "\n",
    "    def train_models_sequentially(self):\n",
    "        selected_indices = self.seq_listbox.curselection()\n",
    "        if not selected_indices:\n",
    "            self.log(\"No models selected for sequential training.\")\n",
    "            return\n",
    "        selected_models = [self.seq_listbox.get(i) for i in selected_indices]\n",
    "        total_models = len(selected_models)\n",
    "        self.progress_bar[\"maximum\"] = total_models\n",
    "        self.progress_bar[\"value\"] = 0\n",
    "\n",
    "        # Clear and populate the sequential training queue\n",
    "        for item in self.queue_tree.get_children():\n",
    "            self.queue_tree.delete(item)\n",
    "        for model in selected_models:\n",
    "            self.queue_tree.insert(\"\", \"end\", iid=model, values=(model, \"Pending\"))\n",
    "\n",
    "        # Start a timer thread\n",
    "        start_time = datetime.now()\n",
    "        def update_timer():\n",
    "            while self.progress_bar[\"value\"] < total_models:\n",
    "                elapsed = datetime.now() - start_time\n",
    "                self.timer_label.config(text=f\"Timer: {str(elapsed).split('.')[0]}\")\n",
    "                time.sleep(1)\n",
    "        timer_thread = threading.Thread(target=update_timer, daemon=True)\n",
    "        timer_thread.start()\n",
    "\n",
    "        def sequential_training():\n",
    "            for idx, model in enumerate(selected_models, start=1):\n",
    "                self.update_queue_status(model, \"In Progress\")\n",
    "                try:\n",
    "                    self._train_model_thread(model_name_override=model)\n",
    "                    self.update_queue_status(model, \"Completed\")\n",
    "                    self.log(f\"Model {model} trained successfully.\")\n",
    "                except Exception as e:\n",
    "                    self.update_queue_status(model, f\"Error: {e}\")\n",
    "                    self.log(f\"Error training model {model}: {e}\")\n",
    "                self.progress_bar[\"value\"] = idx\n",
    "            self.log(\"Sequential training complete.\")\n",
    "        threading.Thread(target=sequential_training, daemon=True).start()\n",
    "\n",
    "    def generate_and_save_gradcam_heatmap(self, model, sample_img_path, img_size, model_name, results_folder):\n",
    "        from tensorflow.keras.preprocessing import image\n",
    "        # Load and preprocess the image\n",
    "        img = image.load_img(sample_img_path, target_size=img_size)\n",
    "        img_array = image.img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array /= 255.0\n",
    "\n",
    "        last_conv_layer_name = find_last_conv_layer(model)\n",
    "        if last_conv_layer_name is None:\n",
    "            self.log(\"No convolutional layer found in the model; Grad-CAM cannot be applied.\")\n",
    "            return\n",
    "\n",
    "        heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n",
    "        output_path = os.path.join(results_folder, f\"{model_name}_gradcam_heatmap.png\")\n",
    "        save_gradcam_overlay(sample_img_path, heatmap, output_path)\n",
    "        self.log(\"Grad-CAM heatmap saved to: \" + output_path)\n",
    "\n",
    "    def plot_training_history(self, history, result_folder, model_name):\n",
    "        epochs_range = range(len(history.history['accuracy']))\n",
    "        if 'precision' in history.history and 'recall' in history.history:\n",
    "            train_f1 = [2*(p*r)/(p+r) if (p+r) > 0 else 0 \n",
    "                        for p, r in zip(history.history['precision'], history.history['recall'])]\n",
    "            val_f1 = [2*(p*r)/(p+r) if (p+r) > 0 else 0 \n",
    "                      for p, r in zip(history.history['val_precision'], history.history['val_recall'])]\n",
    "        else:\n",
    "            train_f1 = [0] * len(epochs_range)\n",
    "            val_f1 = [0] * len(epochs_range)\n",
    "        \n",
    "        fig, axs = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        axs[0, 0].plot(epochs_range, history.history['accuracy'], label='Training Accuracy')\n",
    "        axs[0, 0].plot(epochs_range, history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        axs[0, 0].legend(loc='lower right')\n",
    "        axs[0, 0].set_title('Accuracy')\n",
    "        \n",
    "        axs[0, 1].plot(epochs_range, history.history['loss'], label='Training Loss')\n",
    "        axs[0, 1].plot(epochs_range, history.history['val_loss'], label='Validation Loss')\n",
    "        axs[0, 1].legend(loc='upper right')\n",
    "        axs[0, 1].set_title('Loss')\n",
    "        \n",
    "        if 'precision' in history.history:\n",
    "            axs[0, 2].plot(epochs_range, history.history['precision'], label='Training Precision')\n",
    "            axs[0, 2].plot(epochs_range, history.history['val_precision'], label='Validation Precision')\n",
    "            axs[0, 2].legend(loc='lower right')\n",
    "            axs[0, 2].set_title('Precision')\n",
    "        else:\n",
    "            axs[0, 2].set_visible(False)\n",
    "        \n",
    "        if 'recall' in history.history:\n",
    "            axs[1, 0].plot(epochs_range, history.history['recall'], label='Training Recall')\n",
    "            axs[1, 0].plot(epochs_range, history.history['val_recall'], label='Validation Recall')\n",
    "            axs[1, 0].legend(loc='lower right')\n",
    "            axs[1, 0].set_title('Recall')\n",
    "        else:\n",
    "            axs[1, 0].set_visible(False)\n",
    "        \n",
    "        if 'precision' in history.history and 'recall' in history.history:\n",
    "            axs[1, 1].plot(epochs_range, train_f1, label='Training F1')\n",
    "            axs[1, 1].plot(epochs_range, val_f1, label='Validation F1')\n",
    "            axs[1, 1].legend(loc='lower right')\n",
    "            axs[1, 1].set_title('F1 Score')\n",
    "        else:\n",
    "            axs[1, 1].set_visible(False)\n",
    "        \n",
    "        axs[1, 2].axis('off')\n",
    "        \n",
    "        plt.suptitle('Training Metrics Over Epochs')\n",
    "        training_plot_path = os.path.join(result_folder, f\"{model_name}_training_metrics.png\")\n",
    "        plt.savefig(training_plot_path)\n",
    "        plt.close()\n",
    "\n",
    "    def evaluate_and_save_metrics(self, model, validation_generator, model_name, dataset_path, result_folder):\n",
    "        validation_generator.reset()\n",
    "        Y_pred = model.predict(validation_generator, verbose=1)\n",
    "        y_pred = np.argmax(Y_pred, axis=1)\n",
    "        y_true = validation_generator.classes\n",
    "        class_names = list(validation_generator.class_indices.keys())\n",
    "        \n",
    "        report = classification_report(y_true, y_pred, target_names=class_names)\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred, average='weighted')\n",
    "        recall = recall_score(y_true, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "        \n",
    "        txt_file = os.path.join(result_folder, f\"{model_name}_evaluation.txt\")\n",
    "        with open(txt_file, \"w\") as f:\n",
    "            f.write(f\"Model Name: {model_name}\\n\")\n",
    "            f.write(f\"Dataset Path: {dataset_path}\\n\")\n",
    "            f.write(\"Train-Test Split: Training (80%) | Validation (20%)\\n\\n\")\n",
    "            f.write(\"Evaluation Metrics:\\n\")\n",
    "            f.write(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "            f.write(f\"Precision: {precision:.4f}\\n\")\n",
    "  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "F21DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
