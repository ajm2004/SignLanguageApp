{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image, ImageEnhance, ImageOps\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define augmentation functions\n",
    "def random_rotation(image):\n",
    "    angle = random.uniform(-30, 30)  # Rotate between -30 to 30 degrees\n",
    "    return image.rotate(angle)\n",
    "\n",
    "def random_flip(image):\n",
    "    if random.choice([True, False]):\n",
    "        return ImageOps.mirror(image)\n",
    "    return image\n",
    "\n",
    "def random_brightness(image):\n",
    "    enhancer = ImageEnhance.Brightness(image)\n",
    "    factor = random.uniform(0.7, 1.3)  # Brightness factor\n",
    "    return enhancer.enhance(factor)\n",
    "\n",
    "def random_contrast(image):\n",
    "    enhancer = ImageEnhance.Contrast(image)\n",
    "    factor = random.uniform(0.7, 1.3)  # Contrast factor\n",
    "    return enhancer.enhance(factor)\n",
    "\n",
    "def add_random_noise(image):\n",
    "    np_image = np.array(image)\n",
    "    noise = np.random.normal(0, 25, np_image.shape).astype(np.int16)\n",
    "    noisy_image = np.clip(np_image + noise, 0, 255).astype(np.uint8)\n",
    "    return Image.fromarray(noisy_image)\n",
    "\n",
    "def augment_image(image):\n",
    "    image = random_rotation(image)\n",
    "    image = random_flip(image)\n",
    "    # image = random_brightness(image)\n",
    "    # image = random_contrast(image)\n",
    "    # image = add_random_noise(image)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"C:/Users/User/OneDrive/Documents/SignLanguageApp/SLangDataset/data_est_white\"\n",
    "output_folder = \"C:/Users/User/OneDrive/Documents/SignLanguageApp/SLangDataset/LMARK_aug_data_est_white\"\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data augmentation completed!\n"
     ]
    }
   ],
   "source": [
    "# Iterate over all subfolders and images\n",
    "for subdir, _, files in os.walk(input_folder):\n",
    "    relative_path = os.path.relpath(subdir, input_folder)\n",
    "    output_subdir = os.path.join(output_folder, relative_path)\n",
    "    os.makedirs(output_subdir, exist_ok=True)\n",
    "\n",
    "    for file in files:\n",
    "        if file.lower().endswith(('png', 'jpg', 'jpeg', 'bmp', 'tiff')):\n",
    "            input_path = os.path.join(subdir, file)\n",
    "            output_path = os.path.join(output_subdir, file)\n",
    "\n",
    "            try:\n",
    "                with Image.open(input_path) as img:\n",
    "                    img = img.convert(\"L\")  # Ensure greyscale (black and white)\n",
    "                    augmented_img = augment_image(img)\n",
    "                    augmented_img.save(output_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {input_path}: {e}\")\n",
    "\n",
    "print(\"Data augmentation completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining Augmented Data with Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets merged into: C:\\Users\\User\\OneDrive\\Documents\\SignLanguageApp\\SLangDataset\\LMARK_merged_data_est_CombinedBW\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "dataset1 = r\"C:\\Users\\User\\OneDrive\\Documents\\SignLanguageApp\\SLangDataset\\LMARK_merged_data_est\"\n",
    "dataset2 = r\"C:\\Users\\User\\OneDrive\\Documents\\SignLanguageApp\\SLangDataset\\LMARK_merged_data_est_white\"\n",
    "output_dataset = r\"C:\\Users\\User\\OneDrive\\Documents\\SignLanguageApp\\SLangDataset\\LMARK_merged_data_est_CombinedBW\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dataset, exist_ok=True)\n",
    "\n",
    "# Function to merge datasets with renaming\n",
    "def merge_datasets(source_dir, target_dir, suffix=\"\"):\n",
    "    for class_name in os.listdir(source_dir):\n",
    "        source_class_path = os.path.join(source_dir, class_name)\n",
    "        target_class_path = os.path.join(target_dir, class_name)\n",
    "        \n",
    "        if os.path.isdir(source_class_path):\n",
    "            # Create the class folder in the target if it doesn't exist\n",
    "            if not os.path.exists(target_class_path):\n",
    "                os.makedirs(target_class_path)\n",
    "            \n",
    "            for file_name in os.listdir(source_class_path):\n",
    "                source_file_path = os.path.join(source_class_path, file_name)\n",
    "                # Add the specified suffix to the file name\n",
    "                base_name, ext = os.path.splitext(file_name)\n",
    "                file_name = f\"{base_name}{suffix}{ext}\"\n",
    "                target_file_path = os.path.join(target_class_path, file_name)\n",
    "                \n",
    "                # Copy the file to the target directory\n",
    "                shutil.copy2(source_file_path, target_file_path)\n",
    "\n",
    "# Merge the main dataset\n",
    "merge_datasets(dataset1, output_dataset, suffix=\"_black\")\n",
    "\n",
    "# Merge the augmented dataset with \"_AUG\" renaming\n",
    "merge_datasets(dataset2, output_dataset, suffix=\"_white\")\n",
    "\n",
    "\n",
    "print(f\"Datasets merged into: {output_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Trainer3.py for training the new data. Run to open UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'builder' from 'google.protobuf.internal' (c:\\Users\\User\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\google\\protobuf\\internal\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcvzone\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mHandTrackingModule\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HandDetector\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcvzone\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mClassificationModule\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Classifier\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\mediapipe\\__init__.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2019 - 2022 The MediaPipe Authors.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolutions\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msolutions\u001b[39;00m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtasks\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m framework\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\mediapipe\\python\\solutions\\__init__.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The MediaPipe Authors.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"MediaPipe Solutions Python API.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolutions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdrawing_styles\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolutions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdrawing_utils\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolutions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mface_detection\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\mediapipe\\python\\solutions\\drawing_styles.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolutions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m face_mesh_connections\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolutions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hands_connections\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolutions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdrawing_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DrawingSpec\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolutions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhands\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HandLandmark\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolutions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpose\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PoseLandmark\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\mediapipe\\python\\solutions\\drawing_utils.py:24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m detection_pb2\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m landmark_pb2\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m location_data_pb2\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\mediapipe\\framework\\formats\\detection_pb2.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m descriptor_pool \u001b[38;5;28;01mas\u001b[39;00m _descriptor_pool\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m symbol_database \u001b[38;5;28;01mas\u001b[39;00m _symbol_database\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m builder \u001b[38;5;28;01mas\u001b[39;00m _builder\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'builder' from 'google.protobuf.internal' (c:\\Users\\User\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\google\\protobuf\\internal\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "from cvzone.ClassificationModule import Classifier\n",
    "import numpy as np\n",
    "import math\n",
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "# ---------------------------\n",
    "# Your existing processing code\n",
    "# ---------------------------\n",
    "\n",
    "def detect_skin(frame):\n",
    "    # Convert to YCrCb and equalize the luminance channel\n",
    "    ycrcb = cv2.cvtColor(frame, cv2.COLOR_BGR2YCrCb)\n",
    "    y_channel = ycrcb[:, :, 0]\n",
    "    y_eq = cv2.equalizeHist(y_channel)\n",
    "    ycrcb[:, :, 0] = y_eq\n",
    "\n",
    "    # Adjusted thresholds might be needed after equalization.\n",
    "    lower_skin = np.array([0, 133, 77], dtype=np.uint8)\n",
    "    upper_skin = np.array([255, 173, 127], dtype=np.uint8)\n",
    "    mask = cv2.inRange(ycrcb, lower_skin, upper_skin)\n",
    "    \n",
    "    # Noise reduction using morphological operations\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    mask = cv2.GaussianBlur(mask, (5, 5), 0)\n",
    "    \n",
    "    # Optionally, keep only the largest contour (if needed)\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contour_mask = np.zeros_like(mask)\n",
    "    if contours:\n",
    "        cv2.drawContours(contour_mask, contours, -1, 255, thickness=cv2.FILLED)\n",
    "    mask = cv2.bitwise_and(mask, contour_mask)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# ---------------------------\n",
    "# Initialization\n",
    "# ---------------------------\n",
    "cap = cv2.VideoCapture(0)\n",
    "detector = HandDetector(maxHands=1)\n",
    "classifier = Classifier(\"C:/Users/User/OneDrive/Documents/SignLanguageApp/TrainedBinary2Model/MobileNetV2_model.h5\")\n",
    "\n",
    "offset = 45\n",
    "imgSize = 250\n",
    "labels = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\"]\n",
    "\n",
    "mp_hands = mp.solutions.hands  # for landmark connections\n",
    "hand_connections = mp_hands.HAND_CONNECTIONS\n",
    "\n",
    "# To store the history of predictions\n",
    "prediction_history = []\n",
    "\n",
    "# ---------------------------\n",
    "# Tkinter UI Setup\n",
    "# ---------------------------\n",
    "root = tk.Tk()\n",
    "root.title(\"Sign Language Recognition\")\n",
    "root.geometry(\"1200x800\")  # Adjust window size as needed\n",
    "\n",
    "# Divide the window into two panels:\n",
    "# Left panel (biggest) for the main camera feed.\n",
    "left_frame = tk.Frame(root, width=800, height=800, bg=\"black\")\n",
    "left_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)\n",
    "\n",
    "# Right panel for the additional images and prediction history.\n",
    "right_frame = tk.Frame(root, width=400, height=800)\n",
    "right_frame.pack(side=tk.RIGHT, fill=tk.Y)\n",
    "\n",
    "# Label for the main camera feed (largest display)\n",
    "main_image_label = tk.Label(left_frame)\n",
    "main_image_label.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "# Labels for the processed binary image and the hand landmarks.\n",
    "binary_label = tk.Label(right_frame)\n",
    "binary_label.pack(pady=5)\n",
    "\n",
    "landmarks_label = tk.Label(right_frame)\n",
    "landmarks_label.pack(pady=5)\n",
    "\n",
    "# Scrolled text widget to show prediction history.\n",
    "history_text = scrolledtext.ScrolledText(right_frame, width=40, height=20)\n",
    "history_text.pack(pady=5)\n",
    "history_text.configure(state='disabled')\n",
    "\n",
    "# ---------------------------\n",
    "# Update function for video frames and UI elements\n",
    "# ---------------------------\n",
    "def update_frame():\n",
    "    global prediction_history\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        root.after(10, update_frame)\n",
    "        return\n",
    "\n",
    "    imgOutput = img.copy()\n",
    "    hands, img = detector.findHands(img, draw=False)\n",
    "    \n",
    "    # Variables to hold images for right-panel display.\n",
    "    imgWhite_for_display = None\n",
    "    imgCrop_landmarked_for_display = None\n",
    "    \n",
    "    if hands:\n",
    "        hand = hands[0]\n",
    "        x, y, w, h = hand['bbox']\n",
    "        y1, y2 = max(0, y - offset), min(img.shape[0], y + h + offset)\n",
    "        x1, x2 = max(0, x - offset), min(img.shape[1], x + w + offset)\n",
    "        imgCrop = img[y1:y2, x1:x2]\n",
    "        \n",
    "        if imgCrop.shape[0] > 0 and imgCrop.shape[1] > 0:\n",
    "            # Draw hand landmarks on the cropped image (for visualization)\n",
    "            imgCrop_landmarked = imgCrop.copy()\n",
    "            if 'lmList' in hand:\n",
    "                lm_list = hand['lmList']\n",
    "                for lm in lm_list:\n",
    "                    cv2.circle(imgCrop_landmarked, (lm[0] - x1, lm[1] - y1), 4, (0, 0, 255), -1)\n",
    "                for connection in mp_hands.HAND_CONNECTIONS:\n",
    "                    if connection[0] < len(lm_list) and connection[1] < len(lm_list):\n",
    "                        pt1 = (lm_list[connection[0]][0] - x1, lm_list[connection[0]][1] - y1)\n",
    "                        pt2 = (lm_list[connection[1]][0] - x1, lm_list[connection[1]][1] - y1)\n",
    "                        cv2.line(imgCrop_landmarked, pt1, pt2, (0, 0, 255), 2)\n",
    "            \n",
    "            # Create the binary image from the cropped region.\n",
    "            binaryMask = detect_skin(imgCrop)\n",
    "            binary_result = np.zeros_like(imgCrop)\n",
    "            binary_result[binaryMask > 0] = [255, 255, 255]\n",
    "            \n",
    "            # Overlay landmarks on the binary image.\n",
    "            if 'lmList' in hand:\n",
    "                for lm in lm_list:\n",
    "                    cv2.circle(binary_result, (lm[0] - x1, lm[1] - y1), 4, (0, 0, 0), -1)\n",
    "                for connection in mp_hands.HAND_CONNECTIONS:\n",
    "                    pt1 = (lm_list[connection[0]][0] - x1, lm_list[connection[0]][1] - y1)\n",
    "                    pt2 = (lm_list[connection[1]][0] - x1, lm_list[connection[1]][1] - y1)\n",
    "                    cv2.line(binary_result, pt1, pt2, (0, 0, 0), 2)\n",
    "            \n",
    "            # Resize the binary image to a fixed size (while preserving aspect ratio)\n",
    "            aspectRatio = h / w\n",
    "            imgWhite = np.ones((imgSize, imgSize), np.uint8) * 0\n",
    "            if aspectRatio > 1:\n",
    "                k = imgSize / h\n",
    "                wCal = math.ceil(k * w)\n",
    "                imgResize = cv2.resize(binary_result, (wCal, imgSize))\n",
    "                wGap = math.ceil((imgSize - wCal) / 2)\n",
    "                imgWhite[:, wGap:wCal + wGap] = cv2.cvtColor(imgResize, cv2.COLOR_BGR2GRAY)\n",
    "            else:\n",
    "                k = imgSize / w\n",
    "                hCal = math.ceil(k * h)\n",
    "                imgResize = cv2.resize(binary_result, (imgSize, hCal))\n",
    "                hGap = math.ceil((imgSize - hCal) / 2)\n",
    "                imgWhite[hGap:hCal + hGap, :] = cv2.cvtColor(imgResize, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # Prepare for classification.\n",
    "            imgWhiteRGB = cv2.cvtColor(imgWhite, cv2.COLOR_GRAY2BGR)\n",
    "            prediction, index = classifier.getPrediction(imgWhiteRGB, draw=False)\n",
    "            \n",
    "            # If the confidence is high enough, annotate the main image and record the prediction.\n",
    "            if prediction[index] > 0.75 and 0 <= index < len(labels):\n",
    "                cv2.rectangle(imgOutput, (x - offset, y - offset - 50),\n",
    "                              (x - offset + 90, y - offset - 50 + 50), (255, 0, 255), cv2.FILLED)\n",
    "                cv2.putText(imgOutput, labels[index], (x, y - 26),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 1.7, (255, 255, 255), 2)\n",
    "                cv2.rectangle(imgOutput, (x - offset, y - offset),\n",
    "                              (x + w + offset, y + h + offset), (255, 0, 255), 4)\n",
    "                \n",
    "                # Append the prediction with its probability to the history.\n",
    "                pred_text = f\"{labels[index]}: {prediction[index]:.2f}\"\n",
    "                prediction_history.append(pred_text)\n",
    "                if len(prediction_history) > 50:\n",
    "                    prediction_history = prediction_history[-50:]\n",
    "                history_text.configure(state='normal')\n",
    "                history_text.insert(tk.END, pred_text + \"\\n\")\n",
    "                history_text.see(tk.END)\n",
    "                history_text.configure(state='disabled')\n",
    "            \n",
    "            # Save images to display on the right panel.\n",
    "            imgWhite_for_display = imgWhite.copy()\n",
    "            imgCrop_landmarked_for_display = imgCrop_landmarked.copy()\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Convert OpenCV images to a format Tkinter can display.\n",
    "    # Main image (imgOutput) is in BGR, so convert to RGB.\n",
    "    imgOutput_rgb = cv2.cvtColor(imgOutput, cv2.COLOR_BGR2RGB)\n",
    "    img_pil = Image.fromarray(imgOutput_rgb)\n",
    "    img_tk = ImageTk.PhotoImage(image=img_pil)\n",
    "    main_image_label.imgtk = img_tk\n",
    "    main_image_label.configure(image=img_tk)\n",
    "    \n",
    "    # Update the processed binary image display.\n",
    "    if imgWhite_for_display is not None:\n",
    "        imgWhite_rgb = cv2.cvtColor(imgWhite_for_display, cv2.COLOR_GRAY2RGB)\n",
    "        imgWhite_pil = Image.fromarray(imgWhite_rgb)\n",
    "        imgWhite_tk = ImageTk.PhotoImage(image=imgWhite_pil)\n",
    "        binary_label.imgtk = imgWhite_tk\n",
    "        binary_label.configure(image=imgWhite_tk)\n",
    "    else:\n",
    "        binary_label.configure(image='')\n",
    "    \n",
    "    # Update the hand landmarks display.\n",
    "    if imgCrop_landmarked_for_display is not None:\n",
    "        imgCrop_rgb = cv2.cvtColor(imgCrop_landmarked_for_display, cv2.COLOR_BGR2RGB)\n",
    "        imgCrop_pil = Image.fromarray(imgCrop_rgb)\n",
    "        imgCrop_tk = ImageTk.PhotoImage(image=imgCrop_pil)\n",
    "        landmarks_label.imgtk = imgCrop_tk\n",
    "        landmarks_label.configure(image=imgCrop_tk)\n",
    "    else:\n",
    "        landmarks_label.configure(image='')\n",
    "    \n",
    "    root.after(10, update_frame)\n",
    "\n",
    "# ---------------------------\n",
    "# Handle window closing to release the camera\n",
    "# ---------------------------\n",
    "def on_closing():\n",
    "    cap.release()\n",
    "    root.destroy()\n",
    "\n",
    "root.protocol(\"WM_DELETE_WINDOW\", on_closing)\n",
    "\n",
    "# Start the update loop.\n",
    "update_frame()\n",
    "root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
