{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASL Fingerspelling Recognition\n",
    "\n",
    "In this notebook a brief setup guide along with the processes on how data was collected, preprocesses and trained will be discussed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup (Windows Only)\n",
    "\n",
    "For running and testing, please install the below libraries:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For data collection and preprocessing:\n",
    "(Open-CV, cvzone, NumPy, MediaPipe, Pillow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opencv-python cvzone numpy mediapipe Pillow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. For ML models and Training: (Tensorflow, Tensorflow-Hub, Matplotlib, Seaborn)\n",
    "\n",
    "Please install on the same enviroment. (Required for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (2.10.0)\n",
      "Collecting tensorflow-hub\n",
      "  Using cached tensorflow_hub-0.16.1-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (3.9.2)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorflow) (24.1)\n",
      "Collecting protobuf<3.20,>=3.9.2 (from tensorflow)\n",
      "  Using cached protobuf-3.19.6-cp39-cp39-win_amd64.whl.metadata (807 bytes)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorflow) (72.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp39-cp39-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorflow) (1.48.2)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Collecting tf-keras>=2.14.1 (from tensorflow-hub)\n",
      "  Using cached tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from matplotlib) (6.4.5)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from seaborn) (1.3.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.29.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.0.3)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.18.0-cp39-cp39-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting tensorflow-intel==2.18.0 (from tensorflow)\n",
      "  Downloading tensorflow_intel-2.18.0-cp39-cp39-win_amd64.whl.metadata (4.9 kB)\n",
      "INFO: pip is looking at multiple versions of tensorflow-intel to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tensorboard<2.11,>=2.10 (from tensorflow)\n",
      "  Downloading tensorboard-2.10.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "  Downloading tensorboard-2.10.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting tensorflow-hub\n",
      "  Downloading tensorflow_hub-0.16.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (7.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\ajaym\\anaconda3\\envs\\f21dl\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.2.2)\n",
      "Downloading tensorflow_hub-0.16.0-py2.py3-none-any.whl (30 kB)\n",
      "Using cached protobuf-3.19.6-cp39-cp39-win_amd64.whl (895 kB)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Downloading tensorflow_io_gcs_filesystem-0.31.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 0.5/1.5 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.0/1.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 2.0 MB/s eta 0:00:00\n",
      "Installing collected packages: libclang, tensorflow-io-gcs-filesystem, protobuf, tensorflow-hub, seaborn\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "Successfully installed libclang-18.1.1 protobuf-3.19.6 seaborn-0.13.2 tensorflow-hub-0.16.0 tensorflow-io-gcs-filesystem-0.31.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\ajaym\\anaconda3\\envs\\F21DL\\Lib\\site-packages\\google\\~rotobuf'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow tensorflow-hub matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. For Real-Time Recognition: [Same as collection and preprocessing] (Open-CV, cvzone, NumPy, MediaPipe, Pillow, pyttsx3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !! Skip if installed using step-1\n",
    "%pip install opencv-python cvzone numpy mediapipe Pillow pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyttsx3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "\n",
    "1. There may protobuf based errors when trying to run.\n",
    "Please install/re-install lower version of protobuf if faced (3.20.x or lower)\n",
    "\n",
    "2. There may be mediapipe based errors, please reinstall medipipe suitable for above protobuf version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Collection Pipeline\n",
    "\n",
    "For the process of collecting images of hand along with basic preprocessing while collection.\n",
    "\n",
    "Run the below cell after installing relevant dependancies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "IMPORTANT: WebCam required\n",
    "\n",
    "1. Below script collects 600, 500x500 images for each alphabets/class (A to Y, no Z) \n",
    "2. To start collection press S on the opencv frame (webcam frame)\n",
    "3. Fill in the baseFolder variable with the repository location to save the classes and images.\n",
    "4. After collecting images for the class, ENTER KEY prompt will pop up in VS-code search-bar/top-panel. Press enter there which will start process for next class\n",
    "5. Repeat steps 2 and 3 for subsequent collections\n",
    "6. After the collection of final class the program will exit/halt.\n",
    "7. The images can be found in the baseFolder repository\n",
    "\n",
    "### Preprocessings:\n",
    "\n",
    "1. Cropping hands from complete frame\n",
    "2. Adding hand landmarks to cropped frame\n",
    "3. Converting cropped frame to binary image (black and white pixels only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import mediapipe as mp\n",
    "\n",
    "#--------------------\n",
    "baseFolder = \"C:/path/to/your/output_folder\"  # <-- change this\n",
    "#--------------------\n",
    "\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize hand detector\n",
    "detector = HandDetector(maxHands=1)\n",
    "\n",
    "# Constants\n",
    "imgSize = 500\n",
    "# List of letters A-Y (adjust as needed)\n",
    "letters = [chr(i) for i in range(ord('A'), ord('Y') + 1)]\n",
    "maxImages = 600          # Total images to capture per class\n",
    "paddingFactor = 0.45     # Padding percentage\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# Function to process and resize images (canvas will match input channels)\n",
    "def process_and_resize(imgCrop, aspectRatio, imgSize):\n",
    "    channels = 1 if len(imgCrop.shape) == 2 else imgCrop.shape[2]\n",
    "    imgWhite = np.ones((imgSize, imgSize, channels), np.uint8) * 0\n",
    "    try:\n",
    "        if aspectRatio > 1:\n",
    "            # If height > width:\n",
    "            k = imgSize / imgCrop.shape[0]\n",
    "            wCal = math.ceil(k * imgCrop.shape[1])\n",
    "            imgResize = cv2.resize(imgCrop, (wCal, imgSize))\n",
    "            wGap = math.ceil((imgSize - wCal) / 2)\n",
    "            imgWhite[:, wGap:wCal + wGap] = imgResize\n",
    "        else:\n",
    "            # If width >= height:\n",
    "            k = imgSize / imgCrop.shape[1]\n",
    "            hCal = math.ceil(k * imgCrop.shape[0])\n",
    "            imgResize = cv2.resize(imgCrop, (imgSize, hCal))\n",
    "            hGap = math.ceil((imgSize - hCal) / 2)\n",
    "            imgWhite[hGap:hCal + hGap, :] = imgResize\n",
    "    except Exception as e:\n",
    "        print(f\"Error during image processing: {e}\")\n",
    "        return None\n",
    "    return imgWhite\n",
    "\n",
    "# Function to detect skin using YCrCb thresholds (returns a binary mask)\n",
    "def detect_skin(frame):\n",
    "    ycrcb = cv2.cvtColor(frame, cv2.COLOR_BGR2YCrCb)\n",
    "    lower_skin = np.array([0, 133, 77], dtype=np.uint8)\n",
    "    upper_skin = np.array([255, 173, 127], dtype=np.uint8)\n",
    "    mask = cv2.inRange(ycrcb, lower_skin, upper_skin)\n",
    "    \n",
    "    # Clean up noise\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    mask = cv2.GaussianBlur(mask, (5, 5), 0)\n",
    "    \n",
    "    # (Optional) clean-up by drawing filled contours\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contour_mask = np.zeros_like(mask)\n",
    "    cv2.drawContours(contour_mask, contours, -1, 255, thickness=cv2.FILLED)\n",
    "    mask = cv2.bitwise_and(mask, contour_mask)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# Main loop for each letter\n",
    "for className in letters:\n",
    "    print(f\"Starting collection for: {className}\")\n",
    "    folder = os.path.join(baseFolder, className)\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    counter, collecting = 0, False\n",
    "\n",
    "    while counter < maxImages:\n",
    "        success, img = cap.read()\n",
    "        if not success:\n",
    "            print(\"Camera access failed.\")\n",
    "            break\n",
    "\n",
    "        # Detect hand on the full image (only once)\n",
    "        hands, _ = detector.findHands(img, draw=False)\n",
    "        if hands:\n",
    "            # Use the first detected hand\n",
    "            hand = hands[0]\n",
    "            bbox = hand['bbox']       # [x, y, w, h]\n",
    "            lm_list = hand['lmList']    # list of landmarks in full-image coordinates\n",
    "            x, y, w, h = bbox\n",
    "\n",
    "            # Calculate padding (based on hand size)\n",
    "            xPad = int(w * paddingFactor)\n",
    "            yPad = int(h * paddingFactor)\n",
    "\n",
    "            # Compute crop boundaries (make sure they’re within image bounds)\n",
    "            crop_x1 = max(0, x - xPad)\n",
    "            crop_y1 = max(0, y - yPad)\n",
    "            crop_x2 = min(x + w + xPad, img.shape[1])\n",
    "            crop_y2 = min(y + h + yPad, img.shape[0])\n",
    "            imgCrop = img[crop_y1:crop_y2, crop_x1:crop_x2]\n",
    "\n",
    "            if imgCrop.size > 0:\n",
    "                # ----- STEP 1: Draw landmarks on the cropped image using adjusted coordinates -----\n",
    "                # Create a copy of the crop on which we will draw the landmarks\n",
    "                imgCrop_landmarked = imgCrop.copy()\n",
    "                # Adjust each landmark from full-image coordinates to crop coordinates\n",
    "                for lm in lm_list:\n",
    "                    adj_x = lm[0] - crop_x1\n",
    "                    adj_y = lm[1] - crop_y1\n",
    "                    cv2.circle(imgCrop_landmarked, (adj_x, adj_y), 4, (0, 0, 255), -1)\n",
    "                # Optionally, also draw the connections between landmarks:\n",
    "                for connection in mp.solutions.hands.HAND_CONNECTIONS:\n",
    "                    pt1 = lm_list[connection[0]]\n",
    "                    pt2 = lm_list[connection[1]]\n",
    "                    pt1_adjusted = (pt1[0] - crop_x1, pt1[1] - crop_y1)\n",
    "                    pt2_adjusted = (pt2[0] - crop_x1, pt2[1] - crop_y1)\n",
    "                    cv2.line(imgCrop_landmarked, pt1_adjusted, pt2_adjusted, (0, 0, 255), 2)\n",
    "\n",
    "                # ----- STEP 2: Convert the landmarked crop to a binary image -----\n",
    "                # First, get a binary skin mask from the original crop (without landmarks)\n",
    "                binaryMask = detect_skin(imgCrop)\n",
    "                # Create a blank (black) image and fill white where skin is detected\n",
    "                binary_result = np.zeros_like(imgCrop)\n",
    "                binary_result[binaryMask > 0] = [255, 255, 255]\n",
    "                # Now overlay the landmarks (drawn in black) onto the binary image.\n",
    "                # (Using the same adjusted coordinates from above)\n",
    "                for lm in lm_list:\n",
    "                    adj_x = lm[0] - crop_x1\n",
    "                    adj_y = lm[1] - crop_y1\n",
    "                    cv2.circle(binary_result, (adj_x, adj_y), 4, (0, 0, 0), -1)\n",
    "                for connection in mp.solutions.hands.HAND_CONNECTIONS:\n",
    "                    pt1 = lm_list[connection[0]]\n",
    "                    pt2 = lm_list[connection[1]]\n",
    "                    pt1_adjusted = (pt1[0] - crop_x1, pt1[1] - crop_y1)\n",
    "                    pt2_adjusted = (pt2[0] - crop_x1, pt2[1] - crop_y1)\n",
    "                    cv2.line(binary_result, pt1_adjusted, pt2_adjusted, (0, 0, 0), 2)\n",
    "\n",
    "                # ----- STEP 3: Resize for saving/visualization -----\n",
    "                # Use the crop’s aspect ratio for correct resizing. (You can also use h/w from bbox.)\n",
    "                aspectRatio = (crop_y2 - crop_y1) / (crop_x2 - crop_x1)\n",
    "                imgWhite = process_and_resize(binary_result, aspectRatio, imgSize)\n",
    "                if imgWhite is not None:\n",
    "                    cv2.imshow(\"Processed Binary Image\", imgWhite)\n",
    "                    if collecting:\n",
    "                        counter += 1\n",
    "                        savePath = os.path.join(folder, f\"{className.lower()}_{counter}.jpg\")\n",
    "                        cv2.imwrite(savePath, imgWhite)\n",
    "                        print(f\"Saved {counter}/{maxImages} images for {className}\")\n",
    "\n",
    "        # Show the original live feed\n",
    "        cv2.imshow(\"Live Feed with Landmarks\", img)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('s'):\n",
    "            collecting = True\n",
    "        if key == ord('p'):\n",
    "            collecting = False\n",
    "\n",
    "    print(f\"Completed collection for {className}\")\n",
    "    input(\"Press Enter for next class.\")\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Dataset Basic Preprocessing Pipeline\n",
    "\n",
    "For the basic preprocessing of hand images.\n",
    "\n",
    "Run the below cell after installing relevant dependancies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "1. Enter input folder \n",
    "2. Enter outPut folder for saving in the repository\n",
    "\n",
    "### Preprocessings:\n",
    "\n",
    "1. Adding hand landmarks to cropped frame\n",
    "2. Converting cropped frame to binary image (black and white pixels only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import mediapipe as mp\n",
    "\n",
    "# ----------------------------\n",
    "# Folder containing subfolders for each class (e.g., A, B, C, …)\n",
    "inputFolder = \"C:/path/to/your/input_folder\"    # <-- change this\n",
    "# Folder where the processed images will be saved\n",
    "outputFolder = \"C:/path/to/your/output_folder\"  # <-- change this\n",
    "# ----------------------------\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Configuration and Constants\n",
    "# ----------------------------\n",
    "imgSize = 500  # final output image will be imgSize x imgSize\n",
    "os.makedirs(outputFolder, exist_ok=True)\n",
    "# Initialize the hand detector (using CVZone)\n",
    "detector = HandDetector(maxHands=1)\n",
    "# For drawing hand connections\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# ----------------------------\n",
    "# Utility Functions\n",
    "# ----------------------------\n",
    "def process_and_resize(imgInput, aspectRatio, imgSize):\n",
    "    \"\"\"\n",
    "    Resize an image to fit inside a square canvas while preserving aspect ratio.\n",
    "    \"\"\"\n",
    "    channels = 1 if len(imgInput.shape) == 2 else imgInput.shape[2]\n",
    "    # Create a blank (black) square image\n",
    "    imgWhite = np.ones((imgSize, imgSize, channels), np.uint8) * 0\n",
    "    try:\n",
    "        if aspectRatio > 1:\n",
    "            # If the image is taller than wide:\n",
    "            k = imgSize / imgInput.shape[0]\n",
    "            wCal = math.ceil(k * imgInput.shape[1])\n",
    "            imgResize = cv2.resize(imgInput, (wCal, imgSize))\n",
    "            wGap = math.ceil((imgSize - wCal) / 2)\n",
    "            imgWhite[:, wGap:wCal + wGap] = imgResize\n",
    "        else:\n",
    "            # If the image is wider than tall:\n",
    "            k = imgSize / imgInput.shape[1]\n",
    "            hCal = math.ceil(k * imgInput.shape[0])\n",
    "            imgResize = cv2.resize(imgInput, (imgSize, hCal))\n",
    "            hGap = math.ceil((imgSize - hCal) / 2)\n",
    "            imgWhite[hGap:hCal + hGap, :] = imgResize\n",
    "    except Exception as e:\n",
    "        print(f\"Error during image processing: {e}\")\n",
    "        return None\n",
    "    return imgWhite\n",
    "\n",
    "def detect_skin(frame):\n",
    "    \"\"\"\n",
    "    Detect skin regions using YCrCb color space thresholds and return a binary mask.\n",
    "    \"\"\"\n",
    "    ycrcb = cv2.cvtColor(frame, cv2.COLOR_BGR2YCrCb)\n",
    "    lower_skin = np.array([0, 133, 77], dtype=np.uint8)\n",
    "    upper_skin = np.array([255, 173, 127], dtype=np.uint8)\n",
    "    mask = cv2.inRange(ycrcb, lower_skin, upper_skin)\n",
    "    \n",
    "    # Clean up noise using morphological operations\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    mask = cv2.GaussianBlur(mask, (5, 5), 0)\n",
    "    \n",
    "    # (Optional) Draw filled contours to further clean up the mask\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contour_mask = np.zeros_like(mask)\n",
    "    cv2.drawContours(contour_mask, contours, -1, 255, thickness=cv2.FILLED)\n",
    "    mask = cv2.bitwise_and(mask, contour_mask)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# ----------------------------\n",
    "# Main Processing Loop\n",
    "# ----------------------------\n",
    "# Iterate over each class folder in the input folder.\n",
    "for className in os.listdir(inputFolder):\n",
    "    classPath = os.path.join(inputFolder, className)\n",
    "    if not os.path.isdir(classPath):\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing class: {className}\")\n",
    "    # Create corresponding output folder for this class\n",
    "    outClassFolder = os.path.join(outputFolder, className)\n",
    "    os.makedirs(outClassFolder, exist_ok=True)\n",
    "\n",
    "    # Process each image file in the class folder\n",
    "    for imgFile in os.listdir(classPath):\n",
    "        imgPath = os.path.join(classPath, imgFile)\n",
    "        img = cv2.imread(imgPath)\n",
    "        if img is None:\n",
    "            print(f\"Failed to read image: {imgPath}\")\n",
    "            continue\n",
    "\n",
    "        # --- Hand Landmark Detection ---\n",
    "        hands, _ = detector.findHands(img, draw=False)\n",
    "        if hands:\n",
    "            # Use the first detected hand\n",
    "            hand = hands[0]\n",
    "            lm_list = hand['lmList']\n",
    "\n",
    "            # --- Step 1: Draw Landmarks on a Copy of the Full Image ---\n",
    "            # (Since the images are provided already, we work on the full image.)\n",
    "            img_landmarked = img.copy()\n",
    "            for lm in lm_list:\n",
    "                cv2.circle(img_landmarked, (lm[0], lm[1]), 4, (0, 0, 255), -1)\n",
    "            for connection in mp.solutions.hands.HAND_CONNECTIONS:\n",
    "                pt1 = lm_list[connection[0]]\n",
    "                pt2 = lm_list[connection[1]]\n",
    "                cv2.line(img_landmarked, (pt1[0], pt1[1]), (pt2[0], pt2[1]), (0, 0, 255), 2)\n",
    "\n",
    "            # --- Step 2: Create a Binary Image Using Skin Detection ---\n",
    "            binaryMask = detect_skin(img)\n",
    "            # Create a blank image and fill white where skin is detected\n",
    "            binary_result = np.zeros_like(img)\n",
    "            binary_result[binaryMask > 0] = [255, 255, 255]\n",
    "            # Overlay the landmarks on the binary image (drawn in black)\n",
    "            for lm in lm_list:\n",
    "                cv2.circle(binary_result, (lm[0], lm[1]), 4, (0, 0, 0), -1)\n",
    "            for connection in mp.solutions.hands.HAND_CONNECTIONS:\n",
    "                pt1 = lm_list[connection[0]]\n",
    "                pt2 = lm_list[connection[1]]\n",
    "                cv2.line(binary_result, (pt1[0], pt1[1]), (pt2[0], pt2[1]), (0, 0, 0), 2)\n",
    "\n",
    "            # --- Step 3: Resize for Consistent Output ---\n",
    "            aspectRatio = img.shape[0] / img.shape[1]\n",
    "            imgWhite = process_and_resize(binary_result, aspectRatio, imgSize)\n",
    "            if imgWhite is not None:\n",
    "                # Save the processed image to the output folder.\n",
    "                outPath = os.path.join(outClassFolder, imgFile)\n",
    "                cv2.imwrite(outPath, imgWhite)\n",
    "                print(f\"Processed and saved: {outPath}\")\n",
    "                # (Optional) Display the processed image.\n",
    "                cv2.imshow(\"Processed Image\", imgWhite)\n",
    "                cv2.waitKey(1)\n",
    "        else:\n",
    "            print(f\"No hand detected in image: {imgPath}\")\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentaition Pipeline\n",
    "\n",
    "We further preprocess the images by augmenting them based on rotations and horizontal flips\n",
    "\n",
    "Run the below cell after installing relevant dependancies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "1. Enter the folder location for the preprocessed dataset\n",
    "2. Enter the output location to save the augmented dataset\n",
    "3. There are more options for augmentation, comment or add more as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"C:/path/to/your/input_folder\"  \n",
    "output_folder = \"C:/path/to/your/output_folder\"  \n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmentation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image, ImageEnhance, ImageOps\n",
    "import numpy as np\n",
    "\n",
    "# Define augmentation functions\n",
    "\n",
    "# Rotate image by a random angle between -30 to 30 degrees\n",
    "def random_rotation(image):\n",
    "    angle = random.uniform(-30, 30)  # Rotate between -30 to 30 degrees\n",
    "    return image.rotate(angle)\n",
    "\n",
    "# Flip image horizontally with a 50% chance\n",
    "def random_flip(image):\n",
    "    if random.choice([True, False]):\n",
    "        return ImageOps.mirror(image)\n",
    "    return image\n",
    "\n",
    "# Adjust brightness by a random factor between 0.7 and 1.3\n",
    "def random_brightness(image):\n",
    "    enhancer = ImageEnhance.Brightness(image)\n",
    "    factor = random.uniform(0.7, 1.3)  # Brightness factor\n",
    "    return enhancer.enhance(factor)\n",
    "\n",
    "# Adjust contrast by a random factor between 0.7 and 1.3\n",
    "def random_contrast(image):\n",
    "    enhancer = ImageEnhance.Contrast(image)\n",
    "    factor = random.uniform(0.7, 1.3)  # Contrast factor\n",
    "    return enhancer.enhance(factor)\n",
    "\n",
    "# Add random noise to the image\n",
    "def add_random_noise(image):\n",
    "    np_image = np.array(image)\n",
    "    noise = np.random.normal(0, 25, np_image.shape).astype(np.int16)\n",
    "    noisy_image = np.clip(np_image + noise, 0, 255).astype(np.uint8)\n",
    "    return Image.fromarray(noisy_image)\n",
    "\n",
    "# Augment an image using a combination of random transformations\n",
    "# Comment out or add more functions as needed\n",
    "def augment_image(image):\n",
    "    image = random_rotation(image)\n",
    "    image = random_flip(image)\n",
    "    # image = random_brightness(image)\n",
    "    # image = random_contrast(image)\n",
    "    # image = add_random_noise(image)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To apply the augmentation to all images in the input folder and save them to the output folder\n",
    "# Iterate over all subfolders and images\n",
    "for subdir, _, files in os.walk(input_folder):\n",
    "    relative_path = os.path.relpath(subdir, input_folder)\n",
    "    output_subdir = os.path.join(output_folder, relative_path)\n",
    "    os.makedirs(output_subdir, exist_ok=True)\n",
    "\n",
    "    for file in files:\n",
    "        if file.lower().endswith(('png', 'jpg', 'jpeg', 'bmp', 'tiff')):\n",
    "            input_path = os.path.join(subdir, file)\n",
    "            output_path = os.path.join(output_subdir, file)\n",
    "\n",
    "            try:\n",
    "                with Image.open(input_path) as img:\n",
    "                    img = img.convert(\"L\")  # Ensure greyscale (black and white)\n",
    "                    augmented_img = augment_image(img)\n",
    "                    augmented_img.save(output_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {input_path}: {e}\")\n",
    "\n",
    "print(\"Data augmentation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Augmented Data With Preprocessed Dataset\n",
    "\n",
    "Creating a new folder for combined data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "1. Enter the preprocessed dataset location\n",
    "2. Enter the augmented dataset location\n",
    "3. Enter the new combined datasets save location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = \"C:/path/to/your/input_folder\"  \n",
    "dataset2 = \"C:/path/to/your/input_folder\"  \n",
    "output_dataset = \"C:/path/to/your/output_folder\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dataset, exist_ok=True)\n",
    "\n",
    "# Function to merge datasets with renaming\n",
    "def merge_datasets(source_dir, target_dir, suffix=\"\"):\n",
    "    for class_name in os.listdir(source_dir):\n",
    "        source_class_path = os.path.join(source_dir, class_name)\n",
    "        target_class_path = os.path.join(target_dir, class_name)\n",
    "        \n",
    "        if os.path.isdir(source_class_path):\n",
    "            # Create the class folder in the target if it doesn't exist\n",
    "            if not os.path.exists(target_class_path):\n",
    "                os.makedirs(target_class_path)\n",
    "            \n",
    "            for file_name in os.listdir(source_class_path):\n",
    "                source_file_path = os.path.join(source_class_path, file_name)\n",
    "                # Add the specified suffix to the file name\n",
    "                base_name, ext = os.path.splitext(file_name)\n",
    "                file_name = f\"{base_name}{suffix}{ext}\"\n",
    "                target_file_path = os.path.join(target_class_path, file_name)\n",
    "                \n",
    "                # Copy the file to the target directory\n",
    "                shutil.copy2(source_file_path, target_file_path)\n",
    "\n",
    "# Merge the main dataset\n",
    "merge_datasets(dataset1, output_dataset, suffix=\"_black\")\n",
    "\n",
    "# Merge the augmented dataset with \"_AUG\" renaming\n",
    "merge_datasets(dataset2, output_dataset, suffix=\"_white\")\n",
    "\n",
    "\n",
    "print(f\"Datasets merged into: {output_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training using ML Models\n",
    "\n",
    "In this pipeline the combined dataset will be used to train the ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "Run the below cell which will lead to a UI for training models. \n",
    "1. Enter your dataset location\n",
    "2. Selct the model to train\n",
    "3. Fill in or keep the default trining parameters\n",
    "4. Check or uncheck cross validation and write number of folds based on requirements\n",
    "5. Each epoch will show the training status in UI\n",
    "6. At end the results will be saved in modelName_RESULTS FOLDER and Models in TrainedBinary2Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warning: \n",
    "\n",
    "- Training is resource costly! \n",
    "- Ensure that you have right setup before training. \n",
    "- Process can take more than a day if trained on CPU (varies by model). \n",
    "- Highly recommended to train using GPU if Available\n",
    "- Recommend to use the already trained models in the repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Below Code to Open Training Panel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU devices: []\n"
     ]
    }
   ],
   "source": [
    "%run TrainerV3_2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-Time Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section real-time recognition will be tested out based on the predictions from trained model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "IMPORTANT: WebCam required\n",
    "\n",
    "1. Enter the models location before running\n",
    "2. The Ensure that the lighting conditions are ideal. \n",
    "3. Try to perform in front of dark background if possible, or near non reflective walls\n",
    "4. Be thourough with ASL fingerspelling signs in order for proper classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvzone.ClassificationModule import Classifier\n",
    "\n",
    "classifier = Classifier(\"C:/Users/User/OneDrive/Documents/SignLanguageApp/TrainedBinary2Model/MobileNetV2_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WebCam access and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "def detect_skin(frame):\n",
    "    # Convert to YCrCb and equalize the luminance channel\n",
    "    ycrcb = cv2.cvtColor(frame, cv2.COLOR_BGR2YCrCb)\n",
    "    y_channel = ycrcb[:, :, 0]\n",
    "    y_eq = cv2.equalizeHist(y_channel)\n",
    "    ycrcb[:, :, 0] = y_eq\n",
    "\n",
    "    # Adjusted thresholds might be needed after equalization.\n",
    "    lower_skin = np.array([0, 133, 77], dtype=np.uint8)\n",
    "    upper_skin = np.array([255, 173, 127], dtype=np.uint8)\n",
    "    mask = cv2.inRange(ycrcb, lower_skin, upper_skin)\n",
    "    \n",
    "    # Noise reduction using morphological operations\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    mask = cv2.GaussianBlur(mask, (5, 5), 0)\n",
    "    \n",
    "    # Optionally, keep only the largest contour (if needed)\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contour_mask = np.zeros_like(mask)\n",
    "    if contours:\n",
    "        cv2.drawContours(contour_mask, contours, -1, 255, thickness=cv2.FILLED)\n",
    "    mask = cv2.bitwise_and(mask, contour_mask)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\n",
    "# Initialize camera, detector\n",
    "cap = cv2.VideoCapture(0)\n",
    "detector = HandDetector(maxHands=1)\n",
    "\n",
    "\n",
    "offset = 45\n",
    "imgSize = 250\n",
    "labels = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\"]\n",
    "\n",
    "# Use Mediapipe’s hand connections to draw lines between landmarks.\n",
    "mp_hands = mp.solutions.hands\n",
    "hand_connections = mp_hands.HAND_CONNECTIONS\n",
    "\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "    imgOutput = img.copy()\n",
    "    hands, img = detector.findHands(img, draw=False)\n",
    "    \n",
    "    if hands:\n",
    "        hand = hands[0]\n",
    "        x, y, w, h = hand['bbox']\n",
    "        y1, y2 = max(0, y - offset), min(img.shape[0], y + h + offset)\n",
    "        x1, x2 = max(0, x - offset), min(img.shape[1], x + w + offset)\n",
    "        imgCrop = img[y1:y2, x1:x2]\n",
    "        \n",
    "        if imgCrop.shape[0] > 0 and imgCrop.shape[1] > 0:\n",
    "            # Draw landmarks on a copy of the cropped image (for visualization)\n",
    "            imgCrop_landmarked = imgCrop.copy()\n",
    "            if 'lmList' in hand:\n",
    "                lm_list = hand['lmList']\n",
    "                for lm in lm_list:\n",
    "                    cv2.circle(imgCrop_landmarked, (lm[0] - x1, lm[1] - y1), 4, (0, 0, 255), -1)\n",
    "                for connection in mp_hands.HAND_CONNECTIONS:\n",
    "                    if connection[0] < len(lm_list) and connection[1] < len(lm_list):\n",
    "                        pt1 = (lm_list[connection[0]][0] - x1, lm_list[connection[0]][1] - y1)\n",
    "                        pt2 = (lm_list[connection[1]][0] - x1, lm_list[connection[1]][1] - y1)\n",
    "                        cv2.line(imgCrop_landmarked, pt1, pt2, (0, 0, 255), 2)\n",
    "            \n",
    "            # Process the cropped image to create a binary image\n",
    "            binaryMask = detect_skin(imgCrop)\n",
    "            # Create a black background and set the hand area to white:\n",
    "            binary_result = np.zeros_like(imgCrop)\n",
    "            binary_result[binaryMask > 0] = [255, 255, 255]\n",
    "            \n",
    "            # Overlay the landmarks (black) on the binary image:\n",
    "            if 'lmList' in hand:\n",
    "                for lm in lm_list:\n",
    "                    cv2.circle(binary_result, (lm[0] - x1, lm[1] - y1), 4, (0, 0, 0), -1)\n",
    "                for connection in mp_hands.HAND_CONNECTIONS:\n",
    "                    pt1 = (lm_list[connection[0]][0] - x1, lm_list[connection[0]][1] - y1)\n",
    "                    pt2 = (lm_list[connection[1]][0] - x1, lm_list[connection[1]][1] - y1)\n",
    "                    cv2.line(binary_result, pt1, pt2, (0, 0, 0), 2)\n",
    "            \n",
    "            # Resize the binary_result to a fixed size (e.g., 250x250) while preserving the aspect ratio\n",
    "            aspectRatio = h / w\n",
    "            imgWhite = np.ones((imgSize, imgSize), np.uint8) * 0\n",
    "            if aspectRatio > 1:\n",
    "                k = imgSize / h\n",
    "                wCal = math.ceil(k * w)\n",
    "                imgResize = cv2.resize(binary_result, (wCal, imgSize))\n",
    "                wGap = math.ceil((imgSize - wCal) / 2)\n",
    "                imgWhite[:, wGap:wCal + wGap] = cv2.cvtColor(imgResize, cv2.COLOR_BGR2GRAY)\n",
    "            else:\n",
    "                k = imgSize / w\n",
    "                hCal = math.ceil(k * h)\n",
    "                imgResize = cv2.resize(binary_result, (imgSize, hCal))\n",
    "                hGap = math.ceil((imgSize - hCal) / 2)\n",
    "                imgWhite[hGap:hCal + hGap, :] = cv2.cvtColor(imgResize, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # You can now pass imgWhite (or its RGB version) to your classifier.\n",
    "            imgWhiteRGB = cv2.cvtColor(imgWhite, cv2.COLOR_GRAY2BGR)\n",
    "            prediction, index = classifier.getPrediction(imgWhiteRGB, draw=False)\n",
    "            \n",
    "            # Display classification results if the prediction confidence is high enough.\n",
    "            if prediction[index] > 0.75 and 0 <= index < len(labels):\n",
    "                cv2.rectangle(imgOutput, (x - offset, y - offset - 50),\n",
    "                              (x - offset + 90, y - offset - 50 + 50), (255, 0, 255), cv2.FILLED)\n",
    "                cv2.putText(imgOutput, labels[index], (x, y - 26),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 1.7, (255, 255, 255), 2)\n",
    "                cv2.rectangle(imgOutput, (x - offset, y - offset),\n",
    "                              (x + w + offset, y + h + offset), (255, 0, 255), 4)\n",
    "            \n",
    "            cv2.imshow(\"Processed Binary Image\", imgWhite)\n",
    "            cv2.imshow(\"Hand Landmarks\", imgCrop_landmarked)\n",
    "    \n",
    "    cv2.imshow(\"Image\", imgOutput)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-Time recognition UI/Tkinter App\n",
    "\n",
    "#### Features:\n",
    "\n",
    "1. Panel displaying main frame with prediction label, binary frame, landmark frame to understand workings of the process.\n",
    "2. Word typing. Use space bar to save the predicted letter. For white_space remove hand from frame and press space bar.\n",
    "3. --More to be added--\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    " \n",
    "IMPORTANT: WebCam required\n",
    "\n",
    "1. Change the model name in \"ASLRecogAPP.py\" before running the app.\n",
    "2. The Ensure that the lighting conditions are ideal. \n",
    "3. Try to perform in front of dark background if possible, or near non reflective walls\n",
    "4. Be thourough with ASL fingerspelling signs in order for proper classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Below Cell to open the app:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Labels Found\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%run ASLRecogAPP.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Num GPUs Available:\", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"GPU being used:\", tf.test.gpu_device_name())\n",
    "\n",
    "tf.debugging.set_log_device_placement(True)  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe Selfie Segmentation\n",
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "segmentation = mp_selfie_segmentation.SelfieSegmentation(model_selection=1)\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)  # Webcam input\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Flip the frame for a mirror effect\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Convert to RGB for MediaPipe\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame\n",
    "    results = segmentation.process(rgb_frame)\n",
    "\n",
    "    # Create a mask where the hand is detected (white = hand, black = background)\n",
    "    mask = results.segmentation_mask\n",
    "    threshold = 0.5  # Adjust threshold for better accuracy\n",
    "    mask_binary = (mask > threshold).astype(np.uint8) * 255\n",
    "\n",
    "    # Convert mask to 3 channels\n",
    "    mask_binary = cv2.cvtColor(mask_binary, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    # Apply the mask to the original frame (keep hand, remove background)\n",
    "    segmented_hand = cv2.bitwise_and(frame, mask_binary)\n",
    "\n",
    "    # Show results\n",
    "    cv2.imshow(\"Original\", frame)\n",
    "    cv2.imshow(\"Segmented Hand\", segmented_hand)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting collection for: A\n",
      "Error during image processing: could not broadcast input array from shape (501,500,3) into shape (500,500,3)\n",
      "Error during image processing: could not broadcast input array from shape (501,500,3) into shape (500,500,3)\n"
     ]
    }
   ],
   "source": [
    "# import cv2\n",
    "# from cvzone.HandTrackingModule import HandDetector\n",
    "# import numpy as np\n",
    "# import math\n",
    "# import os\n",
    "# import mediapipe as mp\n",
    "\n",
    "# # ----------------------------\n",
    "# # Initialize MediaPipe Selfie Segmentation (from 1st code)\n",
    "# # ----------------------------\n",
    "# mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "# segmentation = mp_selfie_segmentation.SelfieSegmentation(model_selection=1)\n",
    "\n",
    "# # ----------------------------\n",
    "# # Initialize Webcam and Hand Detector (from 2nd code)\n",
    "# # ----------------------------\n",
    "# cap = cv2.VideoCapture(0)\n",
    "# detector = HandDetector(maxHands=1)\n",
    "\n",
    "# # Constants and folders for saving images\n",
    "# imgSize = 500\n",
    "# baseFolder = \"/SLangDataset/new_Blmark_data\"\n",
    "# # List of letters A-Y (adjust as needed)\n",
    "# letters = [chr(i) for i in range(ord('A'), ord('Y') + 1)]\n",
    "# maxImages = 2000          # Total images to capture per class\n",
    "# paddingFactor = 0.45      # Padding percentage\n",
    "\n",
    "# mp_hands = mp.solutions.hands\n",
    "\n",
    "# # ----------------------------\n",
    "# # Utility function: Process and resize image for saving\n",
    "# # ----------------------------\n",
    "# def process_and_resize(imgCrop, aspectRatio, imgSize):\n",
    "#     channels = 1 if len(imgCrop.shape) == 2 else imgCrop.shape[2]\n",
    "#     imgWhite = np.ones((imgSize, imgSize, channels), np.uint8) * 0\n",
    "#     try:\n",
    "#         if aspectRatio > 1:\n",
    "#             # If height > width:\n",
    "#             k = imgSize / imgCrop.shape[0]\n",
    "#             wCal = math.ceil(k * imgCrop.shape[1])\n",
    "#             imgResize = cv2.resize(imgCrop, (wCal, imgSize))\n",
    "#             wGap = math.ceil((imgSize - wCal) / 2)\n",
    "#             imgWhite[:, wGap:wCal + wGap] = imgResize\n",
    "#         else:\n",
    "#             # If width >= height:\n",
    "#             k = imgSize / imgCrop.shape[1]\n",
    "#             hCal = math.ceil(k * imgCrop.shape[0])\n",
    "#             imgResize = cv2.resize(imgCrop, (imgSize, hCal))\n",
    "#             hGap = math.ceil((imgSize - hCal) / 2)\n",
    "#             imgWhite[hGap:hCal + hGap, :] = imgResize\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error during image processing: {e}\")\n",
    "#         return None\n",
    "#     return imgWhite\n",
    "\n",
    "# # ----------------------------\n",
    "# # Utility function: Detect skin using YCrCb thresholds (binarization)\n",
    "# # ----------------------------\n",
    "# def detect_skin(frame):\n",
    "#     ycrcb = cv2.cvtColor(frame, cv2.COLOR_BGR2YCrCb)\n",
    "#     lower_skin = np.array([0, 133, 77], dtype=np.uint8)\n",
    "#     upper_skin = np.array([255, 173, 127], dtype=np.uint8)\n",
    "#     mask = cv2.inRange(ycrcb, lower_skin, upper_skin)\n",
    "    \n",
    "#     # Clean up noise\n",
    "#     kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "#     mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "#     mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "#     mask = cv2.GaussianBlur(mask, (5, 5), 0)\n",
    "    \n",
    "#     # (Optional) Fill in contours to clean up the mask\n",
    "#     contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "#     contour_mask = np.zeros_like(mask)\n",
    "#     cv2.drawContours(contour_mask, contours, -1, 255, thickness=cv2.FILLED)\n",
    "#     mask = cv2.bitwise_and(mask, contour_mask)\n",
    "    \n",
    "#     return mask\n",
    "\n",
    "# # ----------------------------\n",
    "# # Main Loop: Process each class (letter)\n",
    "# # ----------------------------\n",
    "# for className in letters:\n",
    "#     print(f\"Starting collection for: {className}\")\n",
    "#     folder = os.path.join(baseFolder, className)\n",
    "#     os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "#     counter, collecting = 0, False\n",
    "\n",
    "#     while counter < maxImages:\n",
    "#         success, img = cap.read()\n",
    "#         if not success:\n",
    "#             print(\"Camera access failed.\")\n",
    "#             break\n",
    "\n",
    "#         # Detect hand in the full image\n",
    "#         hands, _ = detector.findHands(img, draw=False)\n",
    "#         if hands:\n",
    "#             # Use the first detected hand\n",
    "#             hand = hands[0]\n",
    "#             bbox = hand['bbox']       # [x, y, w, h]\n",
    "#             lm_list = hand['lmList']    # list of landmarks in full-image coordinates\n",
    "#             x, y, w, h = bbox\n",
    "\n",
    "#             # Calculate padding based on hand size\n",
    "#             xPad = int(w * paddingFactor)\n",
    "#             yPad = int(h * paddingFactor)\n",
    "\n",
    "#             # Compute crop boundaries (making sure they stay within image bounds)\n",
    "#             crop_x1 = max(0, x - xPad)\n",
    "#             crop_y1 = max(0, y - yPad)\n",
    "#             crop_x2 = min(x + w + xPad, img.shape[1])\n",
    "#             crop_y2 = min(y + h + yPad, img.shape[0])\n",
    "#             imgCrop = img[crop_y1:crop_y2, crop_x1:crop_x2]\n",
    "\n",
    "#             if imgCrop.size > 0:\n",
    "#                 # ============================================================\n",
    "#                 # STEP A: Apply segmentation to remove background (from 1st code)\n",
    "#                 # ============================================================\n",
    "#                 # Convert the cropped image to RGB as required by MediaPipe segmentation\n",
    "#                 rgb_crop = cv2.cvtColor(imgCrop, cv2.COLOR_BGR2RGB)\n",
    "#                 results_seg = segmentation.process(rgb_crop)\n",
    "#                 mask_seg = results_seg.segmentation_mask\n",
    "#                 seg_threshold = 0.5  # Adjust threshold if necessary\n",
    "#                 mask_binary_seg = (mask_seg > seg_threshold).astype(np.uint8) * 255\n",
    "#                 mask_binary_seg = cv2.cvtColor(mask_binary_seg, cv2.COLOR_GRAY2BGR)\n",
    "#                 segmented_crop = cv2.bitwise_and(imgCrop, mask_binary_seg)\n",
    "                \n",
    "#                 # ============================================================\n",
    "#                 # STEP 1: Draw landmarks on the segmented crop (adjust coordinates)\n",
    "#                 # ============================================================\n",
    "#                 imgCrop_landmarked = segmented_crop.copy()\n",
    "#                 for lm in lm_list:\n",
    "#                     adj_x = lm[0] - crop_x1\n",
    "#                     adj_y = lm[1] - crop_y1\n",
    "#                     cv2.circle(imgCrop_landmarked, (adj_x, adj_y), 4, (0, 0, 255), -1)\n",
    "#                 for connection in mp.solutions.hands.HAND_CONNECTIONS:\n",
    "#                     pt1 = lm_list[connection[0]]\n",
    "#                     pt2 = lm_list[connection[1]]\n",
    "#                     pt1_adjusted = (pt1[0] - crop_x1, pt1[1] - crop_y1)\n",
    "#                     pt2_adjusted = (pt2[0] - crop_x1, pt2[1] - crop_y1)\n",
    "#                     cv2.line(imgCrop_landmarked, pt1_adjusted, pt2_adjusted, (0, 0, 255), 2)\n",
    "\n",
    "#                 # ============================================================\n",
    "#                 # STEP 2: Binarize the segmented crop with landmarks overlaid\n",
    "#                 # ============================================================\n",
    "#                 # Use the segmented crop (without landmarks) to generate a binary skin mask\n",
    "#                 binaryMask = detect_skin(segmented_crop)\n",
    "#                 binary_result = np.zeros_like(segmented_crop)\n",
    "#                 binary_result[binaryMask > 0] = [255, 255, 255]\n",
    "#                 # Then overlay the landmarks (drawn in black) onto the binary image.\n",
    "#                 for lm in lm_list:\n",
    "#                     adj_x = lm[0] - crop_x1\n",
    "#                     adj_y = lm[1] - crop_y1\n",
    "#                     cv2.circle(binary_result, (adj_x, adj_y), 4, (0, 0, 0), -1)\n",
    "#                 for connection in mp.solutions.hands.HAND_CONNECTIONS:\n",
    "#                     pt1 = lm_list[connection[0]]\n",
    "#                     pt2 = lm_list[connection[1]]\n",
    "#                     pt1_adjusted = (pt1[0] - crop_x1, pt1[1] - crop_y1)\n",
    "#                     pt2_adjusted = (pt2[0] - crop_x1, pt2[1] - crop_y1)\n",
    "#                     cv2.line(binary_result, pt1_adjusted, pt2_adjusted, (0, 0, 0), 2)\n",
    "\n",
    "#                 # ============================================================\n",
    "#                 # STEP 3: Resize the binary image for saving/visualization\n",
    "#                 # ============================================================\n",
    "#                 aspectRatio = (crop_y2 - crop_y1) / (crop_x2 - crop_x1)\n",
    "#                 imgWhite = process_and_resize(binary_result, aspectRatio, imgSize)\n",
    "#                 if imgWhite is not None:\n",
    "#                     cv2.imshow(\"Processed Binary Image\", imgWhite)\n",
    "#                     if collecting:\n",
    "#                         counter += 1\n",
    "#                         savePath = os.path.join(folder, f\"{className.lower()}_{counter}.jpg\")\n",
    "#                         cv2.imwrite(savePath, imgWhite)\n",
    "#                         print(f\"Saved {counter}/{maxImages} images for {className}\")\n",
    "\n",
    "#         # Show the original live feed (for reference)\n",
    "#         cv2.imshow(\"Live Feed with Landmarks\", img)\n",
    "#         key = cv2.waitKey(1)\n",
    "#         if key == ord('s'):\n",
    "#             collecting = True\n",
    "#         if key == ord('p'):\n",
    "#             collecting = False\n",
    "\n",
    "#     print(f\"Completed collection for {className}\")\n",
    "#     input(\"Press Enter for next class.\")\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "import cv2\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import mediapipe as mp\n",
    "\n",
    "# ----------------------------\n",
    "# Initialize MediaPipe Selfie Segmentation (from 1st code)\n",
    "# ----------------------------\n",
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "segmentation = mp_selfie_segmentation.SelfieSegmentation(model_selection=1)\n",
    "\n",
    "# ----------------------------\n",
    "# Initialize Webcam and Hand Detector (from 2nd code)\n",
    "# ----------------------------\n",
    "cap = cv2.VideoCapture(0)\n",
    "detector = HandDetector(maxHands=1)\n",
    "\n",
    "# Constants and folders for saving images\n",
    "imgSize = 500\n",
    "baseFolder = \"/SLangDataset/new_Blmark_data\"\n",
    "# List of letters A-Y (adjust as needed)\n",
    "letters = [chr(i) for i in range(ord('A'), ord('Y') + 1)]\n",
    "maxImages = 2000          # Total images to capture per class\n",
    "paddingFactor = 0.45      # Padding percentage\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# ----------------------------\n",
    "# Utility function: Process and resize image for saving\n",
    "# ----------------------------\n",
    "def process_and_resize(imgCrop, aspectRatio, imgSize):\n",
    "    channels = 1 if len(imgCrop.shape) == 2 else imgCrop.shape[2]\n",
    "    imgWhite = np.ones((imgSize, imgSize, channels), np.uint8) * 0\n",
    "    try:\n",
    "        if aspectRatio > 1:\n",
    "            # If height > width:\n",
    "            k = imgSize / imgCrop.shape[0]\n",
    "            wCal = math.ceil(k * imgCrop.shape[1])\n",
    "            imgResize = cv2.resize(imgCrop, (wCal, imgSize))\n",
    "            wGap = math.ceil((imgSize - wCal) / 2)\n",
    "            imgWhite[:, wGap:wCal + wGap] = imgResize\n",
    "        else:\n",
    "            # If width >= height:\n",
    "            k = imgSize / imgCrop.shape[1]\n",
    "            hCal = math.ceil(k * imgCrop.shape[0])\n",
    "            imgResize = cv2.resize(imgCrop, (imgSize, hCal))\n",
    "            hGap = math.ceil((imgSize - hCal) / 2)\n",
    "            imgWhite[hGap:hCal + hGap, :] = imgResize\n",
    "    except Exception as e:\n",
    "        print(f\"Error during image processing: {e}\")\n",
    "        return None\n",
    "    return imgWhite\n",
    "\n",
    "# ----------------------------\n",
    "# Utility function: Detect skin using YCrCb thresholds (binarization)\n",
    "# ----------------------------\n",
    "def detect_skin(frame):\n",
    "    ycrcb = cv2.cvtColor(frame, cv2.COLOR_BGR2YCrCb)\n",
    "    lower_skin = np.array([0, 133, 77], dtype=np.uint8)\n",
    "    upper_skin = np.array([255, 173, 127], dtype=np.uint8)\n",
    "    mask = cv2.inRange(ycrcb, lower_skin, upper_skin)\n",
    "    \n",
    "    # Clean up noise\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    mask = cv2.GaussianBlur(mask, (5, 5), 0)\n",
    "    \n",
    "    # (Optional) Fill in contours to clean up the mask\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contour_mask = np.zeros_like(mask)\n",
    "    cv2.drawContours(contour_mask, contours, -1, 255, thickness=cv2.FILLED)\n",
    "    mask = cv2.bitwise_and(mask, contour_mask)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# ----------------------------\n",
    "# Main Loop: Process each class (letter)\n",
    "# ----------------------------\n",
    "for className in letters:\n",
    "    print(f\"Starting collection for: {className}\")\n",
    "    folder = os.path.join(baseFolder, className)\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    counter, collecting = 0, False\n",
    "\n",
    "    while counter < maxImages:\n",
    "        success, img = cap.read()\n",
    "        if not success:\n",
    "            print(\"Camera access failed.\")\n",
    "            break\n",
    "\n",
    "        # Detect hand in the full image\n",
    "        hands, _ = detector.findHands(img, draw=False)\n",
    "        if hands:\n",
    "            # Use the first detected hand\n",
    "            hand = hands[0]\n",
    "            bbox = hand['bbox']       # [x, y, w, h]\n",
    "            lm_list = hand['lmList']    # list of landmarks in full-image coordinates\n",
    "            x, y, w, h = bbox\n",
    "\n",
    "            # Calculate padding based on hand size\n",
    "            xPad = int(w * paddingFactor)\n",
    "            yPad = int(h * paddingFactor)\n",
    "\n",
    "            # Compute crop boundaries (making sure they stay within image bounds)\n",
    "            crop_x1 = max(0, x - xPad)\n",
    "            crop_y1 = max(0, y - yPad)\n",
    "            crop_x2 = min(x + w + xPad, img.shape[1])\n",
    "            crop_y2 = min(y + h + yPad, img.shape[0])\n",
    "            imgCrop = img[crop_y1:crop_y2, crop_x1:crop_x2]\n",
    "\n",
    "            if imgCrop.size > 0:\n",
    "                # ============================================================\n",
    "                # STEP A: Apply segmentation to remove background (from 1st code)\n",
    "                # ============================================================\n",
    "                # Convert the cropped image to RGB as required by MediaPipe segmentation\n",
    "                rgb_crop = cv2.cvtColor(imgCrop, cv2.COLOR_BGR2RGB)\n",
    "                results_seg = segmentation.process(rgb_crop)\n",
    "                mask_seg = results_seg.segmentation_mask\n",
    "                seg_threshold = 0.5  # Adjust threshold if necessary\n",
    "                mask_binary_seg = (mask_seg > seg_threshold).astype(np.uint8) * 255\n",
    "                mask_binary_seg = cv2.cvtColor(mask_binary_seg, cv2.COLOR_GRAY2BGR)\n",
    "                segmented_crop = cv2.bitwise_and(imgCrop, mask_binary_seg)\n",
    "                \n",
    "                # ---------------------------\n",
    "                # Show the segmented crop (background removed)\n",
    "                # ---------------------------\n",
    "                cv2.imshow(\"Segmented Crop\", segmented_crop)\n",
    "                \n",
    "                # ============================================================\n",
    "                # STEP 1: Draw landmarks on the segmented crop (adjust coordinates)\n",
    "                # ============================================================\n",
    "                imgCrop_landmarked = segmented_crop.copy()\n",
    "                for lm in lm_list:\n",
    "                    adj_x = lm[0] - crop_x1\n",
    "                    adj_y = lm[1] - crop_y1\n",
    "                    cv2.circle(imgCrop_landmarked, (adj_x, adj_y), 4, (0, 0, 255), -1)\n",
    "                for connection in mp.solutions.hands.HAND_CONNECTIONS:\n",
    "                    pt1 = lm_list[connection[0]]\n",
    "                    pt2 = lm_list[connection[1]]\n",
    "                    pt1_adjusted = (pt1[0] - crop_x1, pt1[1] - crop_y1)\n",
    "                    pt2_adjusted = (pt2[0] - crop_x1, pt2[1] - crop_y1)\n",
    "                    cv2.line(imgCrop_landmarked, pt1_adjusted, pt2_adjusted, (0, 0, 255), 2)\n",
    "\n",
    "                # ============================================================\n",
    "                # STEP 2: Binarize the segmented crop with landmarks overlaid\n",
    "                # ============================================================\n",
    "                # Use the segmented crop (without landmarks) to generate a binary skin mask\n",
    "                binaryMask = detect_skin(segmented_crop)\n",
    "                binary_result = np.zeros_like(segmented_crop)\n",
    "                binary_result[binaryMask > 0] = [255, 255, 255]\n",
    "                # Then overlay the landmarks (drawn in black) onto the binary image.\n",
    "                for lm in lm_list:\n",
    "                    adj_x = lm[0] - crop_x1\n",
    "                    adj_y = lm[1] - crop_y1\n",
    "                    cv2.circle(binary_result, (adj_x, adj_y), 4, (0, 0, 0), -1)\n",
    "                for connection in mp.solutions.hands.HAND_CONNECTIONS:\n",
    "                    pt1 = lm_list[connection[0]]\n",
    "                    pt2 = lm_list[connection[1]]\n",
    "                    pt1_adjusted = (pt1[0] - crop_x1, pt1[1] - crop_y1)\n",
    "                    pt2_adjusted = (pt2[0] - crop_x1, pt2[1] - crop_y1)\n",
    "                    cv2.line(binary_result, pt1_adjusted, pt2_adjusted, (0, 0, 0), 2)\n",
    "\n",
    "                # ============================================================\n",
    "                # STEP 3: Resize the binary image for saving/visualization\n",
    "                # ============================================================\n",
    "                aspectRatio = (crop_y2 - crop_y1) / (crop_x2 - crop_x1)\n",
    "                imgWhite = process_and_resize(binary_result, aspectRatio, imgSize)\n",
    "                if imgWhite is not None:\n",
    "                    cv2.imshow(\"Processed Binary Image\", imgWhite)\n",
    "                    if collecting:\n",
    "                        counter += 1\n",
    "                        savePath = os.path.join(folder, f\"{className.lower()}_{counter}.jpg\")\n",
    "                        cv2.imwrite(savePath, imgWhite)\n",
    "                        print(f\"Saved {counter}/{maxImages} images for {className}\")\n",
    "\n",
    "        # Show the original live feed (for reference)\n",
    "        cv2.imshow(\"Live Feed with Landmarks\", img)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('s'):\n",
    "            collecting = True\n",
    "        if key == ord('p'):\n",
    "            collecting = False\n",
    "\n",
    "    print(f\"Completed collection for {className}\")\n",
    "    input(\"Press Enter for next class.\")\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting collection for: A\n",
      "Error during image processing: could not broadcast input array from shape (501,500,3) into shape (500,500,3)\n",
      "Error during image processing: could not broadcast input array from shape (501,500,3) into shape (500,500,3)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import mediapipe as mp\n",
    "\n",
    "# ----------------------------\n",
    "# Initialize MediaPipe Selfie Segmentation\n",
    "# ----------------------------\n",
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "segmentation = mp_selfie_segmentation.SelfieSegmentation(model_selection=1)\n",
    "\n",
    "# ----------------------------\n",
    "# Initialize Webcam and Hand Detector\n",
    "# ----------------------------\n",
    "cap = cv2.VideoCapture(0)\n",
    "detector = HandDetector(maxHands=1)\n",
    "\n",
    "# Constants and folders for saving images\n",
    "imgSize = 500\n",
    "baseFolder = \"/SignLanguageApp/SLangDataset/new_Blmark_data\"\n",
    "letters = [chr(i) for i in range(ord('A'), ord('Y') + 1)]  # List of letters A-Y\n",
    "maxImages = 2000         # Total images to capture per class\n",
    "paddingFactor = 0.45     # Padding percentage\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# ----------------------------\n",
    "# Utility function: Process and resize image for saving\n",
    "# ----------------------------\n",
    "def process_and_resize(imgCrop, aspectRatio, imgSize):\n",
    "    channels = 1 if len(imgCrop.shape) == 2 else imgCrop.shape[2]\n",
    "    imgWhite = np.ones((imgSize, imgSize, channels), np.uint8) * 0\n",
    "    try:\n",
    "        if aspectRatio > 1:\n",
    "            # Height > width:\n",
    "            k = imgSize / imgCrop.shape[0]\n",
    "            wCal = math.ceil(k * imgCrop.shape[1])\n",
    "            imgResize = cv2.resize(imgCrop, (wCal, imgSize))\n",
    "            wGap = math.ceil((imgSize - wCal) / 2)\n",
    "            imgWhite[:, wGap:wCal + wGap] = imgResize\n",
    "        else:\n",
    "            # Width >= height:\n",
    "            k = imgSize / imgCrop.shape[1]\n",
    "            hCal = math.ceil(k * imgCrop.shape[0])\n",
    "            imgResize = cv2.resize(imgCrop, (imgSize, hCal))\n",
    "            hGap = math.ceil((imgSize - hCal) / 2)\n",
    "            imgWhite[hGap:hCal + hGap, :] = imgResize\n",
    "    except Exception as e:\n",
    "        print(f\"Error during image processing: {e}\")\n",
    "        return None\n",
    "    return imgWhite\n",
    "\n",
    "# ----------------------------\n",
    "# (Optional) Utility function: Detect skin using YCrCb thresholds\n",
    "# (Not used in the updated processing)\n",
    "# ----------------------------\n",
    "def detect_skin(frame):\n",
    "    ycrcb = cv2.cvtColor(frame, cv2.COLOR_BGR2YCrCb)\n",
    "    lower_skin = np.array([0, 133, 77], dtype=np.uint8)\n",
    "    upper_skin = np.array([255, 173, 127], dtype=np.uint8)\n",
    "    mask = cv2.inRange(ycrcb, lower_skin, upper_skin)\n",
    "    \n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    mask = cv2.GaussianBlur(mask, (5, 5), 0)\n",
    "    \n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contour_mask = np.zeros_like(mask)\n",
    "    cv2.drawContours(contour_mask, contours, -1, 255, thickness=cv2.FILLED)\n",
    "    mask = cv2.bitwise_and(mask, contour_mask)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# ----------------------------\n",
    "# Main Loop: Process each class (letter)\n",
    "# ----------------------------\n",
    "for className in letters:\n",
    "    print(f\"Starting collection for: {className}\")\n",
    "    folder = os.path.join(baseFolder, className)\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    counter, collecting = 0, False\n",
    "\n",
    "    while counter < maxImages:\n",
    "        success, img = cap.read()\n",
    "        if not success:\n",
    "            print(\"Camera access failed.\")\n",
    "            break\n",
    "\n",
    "        # Detect hand in the full image\n",
    "        hands, _ = detector.findHands(img, draw=False)\n",
    "        if hands:\n",
    "            # Use the first detected hand\n",
    "            hand = hands[0]\n",
    "            bbox = hand['bbox']       # [x, y, w, h]\n",
    "            lm_list = hand['lmList']    # List of landmarks in full-image coordinates\n",
    "            x, y, w, h = bbox\n",
    "\n",
    "            # Calculate padding based on hand size\n",
    "            xPad = int(w * paddingFactor)\n",
    "            yPad = int(h * paddingFactor)\n",
    "\n",
    "            # Compute crop boundaries (ensure they stay within image bounds)\n",
    "            crop_x1 = max(0, x - xPad)\n",
    "            crop_y1 = max(0, y - yPad)\n",
    "            crop_x2 = min(x + w + xPad, img.shape[1])\n",
    "            crop_y2 = min(y + h + yPad, img.shape[0])\n",
    "            imgCrop = img[crop_y1:crop_y2, crop_x1:crop_x2]\n",
    "\n",
    "            if imgCrop.size > 0:\n",
    "                # -----------------------------------------------------\n",
    "                # STEP A: Apply segmentation to remove background\n",
    "                # -----------------------------------------------------\n",
    "                rgb_crop = cv2.cvtColor(imgCrop, cv2.COLOR_BGR2RGB)\n",
    "                results_seg = segmentation.process(rgb_crop)\n",
    "                mask_seg = results_seg.segmentation_mask\n",
    "                seg_threshold = 0.5  # Adjust threshold if necessary\n",
    "                mask_binary_seg = (mask_seg > seg_threshold).astype(np.uint8) * 255\n",
    "                mask_binary_seg = cv2.cvtColor(mask_binary_seg, cv2.COLOR_GRAY2BGR)\n",
    "                segmented_crop = cv2.bitwise_and(imgCrop, mask_binary_seg)\n",
    "                \n",
    "                # Show the segmented crop (background removed)\n",
    "                cv2.imshow(\"Segmented Crop\", segmented_crop)\n",
    "                \n",
    "                # -----------------------------------------------------\n",
    "                # STEP 1: Draw landmarks on the segmented crop\n",
    "                # -----------------------------------------------------\n",
    "                imgCrop_landmarked = segmented_crop.copy()\n",
    "                for lm in lm_list:\n",
    "                    adj_x = lm[0] - crop_x1\n",
    "                    adj_y = lm[1] - crop_y1\n",
    "                    cv2.circle(imgCrop_landmarked, (adj_x, adj_y), 4, (0, 0, 255), -1)\n",
    "                for connection in mp.solutions.hands.HAND_CONNECTIONS:\n",
    "                    pt1 = lm_list[connection[0]]\n",
    "                    pt2 = lm_list[connection[1]]\n",
    "                    pt1_adjusted = (pt1[0] - crop_x1, pt1[1] - crop_y1)\n",
    "                    pt2_adjusted = (pt2[0] - crop_x1, pt2[1] - crop_y1)\n",
    "                    cv2.line(imgCrop_landmarked, pt1_adjusted, pt2_adjusted, (0, 0, 255), 2)\n",
    "                \n",
    "                # -----------------------------------------------------\n",
    "                # STEP 2: Convert the segmented crop directly to a binary image\n",
    "                # (Skipping additional skin range checking)\n",
    "                # -----------------------------------------------------\n",
    "                # Create a blank image for the binary result\n",
    "                binary_result = np.zeros_like(segmented_crop)\n",
    "                # Convert the segmented crop to grayscale and threshold it\n",
    "                gray = cv2.cvtColor(segmented_crop, cv2.COLOR_BGR2GRAY)\n",
    "                _, binary_from_seg = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "                binary_result[binary_from_seg > 0] = [255, 255, 255]\n",
    "                # Overlay landmarks (drawn in black) on the binary image\n",
    "                for lm in lm_list:\n",
    "                    adj_x = lm[0] - crop_x1\n",
    "                    adj_y = lm[1] - crop_y1\n",
    "                    cv2.circle(binary_result, (adj_x, adj_y), 4, (0, 0, 0), -1)\n",
    "                for connection in mp.solutions.hands.HAND_CONNECTIONS:\n",
    "                    pt1 = lm_list[connection[0]]\n",
    "                    pt2 = lm_list[connection[1]]\n",
    "                    pt1_adjusted = (pt1[0] - crop_x1, pt1[1] - crop_y1)\n",
    "                    pt2_adjusted = (pt2[0] - crop_x1, pt2[1] - crop_y1)\n",
    "                    cv2.line(binary_result, pt1_adjusted, pt2_adjusted, (0, 0, 0), 2)\n",
    "                \n",
    "                # -----------------------------------------------------\n",
    "                # STEP 3: Resize the binary image for saving/visualization\n",
    "                # -----------------------------------------------------\n",
    "                aspectRatio = (crop_y2 - crop_y1) / (crop_x2 - crop_x1)\n",
    "                imgWhite = process_and_resize(binary_result, aspectRatio, imgSize)\n",
    "                if imgWhite is not None:\n",
    "                    cv2.imshow(\"Processed Binary Image\", imgWhite)\n",
    "                    if collecting:\n",
    "                        counter += 1\n",
    "                        savePath = os.path.join(folder, f\"{className.lower()}_{counter}.jpg\")\n",
    "                        cv2.imwrite(savePath, imgWhite)\n",
    "                        print(f\"Saved {counter}/{maxImages} images for {className}\")\n",
    "\n",
    "        # Show the original live feed (for reference)\n",
    "        cv2.imshow(\"Live Feed with Landmarks\", img)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('s'):\n",
    "            collecting = True\n",
    "        if key == ord('p'):\n",
    "            collecting = False\n",
    "\n",
    "    print(f\"Completed collection for {className}\")\n",
    "    input(\"Press Enter for next class.\")\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "F21DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
