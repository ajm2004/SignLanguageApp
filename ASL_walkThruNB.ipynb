{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASL Fingerspelling Recognition\n",
    "\n",
    "In this notebook a brief setup guide along with the processes on how data was collected, preprocesses and trained will be discussed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup (Windows Only)\n",
    "\n",
    "For running and testing, please install the below libraries:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For data collection and preprocessing:\n",
    "(Open-CV, cvzone, NumPy, MediaPipe, Pillow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opencv-python cvzone numpy mediapipe Pillow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. For ML models and Training: (Tensorflow, Tensorflow-Hub, Matplotlib, Seaborn)\n",
    "\n",
    "Please install on the same enviroment. (Required for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow tensorflow-hub matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. For Real-Time Recognition: [Same as collection and preprocessing] (Open-CV, cvzone, NumPy, MediaPipe, Pillow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !! Skip if installed using step-1\n",
    "%pip install opencv-python cvzone numpy mediapipe Pillow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. For text to speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyttsx3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "\n",
    "1. There may protobuf based errors when trying to run.\n",
    "Please install/re-install lower version of protobuf if faced (3.20.x or lower)\n",
    "\n",
    "2. There may be mediapipe based errors, please reinstall medipipe suitable for above protobuf version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Collection Pipeline\n",
    "\n",
    "For the process of collecting images of hand along with basic preprocessing while collection.\n",
    "\n",
    "Run the below cell after installing relevant dependancies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "IMPORTANT: WebCam required\n",
    "\n",
    "1. Below script collects 600, 500x500 images for each alphabets/class (A to Y, no Z) \n",
    "2. To start collection press S on the opencv frame (webcam frame)\n",
    "3. Fill in the baseFolder variable with the repository location to save the classes and images.\n",
    "4. After collecting images for the class, ENTER KEY prompt will pop up in VS-code search-bar/top-panel. Press enter there which will start process for next class\n",
    "5. Repeat steps 2 and 3 for subsequent collections\n",
    "6. After the collection of final class the program will exit/halt.\n",
    "7. The images can be found in the baseFolder repository\n",
    "\n",
    "### Preprocessings:\n",
    "\n",
    "1. Cropping hands from complete frame\n",
    "2. Adding hand landmarks to cropped frame\n",
    "3. Converting cropped frame to binary image (black and white pixels only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import mediapipe as mp\n",
    "\n",
    "#--------------------\n",
    "baseFolder = \"C:/path/to/your/output_folder\"  # <-- change this\n",
    "#--------------------\n",
    "\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize hand detector\n",
    "detector = HandDetector(maxHands=1)\n",
    "\n",
    "# Constants\n",
    "imgSize = 500\n",
    "# List of letters A-Y (adjust as needed)\n",
    "letters = [chr(i) for i in range(ord('A'), ord('Y') + 1)]\n",
    "maxImages = 600          # Total images to capture per class\n",
    "paddingFactor = 0.45     # Padding percentage\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# Function to process and resize images (canvas will match input channels)\n",
    "def process_and_resize(imgCrop, aspectRatio, imgSize):\n",
    "    channels = 1 if len(imgCrop.shape) == 2 else imgCrop.shape[2]\n",
    "    imgWhite = np.ones((imgSize, imgSize, channels), np.uint8) * 0\n",
    "    try:\n",
    "        if aspectRatio > 1:\n",
    "            # If height > width:\n",
    "            k = imgSize / imgCrop.shape[0]\n",
    "            wCal = math.ceil(k * imgCrop.shape[1])\n",
    "            imgResize = cv2.resize(imgCrop, (wCal, imgSize))\n",
    "            wGap = math.ceil((imgSize - wCal) / 2)\n",
    "            imgWhite[:, wGap:wCal + wGap] = imgResize\n",
    "        else:\n",
    "            # If width >= height:\n",
    "            k = imgSize / imgCrop.shape[1]\n",
    "            hCal = math.ceil(k * imgCrop.shape[0])\n",
    "            imgResize = cv2.resize(imgCrop, (imgSize, hCal))\n",
    "            hGap = math.ceil((imgSize - hCal) / 2)\n",
    "            imgWhite[hGap:hCal + hGap, :] = imgResize\n",
    "    except Exception as e:\n",
    "        print(f\"Error during image processing: {e}\")\n",
    "        return None\n",
    "    return imgWhite\n",
    "\n",
    "# Function to detect skin using YCrCb thresholds (returns a binary mask)\n",
    "def detect_skin(frame):\n",
    "    ycrcb = cv2.cvtColor(frame, cv2.COLOR_BGR2YCrCb)\n",
    "    lower_skin = np.array([0, 133, 77], dtype=np.uint8)\n",
    "    upper_skin = np.array([255, 173, 127], dtype=np.uint8)\n",
    "    mask = cv2.inRange(ycrcb, lower_skin, upper_skin)\n",
    "    \n",
    "    # Clean up noise\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    mask = cv2.GaussianBlur(mask, (5, 5), 0)\n",
    "    \n",
    "    # (Optional) clean-up by drawing filled contours\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contour_mask = np.zeros_like(mask)\n",
    "    cv2.drawContours(contour_mask, contours, -1, 255, thickness=cv2.FILLED)\n",
    "    mask = cv2.bitwise_and(mask, contour_mask)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# Main loop for each letter\n",
    "for className in letters:\n",
    "    print(f\"Starting collection for: {className}\")\n",
    "    folder = os.path.join(baseFolder, className)\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    counter, collecting = 0, False\n",
    "\n",
    "    while counter < maxImages:\n",
    "        success, img = cap.read()\n",
    "        if not success:\n",
    "            print(\"Camera access failed.\")\n",
    "            break\n",
    "\n",
    "        # Detect hand on the full image (only once)\n",
    "        hands, _ = detector.findHands(img, draw=False)\n",
    "        if hands:\n",
    "            # Use the first detected hand\n",
    "            hand = hands[0]\n",
    "            bbox = hand['bbox']       # [x, y, w, h]\n",
    "            lm_list = hand['lmList']    # list of landmarks in full-image coordinates\n",
    "            x, y, w, h = bbox\n",
    "\n",
    "            # Calculate padding (based on hand size)\n",
    "            xPad = int(w * paddingFactor)\n",
    "            yPad = int(h * paddingFactor)\n",
    "\n",
    "            # Compute crop boundaries (make sure theyâ€™re within image bounds)\n",
    "            crop_x1 = max(0, x - xPad)\n",
    "            crop_y1 = max(0, y - yPad)\n",
    "            crop_x2 = min(x + w + xPad, img.shape[1])\n",
    "            crop_y2 = min(y + h + yPad, img.shape[0])\n",
    "            imgCrop = img[crop_y1:crop_y2, crop_x1:crop_x2]\n",
    "\n",
    "            if imgCrop.size > 0:\n",
    "                # ----- STEP 1: Draw landmarks on the cropped image using adjusted coordinates -----\n",
    "                # Create a copy of the crop on which we will draw the landmarks\n",
    "                imgCrop_landmarked = imgCrop.copy()\n",
    "                # Adjust each landmark from full-image coordinates to crop coordinates\n",
    "                for lm in lm_list:\n",
    "                    adj_x = lm[0] - crop_x1\n",
    "                    adj_y = lm[1] - crop_y1\n",
    "                    cv2.circle(imgCrop_landmarked, (adj_x, adj_y), 4, (0, 0, 255), -1)\n",
    "                # Optionally, also draw the connections between landmarks:\n",
    "                for connection in mp.solutions.hands.HAND_CONNECTIONS:\n",
    "                    pt1 = lm_list[connection[0]]\n",
    "                    pt2 = lm_list[connection[1]]\n",
    "                    pt1_adjusted = (pt1[0] - crop_x1, pt1[1] - crop_y1)\n",
    "                    pt2_adjusted = (pt2[0] - crop_x1, pt2[1] - crop_y1)\n",
    "                    cv2.line(imgCrop_landmarked, pt1_adjusted, pt2_adjusted, (0, 0, 255), 2)\n",
    "\n",
    "                # ----- STEP 2: Convert the landmarked crop to a binary image -----\n",
    "                # First, get a binary skin mask from the original crop (without landmarks)\n",
    "                binaryMask = detect_skin(imgCrop)\n",
    "                # Create a blank (black) image and fill white where skin is detected\n",
    "                binary_result = np.zeros_like(imgCrop)\n",
    "                binary_result[binaryMask > 0] = [255, 255, 255]\n",
    "                # Now overlay the landmarks (drawn in black) onto the binary image.\n",
    "                # (Using the same adjusted coordinates from above)\n",
    "                for lm in lm_list:\n",
    "                    adj_x = lm[0] - crop_x1\n",
    "                    adj_y = lm[1] - crop_y1\n",
    "                    cv2.circle(binary_result, (adj_x, adj_y), 4, (0, 0, 0), -1)\n",
    "                for connection in mp.solutions.hands.HAND_CONNECTIONS:\n",
    "                    pt1 = lm_list[connection[0]]\n",
    "                    pt2 = lm_list[connection[1]]\n",
    "                    pt1_adjusted = (pt1[0] - crop_x1, pt1[1] - crop_y1)\n",
    "                    pt2_adjusted = (pt2[0] - crop_x1, pt2[1] - crop_y1)\n",
    "                    cv2.line(binary_result, pt1_adjusted, pt2_adjusted, (0, 0, 0), 2)\n",
    "\n",
    "                # ----- STEP 3: Resize for saving/visualization -----\n",
    "                # Use the cropâ€™s aspect ratio for correct resizing. (You can also use h/w from bbox.)\n",
    "                aspectRatio = (crop_y2 - crop_y1) / (crop_x2 - crop_x1)\n",
    "                imgWhite = process_and_resize(binary_result, aspectRatio, imgSize)\n",
    "                if imgWhite is not None:\n",
    "                    cv2.imshow(\"Processed Binary Image\", imgWhite)\n",
    "                    if collecting:\n",
    "                        counter += 1\n",
    "                        savePath = os.path.join(folder, f\"{className.lower()}_{counter}.jpg\")\n",
    "                        cv2.imwrite(savePath, imgWhite)\n",
    "                        print(f\"Saved {counter}/{maxImages} images for {className}\")\n",
    "\n",
    "        # Show the original live feed\n",
    "        cv2.imshow(\"Live Feed with Landmarks\", img)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('s'):\n",
    "            collecting = True\n",
    "        if key == ord('p'):\n",
    "            collecting = False\n",
    "\n",
    "    print(f\"Completed collection for {className}\")\n",
    "    input(\"Press Enter for next class.\")\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Dataset Basic Preprocessing Pipeline\n",
    "\n",
    "For the basic preprocessing of hand images.\n",
    "\n",
    "Run the below cell after installing relevant dependancies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "1. Enter input folder \n",
    "2. Enter outPut folder for saving in the repository\n",
    "\n",
    "### Preprocessings:\n",
    "\n",
    "1. Adding hand landmarks to cropped frame\n",
    "2. Converting cropped frame to binary image (black and white pixels only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import mediapipe as mp\n",
    "\n",
    "# ----------------------------\n",
    "# Folder containing subfolders for each class (e.g., A, B, C, â€¦)\n",
    "inputFolder = \"C:/path/to/your/input_folder\"    # <-- change this\n",
    "# Folder where the processed images will be saved\n",
    "outputFolder = \"C:/path/to/your/output_folder\"  # <-- change this\n",
    "# ----------------------------\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Configuration and Constants\n",
    "# ----------------------------\n",
    "imgSize = 500  # final output image will be imgSize x imgSize\n",
    "os.makedirs(outputFolder, exist_ok=True)\n",
    "# Initialize the hand detector (using CVZone)\n",
    "detector = HandDetector(maxHands=1)\n",
    "# For drawing hand connections\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# ----------------------------\n",
    "# Utility Functions\n",
    "# ----------------------------\n",
    "def process_and_resize(imgInput, aspectRatio, imgSize):\n",
    "    \"\"\"\n",
    "    Resize an image to fit inside a square canvas while preserving aspect ratio.\n",
    "    \"\"\"\n",
    "    channels = 1 if len(imgInput.shape) == 2 else imgInput.shape[2]\n",
    "    # Create a blank (black) square image\n",
    "    imgWhite = np.ones((imgSize, imgSize, channels), np.uint8) * 0\n",
    "    try:\n",
    "        if aspectRatio > 1:\n",
    "            # If the image is taller than wide:\n",
    "            k = imgSize / imgInput.shape[0]\n",
    "            wCal = math.ceil(k * imgInput.shape[1])\n",
    "            imgResize = cv2.resize(imgInput, (wCal, imgSize))\n",
    "            wGap = math.ceil((imgSize - wCal) / 2)\n",
    "            imgWhite[:, wGap:wCal + wGap] = imgResize\n",
    "        else:\n",
    "            # If the image is wider than tall:\n",
    "            k = imgSize / imgInput.shape[1]\n",
    "            hCal = math.ceil(k * imgInput.shape[0])\n",
    "            imgResize = cv2.resize(imgInput, (imgSize, hCal))\n",
    "            hGap = math.ceil((imgSize - hCal) / 2)\n",
    "            imgWhite[hGap:hCal + hGap, :] = imgResize\n",
    "    except Exception as e:\n",
    "        print(f\"Error during image processing: {e}\")\n",
    "        return None\n",
    "    return imgWhite\n",
    "\n",
    "def detect_skin(frame):\n",
    "    \"\"\"\n",
    "    Detect skin regions using YCrCb color space thresholds and return a binary mask.\n",
    "    \"\"\"\n",
    "    ycrcb = cv2.cvtColor(frame, cv2.COLOR_BGR2YCrCb)\n",
    "    lower_skin = np.array([0, 133, 77], dtype=np.uint8)\n",
    "    upper_skin = np.array([255, 173, 127], dtype=np.uint8)\n",
    "    mask = cv2.inRange(ycrcb, lower_skin, upper_skin)\n",
    "    \n",
    "    # Clean up noise using morphological operations\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    mask = cv2.GaussianBlur(mask, (5, 5), 0)\n",
    "    \n",
    "    # (Optional) Draw filled contours to further clean up the mask\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contour_mask = np.zeros_like(mask)\n",
    "    cv2.drawContours(contour_mask, contours, -1, 255, thickness=cv2.FILLED)\n",
    "    mask = cv2.bitwise_and(mask, contour_mask)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# ----------------------------\n",
    "# Main Processing Loop\n",
    "# ----------------------------\n",
    "# Iterate over each class folder in the input folder.\n",
    "for className in os.listdir(inputFolder):\n",
    "    classPath = os.path.join(inputFolder, className)\n",
    "    if not os.path.isdir(classPath):\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing class: {className}\")\n",
    "    # Create corresponding output folder for this class\n",
    "    outClassFolder = os.path.join(outputFolder, className)\n",
    "    os.makedirs(outClassFolder, exist_ok=True)\n",
    "\n",
    "    # Process each image file in the class folder\n",
    "    for imgFile in os.listdir(classPath):\n",
    "        imgPath = os.path.join(classPath, imgFile)\n",
    "        img = cv2.imread(imgPath)\n",
    "        if img is None:\n",
    "            print(f\"Failed to read image: {imgPath}\")\n",
    "            continue\n",
    "\n",
    "        # --- Hand Landmark Detection ---\n",
    "        hands, _ = detector.findHands(img, draw=False)\n",
    "        if hands:\n",
    "            # Use the first detected hand\n",
    "            hand = hands[0]\n",
    "            lm_list = hand['lmList']\n",
    "\n",
    "            # --- Step 1: Draw Landmarks on a Copy of the Full Image ---\n",
    "            # (Since the images are provided already, we work on the full image.)\n",
    "            img_landmarked = img.copy()\n",
    "            for lm in lm_list:\n",
    "                cv2.circle(img_landmarked, (lm[0], lm[1]), 4, (0, 0, 255), -1)\n",
    "            for connection in mp.solutions.hands.HAND_CONNECTIONS:\n",
    "                pt1 = lm_list[connection[0]]\n",
    "                pt2 = lm_list[connection[1]]\n",
    "                cv2.line(img_landmarked, (pt1[0], pt1[1]), (pt2[0], pt2[1]), (0, 0, 255), 2)\n",
    "\n",
    "            # --- Step 2: Create a Binary Image Using Skin Detection ---\n",
    "            binaryMask = detect_skin(img)\n",
    "            # Create a blank image and fill white where skin is detected\n",
    "            binary_result = np.zeros_like(img)\n",
    "            binary_result[binaryMask > 0] = [255, 255, 255]\n",
    "            # Overlay the landmarks on the binary image (drawn in black)\n",
    "            for lm in lm_list:\n",
    "                cv2.circle(binary_result, (lm[0], lm[1]), 4, (0, 0, 0), -1)\n",
    "            for connection in mp.solutions.hands.HAND_CONNECTIONS:\n",
    "                pt1 = lm_list[connection[0]]\n",
    "                pt2 = lm_list[connection[1]]\n",
    "                cv2.line(binary_result, (pt1[0], pt1[1]), (pt2[0], pt2[1]), (0, 0, 0), 2)\n",
    "\n",
    "            # --- Step 3: Resize for Consistent Output ---\n",
    "            aspectRatio = img.shape[0] / img.shape[1]\n",
    "            imgWhite = process_and_resize(binary_result, aspectRatio, imgSize)\n",
    "            if imgWhite is not None:\n",
    "                # Save the processed image to the output folder.\n",
    "                outPath = os.path.join(outClassFolder, imgFile)\n",
    "                cv2.imwrite(outPath, imgWhite)\n",
    "                print(f\"Processed and saved: {outPath}\")\n",
    "                # (Optional) Display the processed image.\n",
    "                cv2.imshow(\"Processed Image\", imgWhite)\n",
    "                cv2.waitKey(1)\n",
    "        else:\n",
    "            print(f\"No hand detected in image: {imgPath}\")\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentaition Pipeline\n",
    "\n",
    "We further preprocess the images by augmenting them based on rotations and horizontal flips\n",
    "\n",
    "Run the below cell after installing relevant dependancies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "1. Enter the folder location for the preprocessed dataset\n",
    "2. Enter the output location to save the augmented dataset\n",
    "3. There are more options for augmentation, comment or add more as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"C:/path/to/your/input_folder\"  \n",
    "output_folder = \"C:/path/to/your/output_folder\"  \n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmentation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image, ImageEnhance, ImageOps\n",
    "import numpy as np\n",
    "\n",
    "# Define augmentation functions\n",
    "\n",
    "# Rotate image by a random angle between -30 to 30 degrees\n",
    "def random_rotation(image):\n",
    "    angle = random.uniform(-30, 30)  # Rotate between -30 to 30 degrees\n",
    "    return image.rotate(angle)\n",
    "\n",
    "# Flip image horizontally with a 50% chance\n",
    "def random_flip(image):\n",
    "    if random.choice([True, False]):\n",
    "        return ImageOps.mirror(image)\n",
    "    return image\n",
    "\n",
    "# Adjust brightness by a random factor between 0.7 and 1.3\n",
    "def random_brightness(image):\n",
    "    enhancer = ImageEnhance.Brightness(image)\n",
    "    factor = random.uniform(0.7, 1.3)  # Brightness factor\n",
    "    return enhancer.enhance(factor)\n",
    "\n",
    "# Adjust contrast by a random factor between 0.7 and 1.3\n",
    "def random_contrast(image):\n",
    "    enhancer = ImageEnhance.Contrast(image)\n",
    "    factor = random.uniform(0.7, 1.3)  # Contrast factor\n",
    "    return enhancer.enhance(factor)\n",
    "\n",
    "# Add random noise to the image\n",
    "def add_random_noise(image):\n",
    "    np_image = np.array(image)\n",
    "    noise = np.random.normal(0, 25, np_image.shape).astype(np.int16)\n",
    "    noisy_image = np.clip(np_image + noise, 0, 255).astype(np.uint8)\n",
    "    return Image.fromarray(noisy_image)\n",
    "\n",
    "# Augment an image using a combination of random transformations\n",
    "# Comment out or add more functions as needed\n",
    "def augment_image(image):\n",
    "    image = random_rotation(image)\n",
    "    image = random_flip(image)\n",
    "    # image = random_brightness(image)\n",
    "    # image = random_contrast(image)\n",
    "    # image = add_random_noise(image)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To apply the augmentation to all images in the input folder and save them to the output folder\n",
    "# Iterate over all subfolders and images\n",
    "for subdir, _, files in os.walk(input_folder):\n",
    "    relative_path = os.path.relpath(subdir, input_folder)\n",
    "    output_subdir = os.path.join(output_folder, relative_path)\n",
    "    os.makedirs(output_subdir, exist_ok=True)\n",
    "\n",
    "    for file in files:\n",
    "        if file.lower().endswith(('png', 'jpg', 'jpeg', 'bmp', 'tiff')):\n",
    "            input_path = os.path.join(subdir, file)\n",
    "            output_path = os.path.join(output_subdir, file)\n",
    "\n",
    "            try:\n",
    "                with Image.open(input_path) as img:\n",
    "                    img = img.convert(\"L\")  # Ensure greyscale (black and white)\n",
    "                    augmented_img = augment_image(img)\n",
    "                    augmented_img.save(output_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {input_path}: {e}\")\n",
    "\n",
    "print(\"Data augmentation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Augmented Data With Preprocessed Dataset\n",
    "\n",
    "Creating a new folder for combined data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "1. Enter the preprocessed dataset location\n",
    "2. Enter the augmented dataset location\n",
    "3. Enter the new combined datasets save location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = \"C:/path/to/your/input_folder\"  \n",
    "dataset2 = \"C:/path/to/your/input_folder\"  \n",
    "output_dataset = \"C:/path/to/your/output_folder\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dataset, exist_ok=True)\n",
    "\n",
    "# Function to merge datasets with renaming\n",
    "def merge_datasets(source_dir, target_dir, suffix=\"\"):\n",
    "    for class_name in os.listdir(source_dir):\n",
    "        source_class_path = os.path.join(source_dir, class_name)\n",
    "        target_class_path = os.path.join(target_dir, class_name)\n",
    "        \n",
    "        if os.path.isdir(source_class_path):\n",
    "            # Create the class folder in the target if it doesn't exist\n",
    "            if not os.path.exists(target_class_path):\n",
    "                os.makedirs(target_class_path)\n",
    "            \n",
    "            for file_name in os.listdir(source_class_path):\n",
    "                source_file_path = os.path.join(source_class_path, file_name)\n",
    "                # Add the specified suffix to the file name\n",
    "                base_name, ext = os.path.splitext(file_name)\n",
    "                file_name = f\"{base_name}{suffix}{ext}\"\n",
    "                target_file_path = os.path.join(target_class_path, file_name)\n",
    "                \n",
    "                # Copy the file to the target directory\n",
    "                shutil.copy2(source_file_path, target_file_path)\n",
    "\n",
    "# Merge the main dataset\n",
    "merge_datasets(dataset1, output_dataset, suffix=\"_black\")\n",
    "\n",
    "# Merge the augmented dataset with \"_AUG\" renaming\n",
    "merge_datasets(dataset2, output_dataset, suffix=\"_white\")\n",
    "\n",
    "\n",
    "print(f\"Datasets merged into: {output_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training using ML Models\n",
    "\n",
    "In this pipeline the combined dataset will be used to train the ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "Run the below cell which will lead to a UI for training models. \n",
    "1. Enter your dataset location\n",
    "2. Selct the model to train\n",
    "3. Fill in or keep the default trining parameters\n",
    "4. Check or uncheck cross validation and write number of folds based on requirements\n",
    "5. Each epoch will show the training status in UI\n",
    "6. At end the results will be saved in modelName_RESULTS FOLDER and Models in TrainedBinary2Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warning: \n",
    "\n",
    "- Training is resource costly! \n",
    "- Ensure that you have right setup before training. \n",
    "- Process can take more than a day if trained on CPU (varies by model). \n",
    "- Highly recommended to train using GPU if Available\n",
    "- Recommend to use the already trained models in the repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Below Code to Open Training Panel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run TrainerV3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-Time Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section real-time recognition will be tested out based on the predictions from trained model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "IMPORTANT: WebCam required\n",
    "\n",
    "1. Enter the models location before running\n",
    "2. The Ensure that the lighting conditions are ideal. \n",
    "3. Try to perform in front of dark background if possible, or near non reflective walls\n",
    "4. Be thourough with ASL fingerspelling signs in order for proper classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvzone.ClassificationModule import Classifier\n",
    "\n",
    "classifier = Classifier(\"C:/Users/User/OneDrive/Documents/SignLanguageApp/TrainedBinary2Model/MobileNetV2_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WebCam access and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "def detect_skin(frame):\n",
    "    # Convert to YCrCb and equalize the luminance channel\n",
    "    ycrcb = cv2.cvtColor(frame, cv2.COLOR_BGR2YCrCb)\n",
    "    y_channel = ycrcb[:, :, 0]\n",
    "    y_eq = cv2.equalizeHist(y_channel)\n",
    "    ycrcb[:, :, 0] = y_eq\n",
    "\n",
    "    # Adjusted thresholds might be needed after equalization.\n",
    "    lower_skin = np.array([0, 133, 77], dtype=np.uint8)\n",
    "    upper_skin = np.array([255, 173, 127], dtype=np.uint8)\n",
    "    mask = cv2.inRange(ycrcb, lower_skin, upper_skin)\n",
    "    \n",
    "    # Noise reduction using morphological operations\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    mask = cv2.GaussianBlur(mask, (5, 5), 0)\n",
    "    \n",
    "    # Optionally, keep only the largest contour (if needed)\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contour_mask = np.zeros_like(mask)\n",
    "    if contours:\n",
    "        cv2.drawContours(contour_mask, contours, -1, 255, thickness=cv2.FILLED)\n",
    "    mask = cv2.bitwise_and(mask, contour_mask)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\n",
    "# Initialize camera, detector\n",
    "cap = cv2.VideoCapture(0)\n",
    "detector = HandDetector(maxHands=1)\n",
    "\n",
    "\n",
    "offset = 45\n",
    "imgSize = 250\n",
    "labels = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\"]\n",
    "\n",
    "# Use Mediapipeâ€™s hand connections to draw lines between landmarks.\n",
    "mp_hands = mp.solutions.hands\n",
    "hand_connections = mp_hands.HAND_CONNECTIONS\n",
    "\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "    imgOutput = img.copy()\n",
    "    hands, img = detector.findHands(img, draw=False)\n",
    "    \n",
    "    if hands:\n",
    "        hand = hands[0]\n",
    "        x, y, w, h = hand['bbox']\n",
    "        y1, y2 = max(0, y - offset), min(img.shape[0], y + h + offset)\n",
    "        x1, x2 = max(0, x - offset), min(img.shape[1], x + w + offset)\n",
    "        imgCrop = img[y1:y2, x1:x2]\n",
    "        \n",
    "        if imgCrop.shape[0] > 0 and imgCrop.shape[1] > 0:\n",
    "            # Draw landmarks on a copy of the cropped image (for visualization)\n",
    "            imgCrop_landmarked = imgCrop.copy()\n",
    "            if 'lmList' in hand:\n",
    "                lm_list = hand['lmList']\n",
    "                for lm in lm_list:\n",
    "                    cv2.circle(imgCrop_landmarked, (lm[0] - x1, lm[1] - y1), 4, (0, 0, 255), -1)\n",
    "                for connection in mp_hands.HAND_CONNECTIONS:\n",
    "                    if connection[0] < len(lm_list) and connection[1] < len(lm_list):\n",
    "                        pt1 = (lm_list[connection[0]][0] - x1, lm_list[connection[0]][1] - y1)\n",
    "                        pt2 = (lm_list[connection[1]][0] - x1, lm_list[connection[1]][1] - y1)\n",
    "                        cv2.line(imgCrop_landmarked, pt1, pt2, (0, 0, 255), 2)\n",
    "            \n",
    "            # Process the cropped image to create a binary image\n",
    "            binaryMask = detect_skin(imgCrop)\n",
    "            # Create a black background and set the hand area to white:\n",
    "            binary_result = np.zeros_like(imgCrop)\n",
    "            binary_result[binaryMask > 0] = [255, 255, 255]\n",
    "            \n",
    "            # Overlay the landmarks (black) on the binary image:\n",
    "            if 'lmList' in hand:\n",
    "                for lm in lm_list:\n",
    "                    cv2.circle(binary_result, (lm[0] - x1, lm[1] - y1), 4, (0, 0, 0), -1)\n",
    "                for connection in mp_hands.HAND_CONNECTIONS:\n",
    "                    pt1 = (lm_list[connection[0]][0] - x1, lm_list[connection[0]][1] - y1)\n",
    "                    pt2 = (lm_list[connection[1]][0] - x1, lm_list[connection[1]][1] - y1)\n",
    "                    cv2.line(binary_result, pt1, pt2, (0, 0, 0), 2)\n",
    "            \n",
    "            # Resize the binary_result to a fixed size (e.g., 250x250) while preserving the aspect ratio\n",
    "            aspectRatio = h / w\n",
    "            imgWhite = np.ones((imgSize, imgSize), np.uint8) * 0\n",
    "            if aspectRatio > 1:\n",
    "                k = imgSize / h\n",
    "                wCal = math.ceil(k * w)\n",
    "                imgResize = cv2.resize(binary_result, (wCal, imgSize))\n",
    "                wGap = math.ceil((imgSize - wCal) / 2)\n",
    "                imgWhite[:, wGap:wCal + wGap] = cv2.cvtColor(imgResize, cv2.COLOR_BGR2GRAY)\n",
    "            else:\n",
    "                k = imgSize / w\n",
    "                hCal = math.ceil(k * h)\n",
    "                imgResize = cv2.resize(binary_result, (imgSize, hCal))\n",
    "                hGap = math.ceil((imgSize - hCal) / 2)\n",
    "                imgWhite[hGap:hCal + hGap, :] = cv2.cvtColor(imgResize, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # You can now pass imgWhite (or its RGB version) to your classifier.\n",
    "            imgWhiteRGB = cv2.cvtColor(imgWhite, cv2.COLOR_GRAY2BGR)\n",
    "            prediction, index = classifier.getPrediction(imgWhiteRGB, draw=False)\n",
    "            \n",
    "            # Display classification results if the prediction confidence is high enough.\n",
    "            if prediction[index] > 0.75 and 0 <= index < len(labels):\n",
    "                cv2.rectangle(imgOutput, (x - offset, y - offset - 50),\n",
    "                              (x - offset + 90, y - offset - 50 + 50), (255, 0, 255), cv2.FILLED)\n",
    "                cv2.putText(imgOutput, labels[index], (x, y - 26),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 1.7, (255, 255, 255), 2)\n",
    "                cv2.rectangle(imgOutput, (x - offset, y - offset),\n",
    "                              (x + w + offset, y + h + offset), (255, 0, 255), 4)\n",
    "            \n",
    "            cv2.imshow(\"Processed Binary Image\", imgWhite)\n",
    "            cv2.imshow(\"Hand Landmarks\", imgCrop_landmarked)\n",
    "    \n",
    "    cv2.imshow(\"Image\", imgOutput)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-Time recognition UI/Tkinter App\n",
    "\n",
    "#### Features:\n",
    "\n",
    "1. Panel displaying main frame with prediction label, binary frame, landmark frame to understand workings of the process.\n",
    "2. Word typing. Use space bar to save the predicted letter. For white_space remove hand from frame and press space bar.\n",
    "3. --More to be added--\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    " \n",
    "IMPORTANT: WebCam required\n",
    "\n",
    "1. Change the model name in \"ASLRecogAPP.py\" before running the app.\n",
    "2. The Ensure that the lighting conditions are ideal. \n",
    "3. Try to perform in front of dark background if possible, or near non reflective walls\n",
    "4. Be thourough with ASL fingerspelling signs in order for proper classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Below Cell to open the app:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ASLRecogAPP.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Labels Found\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "No Labels Found\n",
      "1/1 [==============================] - 0s 333ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\tensorflow_env\\lib\\tkinter\\__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\tensorflow_env\\lib\\tkinter\\__init__.py\", line 814, in callit\n",
      "    func(*args)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_29688\\3321274865.py\", line 473, in update_frame\n",
      "    imgWhite[hGap:hCal + hGap, :] = cv2.cvtColor(imgResize, cv2.COLOR_BGR2GRAY)\n",
      "ValueError: could not broadcast input array from shape (251,250) into shape (250,250)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "from cvzone.ClassificationModule import Classifier\n",
    "import numpy as np\n",
    "import math\n",
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "from PIL import Image, ImageTk\n",
    "import time  # for timing delays\n",
    "import pyttsx3  # for text-to-speech\n",
    "\n",
    "# Determine the resampling method for resizing images\n",
    "try:\n",
    "    resample_method = Image.Resampling.LANCZOS\n",
    "except AttributeError:\n",
    "    resample_method = Image.ANTIALIAS\n",
    "\n",
    "# ---------------------------\n",
    "# Processing Code\n",
    "# ---------------------------\n",
    "def detect_skin(frame):\n",
    "    # Convert to YCrCb and equalize the luminance channel\n",
    "    ycrcb = cv2.cvtColor(frame, cv2.COLOR_BGR2YCrCb)\n",
    "    y_channel = ycrcb[:, :, 0]\n",
    "    y_eq = cv2.equalizeHist(y_channel)\n",
    "    ycrcb[:, :, 0] = y_eq\n",
    "\n",
    "    lower_skin = np.array([0, 133, 77], dtype=np.uint8)\n",
    "    upper_skin = np.array([255, 173, 127], dtype=np.uint8)\n",
    "    mask = cv2.inRange(ycrcb, lower_skin, upper_skin)\n",
    "    \n",
    "    # Noise reduction using morphological operations\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    mask = cv2.GaussianBlur(mask, (5, 5), 0)\n",
    "    \n",
    "    # Optionally, keep only the largest contour (if needed)\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contour_mask = np.zeros_like(mask)\n",
    "    if contours:\n",
    "        cv2.drawContours(contour_mask, contours, -1, 255, thickness=cv2.FILLED)\n",
    "    mask = cv2.bitwise_and(mask, contour_mask)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# ---------------------------\n",
    "# Initialization\n",
    "# ---------------------------\n",
    "cap = cv2.VideoCapture(0)\n",
    "detector = HandDetector(maxHands=1)\n",
    "\n",
    "# Define model options (adjust file paths as needed)\n",
    "model_options = {\n",
    "    \"MobileNetV2 (T2)\": \"C:/Users/User/OneDrive/Documents/SignLanguageApp/TrainedBinary2Model/MobileNetV2_model.h5\",\n",
    "    \"VGG16 (T2)\": \"C:/Users/User/OneDrive/Documents/SignLanguageApp/TrainedBinary2Model/VGG16_model.h5\",\n",
    "    \"DenseNet121 (T2)\": \"C:/Users/User/OneDrive/Documents/SignLanguageApp/TrainedBinary2Model/DenseNet121_model.h5\",\n",
    "    \"V3_VGG16 (T3)\": \"C:/Users/User/OneDrive/Documents/SignLanguageApp/TrainedBinary3Model/VGG16_model.h5\",\n",
    "    \"V4_VGG16 (T4)\": \"C:/Users/User/OneDrive/Documents/SignLanguageApp/TrainedBinary4Model/VGG16_model.h5\",\n",
    "    \"v4_MobileNetV2 (T4)\": \"C:/Users/User/OneDrive/Documents/SignLanguageApp/TrainedBinary4Model/MobileNetV2_model.h5\",\n",
    "    \"V5_MobileNetV2 (T5)\": \"C:/Users/User/OneDrive/Documents/SignLanguageApp/TrainedBinary5Model/MobileNetV2_model.h5\"\n",
    "}\n",
    "\n",
    "# Initially load the default model\n",
    "selected_model = \"VGG16 (T2)\"\n",
    "classifier = Classifier(model_options[selected_model])\n",
    "\n",
    "offset = 45\n",
    "imgSize = 250\n",
    "labels = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \n",
    "          \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\"]\n",
    "\n",
    "mp_hands = mp.solutions.hands  # for landmark connections\n",
    "hand_connections = mp_hands.HAND_CONNECTIONS\n",
    "\n",
    "# ---------------------------\n",
    "# Global variables for word typing\n",
    "# ---------------------------\n",
    "current_word = \"\"         # the final sentence built from letters and spaces\n",
    "current_prediction = \"\"   # the letter predicted by the classifier\n",
    "# Global variables for averaging predictions over 1 second\n",
    "prediction_sum = np.zeros(len(labels))\n",
    "prediction_count = 0\n",
    "last_avg_time = time.time()\n",
    "# Global variable for moving average buffer\n",
    "prediction_buffer = []  # Each element: (timestamp, prediction_array)\n",
    "\n",
    "# Variables for delaying predictions and key presses\n",
    "last_prediction_update_time = 0\n",
    "prediction_update_delay = 0.05  # seconds delay for updating prediction\n",
    "last_space_time = 0\n",
    "space_cooldown = 0.5  # seconds delay between spacebar inputs\n",
    "\n",
    "# ---------------------------\n",
    "# Tkinter UI Setup\n",
    "# ---------------------------\n",
    "root = tk.Tk()\n",
    "root.title(\"ASL - Fingerspelling Recognition\")\n",
    "root.geometry(\"1200x800\")\n",
    "root.configure(bg=\"#000000\")  # overall dark background\n",
    "\n",
    "# Left panel: Camera Feed (fixed)\n",
    "left_frame = tk.Frame(root, width=800, height=800, bg=\"#000000\")\n",
    "left_frame.grid(row=0, column=0, sticky=\"nsew\")\n",
    "left_frame.grid_propagate(False)\n",
    "\n",
    "# Right panel: Controls & Info (fixed)\n",
    "# (Increased width to 750 to allow more space for the right-panel contents.)\n",
    "right_frame = tk.Frame(root, width=715, height=800, bg=\"#1F1F1F\")\n",
    "right_frame.grid(row=0, column=1, sticky=\"nsew\")\n",
    "right_frame.grid_propagate(False)\n",
    "\n",
    "# Configure grid weights for root to stretch panels\n",
    "root.grid_columnconfigure(0, weight=2)\n",
    "root.grid_columnconfigure(1, weight=1)\n",
    "root.grid_rowconfigure(0, weight=1)\n",
    "\n",
    "# --- Right Panel Layout ---\n",
    "# We'll use a grid layout with rows for header, options, image displays, info, and buttons\n",
    "\n",
    "# Header\n",
    "header_label = tk.Label(right_frame, text=\"ASL Fingerspelling Recognition\", font=(\"Helvetica\", 20, \"bold\"),\n",
    "                        bg=\"#1F1F1F\", fg=\"#00FFFF\")\n",
    "header_label.grid(row=0, column=0, columnspan=2, pady=(10, 5), sticky=\"ew\")\n",
    "\n",
    "# 1. Model Selection\n",
    "model_label = tk.Label(right_frame, text=\"Select Model:\", font=(\"Helvetica\", 14),\n",
    "                       bg=\"#1F1F1F\", fg=\"#FFFFFF\")\n",
    "model_label.grid(row=1, column=0, padx=(10, 5), pady=5, sticky=\"w\")\n",
    "\n",
    "selected_model_var = tk.StringVar()\n",
    "selected_model_var.set(selected_model)  # default value\n",
    "\n",
    "def change_model(*args):\n",
    "    global classifier\n",
    "    sel = selected_model_var.get()\n",
    "    new_path = model_options[sel]\n",
    "    classifier = Classifier(new_path)\n",
    "    history_text.configure(state='normal')\n",
    "    history_text.insert(tk.END, f\"Model changed to: {sel}\\n\")\n",
    "    history_text.see(tk.END)\n",
    "    history_text.configure(state='disabled')\n",
    "\n",
    "model_option_menu = tk.OptionMenu(right_frame, selected_model_var, *model_options.keys(), command=change_model)\n",
    "model_option_menu.config(font=(\"Helvetica\", 14), bg=\"#333333\", fg=\"#FFFFFF\", highlightthickness=0)\n",
    "model_option_menu[\"menu\"].config(bg=\"#333333\", fg=\"#FFFFFF\")\n",
    "model_option_menu.grid(row=1, column=1, padx=(5,10), pady=5, sticky=\"e\")\n",
    "\n",
    "# 2. Fixed Image Display Frames\n",
    "# Set fixed dimensions (width x height)\n",
    "FIXED_WIDTH = 280\n",
    "FIXED_HEIGHT = 280\n",
    "\n",
    "# Binary Image Display\n",
    "binary_frame = tk.Frame(right_frame, width=FIXED_WIDTH, height=FIXED_HEIGHT, bg=\"#333333\")\n",
    "binary_frame.grid(row=2, column=0, padx=10, pady=10, sticky=\"nsew\")\n",
    "binary_frame.grid_propagate(False)\n",
    "binary_title = tk.Label(binary_frame, text=\"Binary Image\", font=(\"Helvetica\", 12),\n",
    "                          bg=\"#333333\", fg=\"#FFFFFF\")\n",
    "binary_title.pack(side=\"top\", fill=\"x\")\n",
    "binary_label = tk.Label(binary_frame, bg=\"#333333\")\n",
    "binary_label.pack(expand=True, fill=\"both\")\n",
    "\n",
    "# Hand Landmarks Display\n",
    "landmarks_frame = tk.Frame(right_frame, width=FIXED_WIDTH, height=FIXED_HEIGHT, bg=\"#333333\")\n",
    "landmarks_frame.grid(row=2, column=1, padx=10, pady=10, sticky=\"nsew\")\n",
    "landmarks_frame.grid_propagate(False)\n",
    "landmarks_title = tk.Label(landmarks_frame, text=\"Landmarks\", font=(\"Helvetica\", 12),\n",
    "                            bg=\"#333333\", fg=\"#FFFFFF\")\n",
    "landmarks_title.pack(side=\"top\", fill=\"x\")\n",
    "landmarks_label = tk.Label(landmarks_frame, bg=\"#333333\")\n",
    "landmarks_label.pack(expand=True, fill=\"both\")\n",
    "\n",
    "# --- Set default black images in the binary and landmark panels ---\n",
    "default_black = np.zeros((FIXED_HEIGHT, FIXED_WIDTH, 3), dtype=np.uint8)\n",
    "default_black_pil = Image.fromarray(default_black)\n",
    "default_black_tk = ImageTk.PhotoImage(image=default_black_pil)\n",
    "binary_label.imgtk = default_black_tk\n",
    "binary_label.configure(image=default_black_tk)\n",
    "landmarks_label.imgtk = default_black_tk\n",
    "landmarks_label.configure(image=default_black_tk)\n",
    "\n",
    "# 3. Prediction Info\n",
    "prediction_info_frame = tk.Frame(right_frame, bg=\"#1F1F1F\")\n",
    "prediction_info_frame.grid(row=3, column=0, columnspan=2, padx=10, pady=5, sticky=\"ew\")\n",
    "prediction_info_frame.columnconfigure(0, weight=1)\n",
    "prediction_info_frame.columnconfigure(1, weight=1)\n",
    "\n",
    "word_label = tk.Label(prediction_info_frame, textvariable=tk.StringVar(), font=(\"Helvetica\", 16),\n",
    "                      bg=\"#1F1F1F\", fg=\"#00FF00\")\n",
    "word_label.grid(row=0, column=0, padx=5, pady=5, sticky=\"w\")\n",
    "word_var = tk.StringVar()\n",
    "word_var.set(\"\")\n",
    "word_label.config(textvariable=word_var)\n",
    "\n",
    "prediction_label = tk.Label(prediction_info_frame, textvariable=tk.StringVar(), font=(\"Helvetica\", 16),\n",
    "                            bg=\"#1F1F1F\", fg=\"#FF00FF\")\n",
    "prediction_label.grid(row=0, column=1, padx=5, pady=5, sticky=\"e\")\n",
    "prediction_var = tk.StringVar()\n",
    "prediction_var.set(\"\")\n",
    "prediction_label.config(textvariable=prediction_var)\n",
    "\n",
    "# 4. Buttons (Clear and Pronounce)\n",
    "buttons_frame = tk.Frame(right_frame, bg=\"#1F1F1F\")\n",
    "buttons_frame.grid(row=4, column=0, columnspan=2, padx=10, pady=10, sticky=\"ew\")\n",
    "buttons_frame.columnconfigure(0, weight=1)\n",
    "buttons_frame.columnconfigure(1, weight=1)\n",
    "\n",
    "def clear_word():\n",
    "    global current_word\n",
    "    current_word = \"\"\n",
    "    history_text.configure(state='normal')\n",
    "    history_text.insert(tk.END, \"Clear pressed: Word cleared\\n\")\n",
    "    history_text.see(tk.END)\n",
    "    history_text.configure(state='disabled')\n",
    "    word_var.set(current_word)\n",
    "\n",
    "clear_button = tk.Button(buttons_frame, text=\"Clear\", font=(\"Helvetica\", 14),\n",
    "                           bg=\"#333333\", fg=\"#00FFFF\", command=clear_word)\n",
    "clear_button.grid(row=0, column=0, padx=5, pady=5, sticky=\"ew\")\n",
    "\n",
    "def pronounce_sentence():\n",
    "    global current_word\n",
    "    engine = pyttsx3.init()\n",
    "    sentence = current_word.strip()\n",
    "    if sentence == \"\":\n",
    "        history_text.configure(state='normal')\n",
    "        history_text.insert(tk.END, \"No sentence to pronounce.\\n\")\n",
    "        history_text.see(tk.END)\n",
    "        history_text.configure(state='disabled')\n",
    "        return\n",
    "    history_text.configure(state='normal')\n",
    "    history_text.insert(tk.END, f\"Pronouncing: {sentence}\\n\")\n",
    "    history_text.see(tk.END)\n",
    "    history_text.configure(state='disabled')\n",
    "    engine.say(sentence)\n",
    "    engine.runAndWait()\n",
    "\n",
    "pronounce_button = tk.Button(buttons_frame, text=\"Pronounce\", font=(\"Helvetica\", 14),\n",
    "                              bg=\"#333333\", fg=\"#00FFFF\", command=pronounce_sentence)\n",
    "pronounce_button.grid(row=0, column=1, padx=5, pady=5, sticky=\"ew\")\n",
    "\n",
    "# 5. History Text (at the bottom)\n",
    "history_text = scrolledtext.ScrolledText(right_frame, width=40, height=8,\n",
    "                                           bg=\"#2E2E2E\", fg=\"#FFFFFF\", font=(\"Helvetica\", 12))\n",
    "history_text.grid(row=5, column=0, columnspan=2, padx=10, pady=10, sticky=\"nsew\")\n",
    "history_text.configure(state='disabled')\n",
    "\n",
    "instructions_label = tk.Label(\n",
    "    right_frame,\n",
    "    text=\"1. Enter word: Space Bar  |  2. Delete last character: Backspace\",\n",
    "    font=(\"Helvetica\", 10),\n",
    "    bg=\"#1F1F1F\",\n",
    "    fg=\"#AAAAAA\"\n",
    ")\n",
    "instructions_label.grid(row=6, column=0, columnspan=2, padx=10, pady=(0, 10), sticky=\"ew\")\n",
    "# ---------------------------\n",
    "# Main Camera Feed Display (Left Panel)\n",
    "# ---------------------------\n",
    "main_image_label = tk.Label(left_frame, bg=\"#000000\")\n",
    "main_image_label.place(relwidth=1, relheight=1)  # fill entire left frame\n",
    "\n",
    "# ---------------------------\n",
    "# Key Binding Functions\n",
    "# ---------------------------\n",
    "def on_space_press(event):\n",
    "    global current_word, current_prediction, last_space_time\n",
    "    current_time = time.time()\n",
    "    if current_time - last_space_time < space_cooldown:\n",
    "        return  # Ignore if within cooldown\n",
    "    last_space_time = current_time\n",
    "    if current_prediction != \"\":\n",
    "        current_word += current_prediction\n",
    "        history_text.configure(state='normal')\n",
    "        history_text.insert(tk.END, f\"Entered: {current_prediction}\\n\")\n",
    "        history_text.see(tk.END)\n",
    "        history_text.configure(state='disabled')\n",
    "    else:\n",
    "        current_word += \" \"\n",
    "        history_text.configure(state='normal')\n",
    "        history_text.insert(tk.END, \"Entered: [space]\\n\")\n",
    "        history_text.see(tk.END)\n",
    "        history_text.configure(state='disabled')\n",
    "    word_var.set(current_word)\n",
    "\n",
    "def on_backspace(event):\n",
    "    global current_word\n",
    "    if current_word:\n",
    "        current_word = current_word[:-1]\n",
    "        history_text.configure(state='normal')\n",
    "        history_text.insert(tk.END, \"Backspace pressed: Removed last character\\n\")\n",
    "        history_text.see(tk.END)\n",
    "        history_text.configure(state='disabled')\n",
    "        word_var.set(current_word)\n",
    "\n",
    "root.bind(\"<space>\", on_space_press)\n",
    "root.bind(\"<BackSpace>\", on_backspace)\n",
    "\n",
    "# def update_frame():\n",
    "#     global current_prediction, last_prediction_update_time\n",
    "#     success, img = cap.read()\n",
    "#     if not success:\n",
    "#         root.after(10, update_frame)\n",
    "#         return\n",
    "\n",
    "#     imgOutput = img.copy()\n",
    "#     hands, img = detector.findHands(img, draw=False)\n",
    "    \n",
    "#     imgWhite_for_display = None\n",
    "#     imgCrop_landmarked_for_display = None\n",
    "\n",
    "#     if hands:\n",
    "#         hand = hands[0]\n",
    "#         x, y, w, h = hand['bbox']\n",
    "#         y1, y2 = max(0, y - offset), min(img.shape[0], y + h + offset)\n",
    "#         x1, x2 = max(0, x - offset), min(img.shape[1], x + w + offset)\n",
    "#         imgCrop = img[y1:y2, x1:x2]\n",
    "        \n",
    "#         if imgCrop.shape[0] > 0 and imgCrop.shape[1] > 0:\n",
    "#             imgCrop_landmarked = imgCrop.copy()\n",
    "#             if 'lmList' in hand:\n",
    "#                 lm_list = hand['lmList']\n",
    "#                 for lm in lm_list:\n",
    "#                     cv2.circle(imgCrop_landmarked, (lm[0] - x1, lm[1] - y1), 4, (0, 0, 255), -1)\n",
    "#                 for connection in mp_hands.HAND_CONNECTIONS:\n",
    "#                     if connection[0] < len(lm_list) and connection[1] < len(lm_list):\n",
    "#                         pt1 = (lm_list[connection[0]][0] - x1, lm_list[connection[0]][1] - y1)\n",
    "#                         pt2 = (lm_list[connection[1]][0] - x1, lm_list[connection[1]][1] - y1)\n",
    "#                         cv2.line(imgCrop_landmarked, pt1, pt2, (0, 0, 255), 2)\n",
    "            \n",
    "#             binaryMask = detect_skin(imgCrop)\n",
    "#             binary_result = np.zeros_like(imgCrop)\n",
    "#             binary_result[binaryMask > 0] = [255, 255, 255]\n",
    "            \n",
    "#             if 'lmList' in hand:\n",
    "#                 for lm in lm_list:\n",
    "#                     cv2.circle(binary_result, (lm[0] - x1, lm[1] - y1), 4, (0, 0, 0), -1)\n",
    "#                 for connection in mp_hands.HAND_CONNECTIONS:\n",
    "#                     pt1 = (lm_list[connection[0]][0] - x1, lm_list[connection[0]][1] - y1)\n",
    "#                     pt2 = (lm_list[connection[1]][0] - x1, lm_list[connection[1]][1] - y1)\n",
    "#                     cv2.line(binary_result, pt1, pt2, (0, 0, 0), 2)\n",
    "            \n",
    "#             aspectRatio = h / w\n",
    "#             imgWhite = np.ones((imgSize, imgSize), np.uint8) * 0\n",
    "#             if aspectRatio > 1:\n",
    "#                 k = imgSize / h\n",
    "#                 wCal = math.ceil(k * w)\n",
    "#                 imgResize = cv2.resize(binary_result, (wCal, imgSize))\n",
    "#                 wGap = math.ceil((imgSize - wCal) / 2)\n",
    "#                 imgWhite[:, wGap:wCal + wGap] = cv2.cvtColor(imgResize, cv2.COLOR_BGR2GRAY)\n",
    "#             else:\n",
    "#                 k = imgSize / w\n",
    "#                 hCal = math.ceil(k * h)\n",
    "#                 imgResize = cv2.resize(binary_result, (imgSize, hCal))\n",
    "#                 hGap = math.ceil((imgSize - hCal) / 2)\n",
    "#                 imgWhite[hGap:hCal + hGap, :] = cv2.cvtColor(imgResize, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "#             imgWhiteRGB = cv2.cvtColor(imgWhite, cv2.COLOR_GRAY2BGR)\n",
    "#             prediction, index = classifier.getPrediction(imgWhiteRGB, draw=False)\n",
    "            \n",
    "#             current_time = time.time()\n",
    "#             if current_time - last_prediction_update_time >= prediction_update_delay:\n",
    "#                 if prediction[index] > 0.60 and 0 <= index < len(labels):\n",
    "#                     letter = labels[index]\n",
    "#                     prob = prediction[index]  # probability value\n",
    "#                     current_prediction = letter\n",
    "#                     last_prediction_update_time = current_time\n",
    "#                     # Draw a pink box in the top-left of the hand bounding box.\n",
    "#                     box_x = x - offset\n",
    "#                     box_y = y - offset - 50\n",
    "#                     box_width = 150  # Increase the width to fit probability text\n",
    "#                     box_height = 50\n",
    "#                     cv2.rectangle(imgOutput, (box_x, box_y), (box_x + box_width, box_y + box_height), (255, 0, 255), cv2.FILLED)\n",
    "#                     # Prepare the text with letter and probability (formatted to two decimals).\n",
    "#                     text = f\"{letter}: {prob:.2f}\"\n",
    "#                     # Adjust text position and font scale as needed.\n",
    "#                     cv2.putText(imgOutput, text, (box_x + 5, box_y + 35), cv2.FONT_HERSHEY_COMPLEX, 0.8, (255, 255, 255), 2)\n",
    "#                     cv2.rectangle(imgOutput, (x - offset, y - offset), (x + w + offset, y + h + offset), (255, 0, 255), 4)\n",
    "#                 else:\n",
    "#                     current_prediction = \"\"\n",
    "#                     last_prediction_update_time = current_time\n",
    "            \n",
    "#             prediction_var.set(\"Prediction: \" + (current_prediction if current_prediction != \"\" else \"[None]\"))\n",
    "            \n",
    "#             imgWhite_for_display = imgWhite.copy()\n",
    "#             imgCrop_landmarked_for_display = imgCrop_landmarked.copy()\n",
    "    \n",
    "#     else:\n",
    "#         current_prediction = \"\"\n",
    "#         prediction_var.set(\"Prediction: [None]\")\n",
    "    \n",
    "#     imgOutput_rgb = cv2.cvtColor(imgOutput, cv2.COLOR_BGR2RGB)\n",
    "#     img_pil = Image.fromarray(imgOutput_rgb)\n",
    "#     img_tk = ImageTk.PhotoImage(image=img_pil)\n",
    "#     main_image_label.imgtk = img_tk\n",
    "#     main_image_label.configure(image=img_tk)\n",
    "    \n",
    "#     # Force images to be resized to the fixed dimensions for right-panel displays\n",
    "#     if imgWhite_for_display is not None:\n",
    "#         imgWhite_rgb = cv2.cvtColor(imgWhite_for_display, cv2.COLOR_GRAY2RGB)\n",
    "#         imgWhite_pil = Image.fromarray(imgWhite_rgb).resize((FIXED_WIDTH, FIXED_HEIGHT), resample_method)\n",
    "#         imgWhite_tk = ImageTk.PhotoImage(image=imgWhite_pil)\n",
    "#         binary_label.imgtk = imgWhite_tk\n",
    "#         binary_label.configure(image=imgWhite_tk)\n",
    "    \n",
    "#     if imgCrop_landmarked_for_display is not None:\n",
    "#         imgCrop_rgb = cv2.cvtColor(imgCrop_landmarked_for_display, cv2.COLOR_BGR2RGB)\n",
    "#         imgCrop_pil = Image.fromarray(imgCrop_rgb).resize((FIXED_WIDTH, FIXED_HEIGHT), resample_method)\n",
    "#         imgCrop_tk = ImageTk.PhotoImage(image=imgCrop_pil)\n",
    "#         landmarks_label.imgtk = imgCrop_tk\n",
    "#         landmarks_label.configure(image=imgCrop_tk)\n",
    "    \n",
    "#     root.after(10, update_frame)\n",
    "\n",
    "def update_frame():\n",
    "    global current_prediction, last_prediction_update_time, prediction_buffer\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        root.after(10, update_frame)\n",
    "        return\n",
    "\n",
    "    imgOutput = img.copy()\n",
    "    hands, img = detector.findHands(img, draw=False)\n",
    "\n",
    "    imgWhite_for_display = None\n",
    "    imgCrop_landmarked_for_display = None\n",
    "\n",
    "    if hands:\n",
    "        hand = hands[0]\n",
    "        x, y, w, h = hand['bbox']\n",
    "        y1, y2 = max(0, y - offset), min(img.shape[0], y + h + offset)\n",
    "        x1, x2 = max(0, x - offset), min(img.shape[1], x + w + offset)\n",
    "        imgCrop = img[y1:y2, x1:x2]\n",
    "\n",
    "        if imgCrop.shape[0] > 0 and imgCrop.shape[1] > 0:\n",
    "            imgCrop_landmarked = imgCrop.copy()\n",
    "            if 'lmList' in hand:\n",
    "                lm_list = hand['lmList']\n",
    "                for lm in lm_list:\n",
    "                    cv2.circle(imgCrop_landmarked, (lm[0] - x1, lm[1] - y1), 4, (0, 0, 255), -1)\n",
    "                for connection in mp_hands.HAND_CONNECTIONS:\n",
    "                    if connection[0] < len(lm_list) and connection[1] < len(lm_list):\n",
    "                        pt1 = (lm_list[connection[0]][0] - x1, lm_list[connection[0]][1] - y1)\n",
    "                        pt2 = (lm_list[connection[1]][0] - x1, lm_list[connection[1]][1] - y1)\n",
    "                        cv2.line(imgCrop_landmarked, pt1, pt2, (0, 0, 255), 2)\n",
    "\n",
    "            binaryMask = detect_skin(imgCrop)\n",
    "            binary_result = np.zeros_like(imgCrop)\n",
    "            binary_result[binaryMask > 0] = [255, 255, 255]\n",
    "\n",
    "            if 'lmList' in hand:\n",
    "                for lm in lm_list:\n",
    "                    cv2.circle(binary_result, (lm[0] - x1, lm[1] - y1), 4, (0, 0, 0), -1)\n",
    "                for connection in mp_hands.HAND_CONNECTIONS:\n",
    "                    pt1 = (lm_list[connection[0]][0] - x1, lm_list[connection[0]][1] - y1)\n",
    "                    pt2 = (lm_list[connection[1]][0] - x1, lm_list[connection[1]][1] - y1)\n",
    "                    cv2.line(binary_result, pt1, pt2, (0, 0, 0), 2)\n",
    "\n",
    "            aspectRatio = h / w\n",
    "            imgWhite = np.ones((imgSize, imgSize), np.uint8) * 0\n",
    "            if aspectRatio > 1:\n",
    "                k = imgSize / h\n",
    "                wCal = math.ceil(k * w)\n",
    "                imgResize = cv2.resize(binary_result, (wCal, imgSize))\n",
    "                wGap = math.ceil((imgSize - wCal) / 2)\n",
    "                imgWhite[:, wGap:wCal + wGap] = cv2.cvtColor(imgResize, cv2.COLOR_BGR2GRAY)\n",
    "            else:\n",
    "                k = imgSize / w\n",
    "                hCal = math.ceil(k * h)\n",
    "                imgResize = cv2.resize(binary_result, (imgSize, hCal))\n",
    "                hGap = math.ceil((imgSize - hCal) / 2)\n",
    "                imgWhite[hGap:hCal + hGap, :] = cv2.cvtColor(imgResize, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            imgWhiteRGB = cv2.cvtColor(imgWhite, cv2.COLOR_GRAY2BGR)\n",
    "            prediction, index = classifier.getPrediction(imgWhiteRGB, draw=False)\n",
    "\n",
    "            # --- Moving Average over last 1 second ---\n",
    "            current_time = time.time()\n",
    "            # Append the current prediction distribution to the buffer.\n",
    "            prediction_buffer.append((current_time, prediction))\n",
    "            # Remove predictions older than 1 second.\n",
    "            prediction_buffer = [(t, p) for (t, p) in prediction_buffer if current_time - t <= 1.0]\n",
    "            if len(prediction_buffer) > 0:\n",
    "                # Compute the average prediction distribution.\n",
    "                avg_prediction = np.mean([p for (t, p) in prediction_buffer], axis=0)\n",
    "                best_index = np.argmax(avg_prediction)\n",
    "                best_letter = labels[best_index]\n",
    "                best_prob = avg_prediction[best_index]\n",
    "                current_prediction = best_letter\n",
    "\n",
    "                # Draw a continuously visible pink box around the hand.\n",
    "                box_x = x - offset\n",
    "                box_y = y - offset - 50\n",
    "                box_width = 150  # Adjust width as needed\n",
    "                box_height = 50\n",
    "                cv2.rectangle(imgOutput, (box_x, box_y), (box_x + box_width, box_y + box_height), (255, 0, 255), cv2.FILLED)\n",
    "                text = f\"{best_letter}: {best_prob:.2f}\"\n",
    "                cv2.putText(imgOutput, text, (box_x + 5, box_y + 35), cv2.FONT_HERSHEY_COMPLEX, 0.8, (255, 255, 255), 2)\n",
    "                cv2.rectangle(imgOutput, (x - offset, y - offset), (x + w + offset, y + h + offset), (255, 0, 255), 4)\n",
    "            else:\n",
    "                current_prediction = \"\"\n",
    "            \n",
    "            prediction_var.set(\"Prediction: \" + (current_prediction if current_prediction != \"\" else \"[None]\"))\n",
    "            \n",
    "            imgWhite_for_display = imgWhite.copy()\n",
    "            imgCrop_landmarked_for_display = imgCrop_landmarked.copy()\n",
    "    \n",
    "    else:\n",
    "        current_prediction = \"\"\n",
    "        prediction_var.set(\"Prediction: [None]\")\n",
    "\n",
    "    imgOutput_rgb = cv2.cvtColor(imgOutput, cv2.COLOR_BGR2RGB)\n",
    "    img_pil = Image.fromarray(imgOutput_rgb)\n",
    "    img_tk = ImageTk.PhotoImage(image=img_pil)\n",
    "    main_image_label.imgtk = img_tk\n",
    "    main_image_label.configure(image=img_tk)\n",
    "    \n",
    "    # Force images to be resized to the fixed dimensions for right-panel displays.\n",
    "    if imgWhite_for_display is not None:\n",
    "        imgWhite_rgb = cv2.cvtColor(imgWhite_for_display, cv2.COLOR_GRAY2RGB)\n",
    "        imgWhite_pil = Image.fromarray(imgWhite_rgb).resize((FIXED_WIDTH, FIXED_HEIGHT), resample_method)\n",
    "        imgWhite_tk = ImageTk.PhotoImage(image=imgWhite_pil)\n",
    "        binary_label.imgtk = imgWhite_tk\n",
    "        binary_label.configure(image=imgWhite_tk)\n",
    "    \n",
    "    if imgCrop_landmarked_for_display is not None:\n",
    "        imgCrop_rgb = cv2.cvtColor(imgCrop_landmarked_for_display, cv2.COLOR_BGR2RGB)\n",
    "        imgCrop_pil = Image.fromarray(imgCrop_rgb).resize((FIXED_WIDTH, FIXED_HEIGHT), resample_method)\n",
    "        imgCrop_tk = ImageTk.PhotoImage(image=imgCrop_pil)\n",
    "        landmarks_label.imgtk = imgCrop_tk\n",
    "        landmarks_label.configure(image=imgCrop_tk)\n",
    "    \n",
    "    root.after(10, update_frame)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Clean up on exit\n",
    "# ---------------------------\n",
    "def on_closing():\n",
    "    cap.release()\n",
    "    root.destroy()\n",
    "\n",
    "root.protocol(\"WM_DELETE_WINDOW\", on_closing)\n",
    "\n",
    "# Start the update loop.\n",
    "update_frame()\n",
    "root.mainloop()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
