{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image, ImageEnhance, ImageOps\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define augmentation functions\n",
    "def random_rotation(image):\n",
    "    angle = random.uniform(-30, 30)  # Rotate between -30 to 30 degrees\n",
    "    return image.rotate(angle)\n",
    "\n",
    "def random_flip(image):\n",
    "    if random.choice([True, False]):\n",
    "        return ImageOps.mirror(image)\n",
    "    return image\n",
    "\n",
    "def random_brightness(image):\n",
    "    enhancer = ImageEnhance.Brightness(image)\n",
    "    factor = random.uniform(0.7, 1.3)  # Brightness factor\n",
    "    return enhancer.enhance(factor)\n",
    "\n",
    "def random_contrast(image):\n",
    "    enhancer = ImageEnhance.Contrast(image)\n",
    "    factor = random.uniform(0.7, 1.3)  # Contrast factor\n",
    "    return enhancer.enhance(factor)\n",
    "\n",
    "def add_random_noise(image):\n",
    "    np_image = np.array(image)\n",
    "    noise = np.random.normal(0, 25, np_image.shape).astype(np.int16)\n",
    "    noisy_image = np.clip(np_image + noise, 0, 255).astype(np.uint8)\n",
    "    return Image.fromarray(noisy_image)\n",
    "\n",
    "def augment_image(image):\n",
    "    image = random_rotation(image)\n",
    "    image = random_flip(image)\n",
    "    image = random_brightness(image)\n",
    "    image = random_contrast(image)\n",
    "    image = add_random_noise(image)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"C:/Users/User/OneDrive/Documents/SignLanguageApp/SLangDataset/mergedData2\"\n",
    "output_folder = \"C:/Users/User/OneDrive/Documents/SignLanguageApp/SLangDataset/augmentedData2\"\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data augmentation completed!\n"
     ]
    }
   ],
   "source": [
    "# Iterate over all subfolders and images\n",
    "for subdir, _, files in os.walk(input_folder):\n",
    "    relative_path = os.path.relpath(subdir, input_folder)\n",
    "    output_subdir = os.path.join(output_folder, relative_path)\n",
    "    os.makedirs(output_subdir, exist_ok=True)\n",
    "\n",
    "    for file in files:\n",
    "        if file.lower().endswith(('png', 'jpg', 'jpeg', 'bmp', 'tiff')):\n",
    "            input_path = os.path.join(subdir, file)\n",
    "            output_path = os.path.join(output_subdir, file)\n",
    "\n",
    "            try:\n",
    "                with Image.open(input_path) as img:\n",
    "                    img = img.convert(\"L\")  # Ensure greyscale (black and white)\n",
    "                    augmented_img = augment_image(img)\n",
    "                    augmented_img.save(output_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {input_path}: {e}\")\n",
    "\n",
    "print(\"Data augmentation completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 43120 images belonging to 27 classes.\n",
      "Found 10780 images belonging to 27 classes.\n",
      "Epoch 1/10\n",
      "1347/1347 [==============================] - 240s 176ms/step - batch: 673.0000 - size: 31.9881 - loss: 0.3599 - accuracy: 0.8953 - val_loss: 3.5143 - val_accuracy: 0.3595\n",
      "Epoch 2/10\n",
      "1347/1347 [==============================] - 241s 179ms/step - batch: 673.0000 - size: 31.9881 - loss: 0.1329 - accuracy: 0.9549 - val_loss: 4.7407 - val_accuracy: 0.3295\n",
      "Epoch 3/10\n",
      "1347/1347 [==============================] - 249s 185ms/step - batch: 673.0000 - size: 31.9881 - loss: 0.1077 - accuracy: 0.9635 - val_loss: 5.3248 - val_accuracy: 0.3370\n",
      "Epoch 4/10\n",
      "1347/1347 [==============================] - 245s 182ms/step - batch: 673.0000 - size: 31.9881 - loss: 0.0906 - accuracy: 0.9681 - val_loss: 5.8540 - val_accuracy: 0.3376\n",
      "Epoch 5/10\n",
      "1347/1347 [==============================] - 249s 185ms/step - batch: 673.0000 - size: 31.9881 - loss: 0.0878 - accuracy: 0.9688 - val_loss: 5.1617 - val_accuracy: 0.3797\n",
      "Epoch 6/10\n",
      "1347/1347 [==============================] - 256s 190ms/step - batch: 673.0000 - size: 31.9881 - loss: 0.0780 - accuracy: 0.9726 - val_loss: 6.3763 - val_accuracy: 0.3144\n",
      "Epoch 7/10\n",
      "1347/1347 [==============================] - 247s 183ms/step - batch: 673.0000 - size: 31.9881 - loss: 0.0713 - accuracy: 0.9753 - val_loss: 5.6363 - val_accuracy: 0.3529\n",
      "Epoch 8/10\n",
      "1347/1347 [==============================] - 246s 182ms/step - batch: 673.0000 - size: 31.9881 - loss: 0.0666 - accuracy: 0.9766 - val_loss: 5.7265 - val_accuracy: 0.4035\n",
      "Epoch 9/10\n",
      "1347/1347 [==============================] - 223s 166ms/step - batch: 673.0000 - size: 31.9881 - loss: 0.0623 - accuracy: 0.9780 - val_loss: 7.5768 - val_accuracy: 0.3044\n",
      "Epoch 10/10\n",
      "1347/1347 [==============================] - 187s 139ms/step - batch: 673.0000 - size: 31.9881 - loss: 0.0600 - accuracy: 0.9787 - val_loss: 7.5459 - val_accuracy: 0.3343\n",
      "Epoch 1/5\n",
      "1347/1347 [==============================] - 197s 137ms/step - batch: 673.0000 - size: 31.9881 - loss: 0.0324 - accuracy: 0.9902 - val_loss: 0.5421 - val_accuracy: 0.8872\n",
      "Epoch 2/5\n",
      "1347/1347 [==============================] - 185s 138ms/step - batch: 673.0000 - size: 31.9881 - loss: 0.0078 - accuracy: 0.9981 - val_loss: 2.3270 - val_accuracy: 0.7120\n",
      "Epoch 3/5\n",
      "1347/1347 [==============================] - 187s 139ms/step - batch: 673.0000 - size: 31.9881 - loss: 0.0095 - accuracy: 0.9975 - val_loss: 0.2795 - val_accuracy: 0.9559\n",
      "Epoch 4/5\n",
      "1347/1347 [==============================] - 190s 141ms/step - batch: 673.0000 - size: 31.9881 - loss: 0.0106 - accuracy: 0.9973 - val_loss: 0.0696 - val_accuracy: 0.9828\n",
      "Epoch 5/5\n",
      "1347/1347 [==============================] - 182s 135ms/step - batch: 673.0000 - size: 31.9881 - loss: 0.0066 - accuracy: 0.9987 - val_loss: 0.0017 - val_accuracy: 0.9992\n",
      "Validation Loss: 0.0016768763362245273, Validation Accuracy: 0.9991651177406311\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os\n",
    "\n",
    "# Define the dataset path\n",
    "data_dir = r\"C:\\Users\\User\\OneDrive\\Documents\\SignLanguageApp\\SLangDataset\\augmentedData\"\n",
    "\n",
    "# Parameters\n",
    "img_size = (224, 224)  # Image size for resizing\n",
    "batch_size = 32\n",
    "num_classes = len(os.listdir(data_dir))  # Number of classes inferred from folders\n",
    "\n",
    "# Train-test-validation split\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255, validation_split=0.2  # Rescale pixel values and split\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Model selection: MobileNetV2 (simpler and lightweight)\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(img_size[0], img_size[1], 3))\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Freeze the base model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "epochs = 10\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    validation_steps=validation_generator.samples // batch_size\n",
    ")\n",
    "\n",
    "# Unfreeze and fine-tune the base model (optional)\n",
    "base_model.trainable = True\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "fine_tune_epochs = 5\n",
    "history_fine_tune = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=fine_tune_epochs,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    validation_steps=validation_generator.samples // batch_size\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"asl_sign_language_model_mobilenet.h5\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "loss, accuracy = model.evaluate(validation_generator)\n",
    "print(f\"Validation Loss: {loss}, Validation Accuracy: {accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
