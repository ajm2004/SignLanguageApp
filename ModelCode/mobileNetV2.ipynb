{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize hand detector\n",
    "detector = HandDetector(maxHands=1)\n",
    "\n",
    "# Constants\n",
    "offset = 20\n",
    "imgSize = 500\n",
    "baseFolder = \"C:/Users/User/OneDrive/Documents/SignLanguageApp/SLangDataset/ownData/training\"\n",
    "letters = [chr(i) for i in range(ord('A'), ord('Z') + 1)]  # List of letters A-Z\n",
    "captureInterval = 0.2  # Interval between captures in seconds\n",
    "\n",
    "# Loop through each letter\n",
    "for className in letters:\n",
    "    print(f\"Starting collection for class: {className}\")\n",
    "    folder = os.path.join(baseFolder, className)\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    # Initialize counter for the current class\n",
    "    counter = 0\n",
    "    maxImages = 300\n",
    "\n",
    "    collecting = False\n",
    "\n",
    "    while counter < maxImages:\n",
    "        success, img = cap.read()\n",
    "        if not success:\n",
    "            print(\"Failed to access camera.\")\n",
    "            break\n",
    "\n",
    "        # Detect hands in the image\n",
    "        hands, img = detector.findHands(img)\n",
    "        if hands:\n",
    "            hand = hands[0]\n",
    "            x, y, w, h = hand['bbox']\n",
    "\n",
    "            # Create a white canvas\n",
    "            imgWhite = np.ones((imgSize, imgSize, 3), np.uint8) * 255\n",
    "\n",
    "            # Crop the image around the detected hand\n",
    "            imgCrop = img[y - offset:y + h + offset, x - offset:x + w + offset]\n",
    "\n",
    "            # Prevent errors from invalid crop boundaries\n",
    "            if imgCrop.size == 0:\n",
    "                continue\n",
    "\n",
    "            aspectRatio = h / w\n",
    "\n",
    "            try:\n",
    "                if aspectRatio > 1:\n",
    "                    # Height is greater than width\n",
    "                    k = imgSize / h\n",
    "                    wCal = math.ceil(k * w)\n",
    "                    imgResize = cv2.resize(imgCrop, (wCal, imgSize))\n",
    "                    wGap = math.ceil((imgSize - wCal) / 2)\n",
    "                    imgWhite[:, wGap:wCal + wGap] = imgResize\n",
    "                else:\n",
    "                    # Width is greater than height\n",
    "                    k = imgSize / w\n",
    "                    hCal = math.ceil(k * h)\n",
    "                    imgResize = cv2.resize(imgCrop, (imgSize, hCal))\n",
    "                    hGap = math.ceil((imgSize - hCal) / 2)\n",
    "                    imgWhite[hGap:hCal + hGap, :] = imgResize\n",
    "\n",
    "                # Display the processed images\n",
    "                cv2.imshow(\"ImageCrop\", imgCrop)\n",
    "                cv2.imshow(\"ImageWhite\", imgWhite)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image: {e}\")\n",
    "                continue\n",
    "\n",
    "            if collecting:\n",
    "                # Save the image automatically\n",
    "                counter += 1\n",
    "                filePath = os.path.join(folder, f\"{className.lower()}_{counter}.jpg\")\n",
    "                cv2.imwrite(filePath, imgWhite)\n",
    "                print(f\"Saved {counter}/{maxImages} images for class {className}\")\n",
    "\n",
    "        # Display the original image\n",
    "        cv2.imshow(\"Image\", img)\n",
    "\n",
    "        # Check for key press\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('q'):\n",
    "            collecting = True\n",
    "\n",
    "        # Wait for the interval\n",
    "        time.sleep(captureInterval)\n",
    "\n",
    "    print(f\"Completed collection for {className}.\")\n",
    "    input(\"Press 'Enter' to continue to the next letter.\")\n",
    "\n",
    "# Release the webcam and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize hand detector\n",
    "detector = HandDetector(maxHands=1)\n",
    "\n",
    "# Constants\n",
    "offset = 20\n",
    "imgSize = 500\n",
    "baseFolder = \"C:/Users/User/OneDrive/Documents/SignLanguageApp/SLangDataset/ownData/training\"\n",
    "captureInterval = 0.2  # Interval between captures in seconds\n",
    "\n",
    "# Process only the letter 'Space'\n",
    "className = 'Space'\n",
    "print(f\"Starting collection for class: {className}\")\n",
    "folder = os.path.join(baseFolder, className)\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# Initialize counter for the current class\n",
    "counter = 0\n",
    "maxImages = 300\n",
    "\n",
    "collecting = False\n",
    "\n",
    "while counter < maxImages:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        print(\"Failed to access camera.\")\n",
    "        break\n",
    "\n",
    "    # Detect hands in the image\n",
    "    hands, img = detector.findHands(img)\n",
    "    if hands:\n",
    "        hand = hands[0]\n",
    "        x, y, w, h = hand['bbox']\n",
    "\n",
    "        # Create a white canvas\n",
    "        imgWhite = np.ones((imgSize, imgSize, 3), np.uint8) * 255\n",
    "\n",
    "        # Crop the image around the detected hand\n",
    "        imgCrop = img[y - offset:y + h + offset, x - offset:x + w + offset]\n",
    "\n",
    "        # Prevent errors from invalid crop boundaries\n",
    "        if imgCrop.size == 0:\n",
    "            continue\n",
    "\n",
    "        aspectRatio = h / w\n",
    "\n",
    "        try:\n",
    "            if aspectRatio > 1:\n",
    "                # Height is greater than width\n",
    "                k = imgSize / h\n",
    "                wCal = math.ceil(k * w)\n",
    "                imgResize = cv2.resize(imgCrop, (wCal, imgSize))\n",
    "                wGap = math.ceil((imgSize - wCal) / 2)\n",
    "                imgWhite[:, wGap:wCal + wGap] = imgResize\n",
    "            else:\n",
    "                # Width is greater than height\n",
    "                k = imgSize / w\n",
    "                hCal = math.ceil(k * h)\n",
    "                imgResize = cv2.resize(imgCrop, (imgSize, hCal))\n",
    "                hGap = math.ceil((imgSize - hCal) / 2)\n",
    "                imgWhite[hGap:hCal + hGap, :] = imgResize\n",
    "\n",
    "            # Display the processed images\n",
    "            cv2.imshow(\"ImageCrop\", imgCrop)\n",
    "            cv2.imshow(\"ImageWhite\", imgWhite)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image: {e}\")\n",
    "            continue\n",
    "\n",
    "        if collecting:\n",
    "            # Save the image automatically\n",
    "            counter += 1\n",
    "            filePath = os.path.join(folder, f\"{className.lower()}_{counter}.jpg\")\n",
    "            cv2.imwrite(filePath, imgWhite)\n",
    "            print(f\"Saved {counter}/{maxImages} images for class {className}\")\n",
    "\n",
    "    # Display the original image\n",
    "    cv2.imshow(\"Image\", img)\n",
    "\n",
    "    # Check for key press\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        collecting = True\n",
    "\n",
    "    # Wait for the interval\n",
    "    time.sleep(captureInterval)\n",
    "\n",
    "print(f\"Completed collection for {className}.\")\n",
    "input(\"Press 'Enter' to exit.\")\n",
    "\n",
    "# Release the webcam and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1491 images belonging to 24 classes.\n",
      "Found 364 images belonging to 24 classes.\n",
      "Epoch 1/30\n",
      "373/373 [==============================] - 10s 23ms/step - loss: 2.9724 - accuracy: 0.1395 - val_loss: 1.9860 - val_accuracy: 0.3791\n",
      "Epoch 2/30\n",
      "373/373 [==============================] - 8s 22ms/step - loss: 2.1552 - accuracy: 0.2991 - val_loss: 1.5588 - val_accuracy: 0.5000\n",
      "Epoch 3/30\n",
      "373/373 [==============================] - 8s 22ms/step - loss: 1.7991 - accuracy: 0.3930 - val_loss: 1.2230 - val_accuracy: 0.6071\n",
      "Epoch 4/30\n",
      "373/373 [==============================] - 8s 22ms/step - loss: 1.6216 - accuracy: 0.4272 - val_loss: 1.1145 - val_accuracy: 0.6374\n",
      "Epoch 5/30\n",
      "373/373 [==============================] - 8s 22ms/step - loss: 1.5207 - accuracy: 0.4628 - val_loss: 1.0385 - val_accuracy: 0.6786\n",
      "Epoch 6/30\n",
      "373/373 [==============================] - 8s 22ms/step - loss: 1.4463 - accuracy: 0.4903 - val_loss: 0.9313 - val_accuracy: 0.7143\n",
      "Epoch 7/30\n",
      "373/373 [==============================] - 8s 22ms/step - loss: 1.3123 - accuracy: 0.5439 - val_loss: 0.8701 - val_accuracy: 0.7115\n",
      "Epoch 8/30\n",
      "373/373 [==============================] - 8s 22ms/step - loss: 1.3068 - accuracy: 0.5205 - val_loss: 0.8289 - val_accuracy: 0.7280\n",
      "Epoch 9/30\n",
      "373/373 [==============================] - 8s 22ms/step - loss: 1.2163 - accuracy: 0.5553 - val_loss: 0.9495 - val_accuracy: 0.6484\n",
      "Epoch 10/30\n",
      "373/373 [==============================] - 8s 22ms/step - loss: 1.2299 - accuracy: 0.5647 - val_loss: 0.7934 - val_accuracy: 0.7445\n",
      "Epoch 11/30\n",
      "373/373 [==============================] - 8s 22ms/step - loss: 1.2212 - accuracy: 0.5506 - val_loss: 0.8404 - val_accuracy: 0.7500\n",
      "Epoch 12/30\n",
      "373/373 [==============================] - 8s 22ms/step - loss: 1.1089 - accuracy: 0.5989 - val_loss: 0.8005 - val_accuracy: 0.7060\n",
      "Epoch 13/30\n",
      "373/373 [==============================] - 8s 22ms/step - loss: 1.1611 - accuracy: 0.5915 - val_loss: 0.6945 - val_accuracy: 0.7500\n",
      "Epoch 14/30\n",
      "373/373 [==============================] - 8s 22ms/step - loss: 1.1335 - accuracy: 0.5882 - val_loss: 0.7177 - val_accuracy: 0.7170\n",
      "Epoch 15/30\n",
      "373/373 [==============================] - 8s 22ms/step - loss: 1.1002 - accuracy: 0.6023 - val_loss: 0.8789 - val_accuracy: 0.6648\n",
      "Epoch 16/30\n",
      "373/373 [==============================] - 8s 22ms/step - loss: 1.1174 - accuracy: 0.6144 - val_loss: 0.8560 - val_accuracy: 0.7170\n",
      "Epoch 17/30\n",
      "373/373 [==============================] - 8s 22ms/step - loss: 1.0987 - accuracy: 0.6009 - val_loss: 0.8824 - val_accuracy: 0.6896\n",
      "Epoch 18/30\n",
      "373/373 [==============================] - 8s 22ms/step - loss: 1.1203 - accuracy: 0.5956 - val_loss: 0.6930 - val_accuracy: 0.8049\n",
      "Epoch 19/30\n",
      "373/373 [==============================] - 8s 22ms/step - loss: 1.1043 - accuracy: 0.6063 - val_loss: 0.7519 - val_accuracy: 0.7555\n",
      "Epoch 20/30\n",
      "373/373 [==============================] - 8s 22ms/step - loss: 1.1016 - accuracy: 0.5976 - val_loss: 0.7238 - val_accuracy: 0.7473\n",
      "Epoch 21/30\n",
      "373/373 [==============================] - 8s 22ms/step - loss: 1.0463 - accuracy: 0.6130 - val_loss: 0.7472 - val_accuracy: 0.7775\n",
      "Epoch 22/30\n",
      "373/373 [==============================] - 8s 22ms/step - loss: 1.0241 - accuracy: 0.6325 - val_loss: 0.7427 - val_accuracy: 0.7198\n",
      "Epoch 23/30\n",
      "373/373 [==============================] - 8s 21ms/step - loss: 1.0437 - accuracy: 0.6123 - val_loss: 0.6973 - val_accuracy: 0.7582\n",
      "Epoch 24/30\n",
      "373/373 [==============================] - 8s 22ms/step - loss: 1.0601 - accuracy: 0.6217 - val_loss: 0.7321 - val_accuracy: 0.7637\n",
      "Epoch 25/30\n",
      "373/373 [==============================] - 8s 22ms/step - loss: 1.0275 - accuracy: 0.6378 - val_loss: 0.7409 - val_accuracy: 0.7802\n",
      "Epoch 26/30\n",
      "373/373 [==============================] - 8s 21ms/step - loss: 1.0502 - accuracy: 0.6372 - val_loss: 0.6534 - val_accuracy: 0.7555\n",
      "Epoch 27/30\n",
      "373/373 [==============================] - 8s 22ms/step - loss: 1.0272 - accuracy: 0.6150 - val_loss: 0.6649 - val_accuracy: 0.7637\n",
      "Epoch 28/30\n",
      "373/373 [==============================] - 8s 22ms/step - loss: 1.0399 - accuracy: 0.6392 - val_loss: 0.6859 - val_accuracy: 0.7418\n",
      "Epoch 29/30\n",
      "373/373 [==============================] - 8s 22ms/step - loss: 0.9973 - accuracy: 0.6338 - val_loss: 0.6637 - val_accuracy: 0.7885\n",
      "Epoch 30/30\n",
      "373/373 [==============================] - 8s 21ms/step - loss: 1.0250 - accuracy: 0.6177 - val_loss: 0.5802 - val_accuracy: 0.7802\n",
      "Epoch 30/40\n",
      "373/373 [==============================] - 10s 23ms/step - loss: 4.0502 - accuracy: 0.2448 - val_loss: 0.7781 - val_accuracy: 0.7418\n",
      "Epoch 31/40\n",
      "373/373 [==============================] - 8s 21ms/step - loss: 2.4179 - accuracy: 0.2998 - val_loss: 0.8792 - val_accuracy: 0.6786\n",
      "Epoch 32/40\n",
      "373/373 [==============================] - 8s 21ms/step - loss: 2.0909 - accuracy: 0.3555 - val_loss: 0.8115 - val_accuracy: 0.7005\n",
      "Epoch 33/40\n",
      "373/373 [==============================] - 8s 21ms/step - loss: 1.8979 - accuracy: 0.4024 - val_loss: 0.8507 - val_accuracy: 0.6951\n",
      "Epoch 34/40\n",
      "373/373 [==============================] - 8s 21ms/step - loss: 1.7475 - accuracy: 0.4319 - val_loss: 0.8350 - val_accuracy: 0.6951\n",
      "Epoch 35/40\n",
      "373/373 [==============================] - 8s 21ms/step - loss: 1.6277 - accuracy: 0.4829 - val_loss: 0.8599 - val_accuracy: 0.6813\n",
      "Epoch 36/40\n",
      "373/373 [==============================] - 8s 22ms/step - loss: 1.5495 - accuracy: 0.4681 - val_loss: 0.7894 - val_accuracy: 0.6951\n",
      "Epoch 37/40\n",
      "373/373 [==============================] - 8s 22ms/step - loss: 1.4476 - accuracy: 0.5023 - val_loss: 0.7221 - val_accuracy: 0.7308\n",
      "Epoch 38/40\n",
      "373/373 [==============================] - 8s 22ms/step - loss: 1.4058 - accuracy: 0.5191 - val_loss: 0.7442 - val_accuracy: 0.7253\n",
      "Epoch 39/40\n",
      "373/373 [==============================] - 8s 22ms/step - loss: 1.3314 - accuracy: 0.5399 - val_loss: 0.7713 - val_accuracy: 0.7280\n",
      "Epoch 40/40\n",
      "373/373 [==============================] - 8s 22ms/step - loss: 1.2442 - accuracy: 0.5627 - val_loss: 0.6836 - val_accuracy: 0.7665\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os\n",
    "\n",
    "# Path to dataset\n",
    "baseFolder = \"C:/Users/User/OneDrive/Documents/SignLanguageApp/SLangDataset/asl-alphabet-2/training_annotated\"\n",
    "image_size = 224  # MobileNetV2 input size\n",
    "batch_size = 4\n",
    "num_classes = len(os.listdir(baseFolder))  # Number of classes (letters)\n",
    "\n",
    "# Data augmentation and preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2,  # Use 20% of data for validation\n",
    ")\n",
    "\n",
    "# Load training data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    baseFolder,\n",
    "    target_size=(image_size, image_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"training\",\n",
    ")\n",
    "\n",
    "# Load validation data\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    baseFolder,\n",
    "    target_size=(image_size, image_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"validation\",\n",
    ")\n",
    "\n",
    "# Load the MobileNetV2 model\n",
    "base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(image_size, image_size, 3))\n",
    "\n",
    "# Freeze the base model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add custom layers for classification\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)  # Regularization\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "# Create the final model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "epochs = 30\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_generator,\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"sign_language_mobilenetv2.h5\")\n",
    "\n",
    "# Fine-tune the model (unfreeze some base model layers)\n",
    "base_model.trainable = True\n",
    "fine_tune_at = 100  # Unfreeze from this layer onwards\n",
    "\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile the model with a lower learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Fine-tuning\n",
    "fine_tune_epochs = 10\n",
    "total_epochs = epochs + fine_tune_epochs\n",
    "history_fine = model.fit(\n",
    "    train_generator,\n",
    "    epochs=total_epochs,\n",
    "    initial_epoch=history.epoch[-1],\n",
    "    validation_data=val_generator,\n",
    ")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save(\"sign_language_mobilenetv2_fine_tuned.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Labels Found\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     15\u001b[0m     success, img \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m---> 16\u001b[0m     imgOutput \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m()\n\u001b[0;32m     17\u001b[0m     hands, img \u001b[38;5;241m=\u001b[39m detector\u001b[38;5;241m.\u001b[39mfindHands(img)\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hands:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "from cvzone.ClassificationModule import Classifier\n",
    "import numpy as np\n",
    "import math\n",
    "cap = cv2.VideoCapture(0)\n",
    "detector = HandDetector(maxHands=1)\n",
    "# classifier = Classifier(\"Model/keras_model.h5\", \"Model/labels.txt\")\n",
    "classifier = Classifier(\"C:/Users/User/OneDrive/Documents/SignLanguageApp/ModelCode/sign_language_mobilenetv2_fine_tuned.h5\")\n",
    "offset = 20\n",
    "imgSize = 300\n",
    "counter = 0\n",
    "labels = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\", \"Space\"]\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    imgOutput = img.copy()\n",
    "    hands, img = detector.findHands(img)\n",
    "    if hands:\n",
    "        hand = hands[0]\n",
    "        x, y, w, h = hand['bbox']\n",
    "        imgWhite = np.ones((imgSize, imgSize, 3), np.uint8) * 255\n",
    "\n",
    "        # Ensure the cropping boundaries are within the image dimensions\n",
    "        y1, y2 = max(0, y - offset), min(img.shape[0], y + h + offset)\n",
    "        x1, x2 = max(0, x - offset), min(img.shape[1], x + w + offset)\n",
    "        imgCrop = img[y1:y2, x1:x2]\n",
    "\n",
    "        imgCropShape = imgCrop.shape\n",
    "        if imgCropShape[0] > 0 and imgCropShape[1] > 0:  # Ensure the cropped image is not empty\n",
    "            aspectRatio = h / w\n",
    "            if aspectRatio > 1:\n",
    "                k = imgSize / h\n",
    "                wCal = math.ceil(k * w)\n",
    "                imgResize = cv2.resize(imgCrop, (wCal, imgSize))\n",
    "                imgResizeShape = imgResize.shape\n",
    "                wGap = math.ceil((imgSize - wCal) / 2)\n",
    "                imgWhite[:, wGap:wCal + wGap] = imgResize\n",
    "                prediction, index = classifier.getPrediction(imgWhite, draw=False)\n",
    "                print(prediction, index)\n",
    "            else:\n",
    "                k = imgSize / w\n",
    "                hCal = math.ceil(k * h)\n",
    "                imgResize = cv2.resize(imgCrop, (imgSize, hCal))\n",
    "                imgResizeShape = imgResize.shape\n",
    "                hGap = math.ceil((imgSize - hCal) / 2)\n",
    "                imgWhite[hGap:hCal + hGap, :] = imgResize\n",
    "                prediction, index = classifier.getPrediction(imgWhite, draw=False)\n",
    "        if 0 <= index < len(labels):\n",
    "            cv2.rectangle(imgOutput, (x - offset, y - offset-50),\n",
    "                          (x - offset+90, y - offset-50+50), (255, 0, 255), cv2.FILLED)\n",
    "            cv2.putText(imgOutput, labels[index], (x, y -26), cv2.FONT_HERSHEY_COMPLEX, 1.7, (255, 255, 255), 2)\n",
    "            cv2.rectangle(imgOutput, (x-offset, y-offset),\n",
    "                          (x + w+offset, y + h+offset), (255, 0, 255), 4)\n",
    "        cv2.imshow(\"ImageCrop\", imgCrop)\n",
    "        cv2.imshow(\"ImageWhite\", imgWhite)\n",
    "    cv2.imshow(\"Image\", imgOutput)\n",
    "    cv2.waitKey(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
