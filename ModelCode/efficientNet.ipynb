{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 43200 images belonging to 27 classes.\n",
      "Found 10800 images belonging to 27 classes.\n",
      "Epoch 2/40\n",
      " 1498/10800 [===>..........................] - ETA: 3:28 - loss: 0.1744 - accuracy: 0.9549"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 89\u001b[0m\n\u001b[0;32m     87\u001b[0m fine_tune_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m     88\u001b[0m total_epochs \u001b[38;5;241m=\u001b[39m epochs \u001b[38;5;241m+\u001b[39m fine_tune_epochs\n\u001b[1;32m---> 89\u001b[0m history_fine \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Save the fine-tuned model\u001b[39;00m\n\u001b[0;32m     97\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msign_language_mobilenetv2_fine_tuned.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os\n",
    "\n",
    "# Path to dataset\n",
    "baseFolder = \"C:/Users/User/OneDrive/Documents/SignLanguageApp/SLangDataset/data\"\n",
    "image_size = 224  # MobileNetV2 input size\n",
    "batch_size = 4\n",
    "num_classes = len(os.listdir(baseFolder))  # Number of classes (letters)\n",
    "\n",
    "# Data augmentation and preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2,  # Use 20% of data for validation\n",
    ")\n",
    "\n",
    "# Load training data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    baseFolder,\n",
    "    target_size=(image_size, image_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"training\",\n",
    ")\n",
    "\n",
    "# Load validation data\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    baseFolder,\n",
    "    target_size=(image_size, image_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"validation\",\n",
    ")\n",
    "\n",
    "# Load the MobileNetV2 model\n",
    "base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(image_size, image_size, 3))\n",
    "\n",
    "# # Freeze the base model\n",
    "# base_model.trainable = False\n",
    "\n",
    "# # Add custom layers for classification\n",
    "# x = base_model.output\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "# x = Dropout(0.5)(x)  # Regularization\n",
    "# x = Dense(128, activation=\"relu\")(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "# predictions = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "# # Create the final model\n",
    "# model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer=Adam(learning_rate=0.001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# # Train the model\n",
    "# epochs = 30\n",
    "# history = model.fit(\n",
    "#     train_generator,\n",
    "#     epochs=epochs,\n",
    "#     validation_data=val_generator,\n",
    "# )\n",
    "\n",
    "# # Save the model\n",
    "# model.save(\"sign_language_mobilenetv2.h5\")\n",
    "model = tf.keras.models.load_model(\"sign_language_mobilenetv2.h5\")\n",
    "# Fine-tune the model (unfreeze some base model layers)\n",
    "base_model.trainable = True\n",
    "fine_tune_at = 100  # Unfreeze from this layer onwards\n",
    "\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile the model with a lower learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Fine-tuning\n",
    "fine_tune_epochs = 10\n",
    "total_epochs = epochs + fine_tune_epochs\n",
    "history_fine = model.fit(\n",
    "    train_generator,\n",
    "    epochs=total_epochs,\n",
    "    initial_epoch=history.epoch[1],\n",
    "    validation_data=val_generator,\n",
    ")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save(\"sign_language_mobilenetv2_fine_tuned.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 43200 images belonging to 27 classes.\n",
      "Found 10800 images belonging to 27 classes.\n",
      "Epoch 1/20\n",
      "1350/1350 [==============================] - 300s 221ms/step - loss: 2.6574 - accuracy: 0.1597 - val_loss: 1.9378 - val_accuracy: 0.3586\n",
      "Epoch 2/20\n",
      "1350/1350 [==============================] - 292s 217ms/step - loss: 2.1267 - accuracy: 0.2696 - val_loss: 1.6999 - val_accuracy: 0.4494\n",
      "Epoch 3/20\n",
      "1350/1350 [==============================] - 293s 217ms/step - loss: 1.9834 - accuracy: 0.3105 - val_loss: 1.5697 - val_accuracy: 0.4853\n",
      "Epoch 4/20\n",
      "1350/1350 [==============================] - 289s 214ms/step - loss: 1.8914 - accuracy: 0.3386 - val_loss: 1.4648 - val_accuracy: 0.5179\n",
      "Epoch 5/20\n",
      "1350/1350 [==============================] - 290s 215ms/step - loss: 1.8479 - accuracy: 0.3561 - val_loss: 1.3960 - val_accuracy: 0.5506\n",
      "Epoch 6/20\n",
      "1350/1350 [==============================] - 296s 219ms/step - loss: 1.8057 - accuracy: 0.3639 - val_loss: 1.3508 - val_accuracy: 0.5418\n",
      "Epoch 7/20\n",
      "1350/1350 [==============================] - 296s 219ms/step - loss: 1.7618 - accuracy: 0.3763 - val_loss: 1.3505 - val_accuracy: 0.5350\n",
      "Epoch 8/20\n",
      "1350/1350 [==============================] - 291s 216ms/step - loss: 1.7472 - accuracy: 0.3836 - val_loss: 1.3108 - val_accuracy: 0.5727\n",
      "Epoch 9/20\n",
      "1350/1350 [==============================] - 289s 214ms/step - loss: 1.7174 - accuracy: 0.3894 - val_loss: 1.2648 - val_accuracy: 0.5818\n",
      "Epoch 10/20\n",
      "1350/1350 [==============================] - 291s 215ms/step - loss: 1.7001 - accuracy: 0.4005 - val_loss: 1.2233 - val_accuracy: 0.6153\n",
      "Epoch 11/20\n",
      "1350/1350 [==============================] - 291s 215ms/step - loss: 1.6929 - accuracy: 0.3990 - val_loss: 1.2537 - val_accuracy: 0.5971\n",
      "Epoch 12/20\n",
      "1350/1350 [==============================] - 291s 215ms/step - loss: 1.6777 - accuracy: 0.4019 - val_loss: 1.2077 - val_accuracy: 0.6101\n",
      "Epoch 13/20\n",
      "1350/1350 [==============================] - 291s 215ms/step - loss: 1.6629 - accuracy: 0.4093 - val_loss: 1.2303 - val_accuracy: 0.6001\n",
      "Epoch 14/20\n",
      "1350/1350 [==============================] - 290s 215ms/step - loss: 1.6560 - accuracy: 0.4127 - val_loss: 1.1476 - val_accuracy: 0.6161\n",
      "Epoch 15/20\n",
      "1350/1350 [==============================] - 291s 216ms/step - loss: 1.6571 - accuracy: 0.4117 - val_loss: 1.1919 - val_accuracy: 0.6242\n",
      "Epoch 16/20\n",
      "1350/1350 [==============================] - 292s 216ms/step - loss: 1.6440 - accuracy: 0.4136 - val_loss: 1.1476 - val_accuracy: 0.6144\n",
      "Epoch 17/20\n",
      "1350/1350 [==============================] - 290s 215ms/step - loss: 1.6359 - accuracy: 0.4191 - val_loss: 1.1662 - val_accuracy: 0.6256\n",
      "Epoch 18/20\n",
      "1350/1350 [==============================] - 291s 215ms/step - loss: 1.6249 - accuracy: 0.4201 - val_loss: 1.1552 - val_accuracy: 0.6152\n",
      "Epoch 19/20\n",
      "1350/1350 [==============================] - 291s 215ms/step - loss: 1.6192 - accuracy: 0.4229 - val_loss: 1.1534 - val_accuracy: 0.6298\n",
      "Epoch 20/20\n",
      "1350/1350 [==============================] - 377s 279ms/step - loss: 1.6118 - accuracy: 0.4269 - val_loss: 1.1214 - val_accuracy: 0.6550\n",
      "Epoch 20/30\n",
      "1350/1350 [==============================] - 427s 315ms/step - loss: 18.0620 - accuracy: 0.2427 - val_loss: 1.6207 - val_accuracy: 0.4154\n",
      "Epoch 21/30\n",
      "1350/1350 [==============================] - 295s 219ms/step - loss: 1.9289 - accuracy: 0.3447 - val_loss: 1.1648 - val_accuracy: 0.5658\n",
      "Epoch 22/30\n",
      "1350/1350 [==============================] - 297s 220ms/step - loss: 1.4863 - accuracy: 0.4597 - val_loss: 0.8080 - val_accuracy: 0.7367\n",
      "Epoch 23/30\n",
      "1350/1350 [==============================] - 303s 224ms/step - loss: 1.2157 - accuracy: 0.5501 - val_loss: 0.5948 - val_accuracy: 0.8326\n",
      "Epoch 24/30\n",
      "1350/1350 [==============================] - 301s 223ms/step - loss: 1.0348 - accuracy: 0.6140 - val_loss: 0.5378 - val_accuracy: 0.8111\n",
      "Epoch 25/30\n",
      "1350/1350 [==============================] - 296s 220ms/step - loss: 0.8907 - accuracy: 0.6656 - val_loss: 0.3908 - val_accuracy: 0.8743\n",
      "Epoch 26/30\n",
      "1350/1350 [==============================] - 300s 222ms/step - loss: 0.7844 - accuracy: 0.7059 - val_loss: 0.3087 - val_accuracy: 0.9152\n",
      "Epoch 27/30\n",
      "1350/1350 [==============================] - 300s 222ms/step - loss: 0.6798 - accuracy: 0.7477 - val_loss: 0.2647 - val_accuracy: 0.9145\n",
      "Epoch 28/30\n",
      "1350/1350 [==============================] - 299s 222ms/step - loss: 0.5749 - accuracy: 0.7883 - val_loss: 0.1847 - val_accuracy: 0.9527\n",
      "Epoch 29/30\n",
      "1350/1350 [==============================] - 298s 221ms/step - loss: 0.4731 - accuracy: 0.8317 - val_loss: 0.1205 - val_accuracy: 0.9718\n",
      "Epoch 30/30\n",
      "1350/1350 [==============================] - 301s 223ms/step - loss: 0.3705 - accuracy: 0.8768 - val_loss: 0.0756 - val_accuracy: 0.9804\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os\n",
    "\n",
    "# Path to dataset\n",
    "baseFolder = \"C:/Users/User/OneDrive/Documents/SignLanguageApp/SLangDataset/data\"\n",
    "image_size = 224  # Input size for ResNet\n",
    "batch_size = 32\n",
    "num_classes = len(os.listdir(baseFolder))  # Number of classes (letters)\n",
    "\n",
    "# Data augmentation and preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2,  # Use 20% of data for validation\n",
    ")\n",
    "\n",
    "# Load training data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    baseFolder,\n",
    "    target_size=(image_size, image_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"training\",\n",
    ")\n",
    "\n",
    "# Load validation data\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    baseFolder,\n",
    "    target_size=(image_size, image_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"validation\",\n",
    ")\n",
    "\n",
    "# Load the ResNet50 model\n",
    "base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(image_size, image_size, 3))\n",
    "\n",
    "# Freeze the base model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add custom layers for classification\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)  # Regularization\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "# Create the final model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "epochs = 20\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_generator,\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"sign_language_resnet50.h5\")\n",
    "\n",
    "# Fine-tune the model (unfreeze some base model layers)\n",
    "base_model.trainable = True\n",
    "fine_tune_at = 100  # Unfreeze from this layer onwards\n",
    "\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile the model with a lower learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Fine-tuning\n",
    "fine_tune_epochs = 10\n",
    "total_epochs = epochs + fine_tune_epochs\n",
    "history_fine = model.fit(\n",
    "    train_generator,\n",
    "    epochs=total_epochs,\n",
    "    initial_epoch=history.epoch[-1],\n",
    "    validation_data=val_generator,\n",
    ")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save(\"sign_language_resnet50_fine_tuned.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Labels Found\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'imgOutput' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 59\u001b[0m\n\u001b[0;32m     57\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage\u001b[39m\u001b[38;5;124m\"\u001b[39m, imgOutput)\n\u001b[0;32m     58\u001b[0m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 59\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mimgOutput\u001b[49m)\n\u001b[0;32m     60\u001b[0m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'imgOutput' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "from cvzone.ClassificationModule import Classifier\n",
    "import numpy as np\n",
    "import math\n",
    "cap = cv2.VideoCapture(0)\n",
    "detector = HandDetector(maxHands=1)\n",
    "# classifier = Classifier(\"Model/keras_model.h5\", \"Model/labels.txt\")\n",
    "classifier = Classifier(\"C:/Users/User/OneDrive/Documents/SignLanguageApp/ModelCode/sign_language_mobilenetv2.h5\")\n",
    "offset = 20\n",
    "imgSize = 300\n",
    "counter = 0\n",
    "labels = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"Space\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"]\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if success:\n",
    "        imgOutput = img.copy()\n",
    "        hands, img = detector.findHands(img)\n",
    "        if hands:\n",
    "            hand = hands[0]\n",
    "            x, y, w, h = hand['bbox']\n",
    "            imgWhite = np.ones((imgSize, imgSize, 3), np.uint8) * 255\n",
    "\n",
    "            # Ensure the cropping boundaries are within the image dimensions\n",
    "            y1, y2 = max(0, y - offset), min(img.shape[0], y + h + offset)\n",
    "            x1, x2 = max(0, x - offset), min(img.shape[1], x + w + offset)\n",
    "            imgCrop = img[y1:y2, x1:x2]\n",
    "\n",
    "            imgCropShape = imgCrop.shape\n",
    "            if imgCropShape[0] > 0 and imgCropShape[1] > 0:  # Ensure the cropped image is not empty\n",
    "                aspectRatio = h / w\n",
    "                if aspectRatio > 1:\n",
    "                    k = imgSize / h\n",
    "                    wCal = math.ceil(k * w)\n",
    "                    imgResize = cv2.resize(imgCrop, (wCal, imgSize))\n",
    "                    imgResizeShape = imgResize.shape\n",
    "                    wGap = math.ceil((imgSize - wCal) / 2)\n",
    "                    imgWhite[:, wGap:wCal + wGap] = imgResize\n",
    "                    prediction, index = classifier.getPrediction(imgWhite, draw=False)\n",
    "                    print(prediction, index)\n",
    "                else:\n",
    "                    k = imgSize / w\n",
    "                    hCal = math.ceil(k * h)\n",
    "                    imgResize = cv2.resize(imgCrop, (imgSize, hCal))\n",
    "                    imgResizeShape = imgResize.shape\n",
    "                    hGap = math.ceil((imgSize - hCal) / 2)\n",
    "                    imgWhite[hGap:hCal + hGap, :] = imgResize\n",
    "                    prediction, index = classifier.getPrediction(imgWhite, draw=False)\n",
    "            if 0 <= index < len(labels):\n",
    "                cv2.rectangle(imgOutput, (x - offset, y - offset-50),\n",
    "                              (x - offset+90, y - offset-50+50), (255, 0, 255), cv2.FILLED)\n",
    "                cv2.putText(imgOutput, labels[index], (x, y -26), cv2.FONT_HERSHEY_COMPLEX, 1.7, (255, 255, 255), 2)\n",
    "                cv2.rectangle(imgOutput, (x-offset, y-offset),\n",
    "                              (x + w+offset, y + h+offset), (255, 0, 255), 4)\n",
    "            cv2.imshow(\"ImageCrop\", imgCrop)\n",
    "            cv2.imshow(\"ImageWhite\", imgWhite)\n",
    "        cv2.imshow(\"Image\", imgOutput)\n",
    "    cv2.waitKey(1)\n",
    "    cv2.imshow(\"Image\", imgOutput)\n",
    "    cv2.waitKey(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
